{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(fname, get_embeddings=True, get_w2i=False, get_i2w=False, skip_first_line=True):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    \n",
    "    if skip_first_line:\n",
    "        fin.readline()\n",
    "    \n",
    "    num_embeddings = 0\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.rstrip().split(' ')\n",
    "        \n",
    "        if get_w2i:\n",
    "            word2idx[line[0]] = num_embeddings\n",
    "        if get_i2w:\n",
    "            idx2word[num_embeddings] = line[0]\n",
    "        if get_embeddings:       \n",
    "            embeddings.append([float(num) for num in line[1:]])\n",
    "        \n",
    "        num_embeddings += 1\n",
    "        \n",
    "        \n",
    "    return torch.FloatTensor(embeddings), word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx = load_embeddings('../embeddings/wiki-news-300d-1M.vec', get_embeddings=False, get_w2i=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "      \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = torch.matmul(q , k.transpose(-2, -1) / math.sqrt(q.size(-1)))\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask.unsqueeze(1) == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))        \n",
    "        res = torch.matmul(attn, v)\n",
    "\n",
    "        return res, attn\n",
    "\n",
    "class AdditiveSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.w = nn.Linear(d_model, d_model)\n",
    "        self.q = torch.nn.Parameter(torch.FloatTensor(d_model).uniform_(-0.1, 0.1))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn = torch.tanh(self.dropout(self.w(x)))        \n",
    "        attn = torch.matmul(attn, self.q)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        \n",
    "        res = torch.einsum('ijk, ij->ik', x, attn)\n",
    "        return res, attn\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_qk, d_v, track_agreement=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_qk = d_qk\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, num_heads * d_v, bias=False)\n",
    "        \n",
    "        self.w_fc = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "        \n",
    "        self.attention = MultiplicativeAttention(dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "        self.track_agreement = track_agreement\n",
    "        self.v_agreement = 0\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):     \n",
    "        batch_size = q.shape[0]\n",
    "        seq_size = q.shape[1]\n",
    "        \n",
    "        q_proj = self.w_q(q).view(q.shape[0], q.shape[1], self.num_heads, self.d_qk)\n",
    "        k_proj = self.w_k(k).view(k.shape[0], k.shape[1], self.num_heads, self.d_qk)\n",
    "        v_proj = self.w_v(v).view(v.shape[0], v.shape[1], self.num_heads, self.d_v) \n",
    "\n",
    "        if self.track_agreement:\n",
    "            self.v_agreement += torch.einsum('bshd, bsnd->', F.normalize(v_proj, dim=3), F.normalize(v_proj, dim=3)) / self.num_heads**2\n",
    "\n",
    "        if mask is None:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2))\n",
    "        else:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2), mask.unsqueeze(1))\n",
    "        \n",
    "        q = q.transpose(1, 2).contiguous()\n",
    "        q = q.view(batch_size, seq_size, -1)\n",
    "\n",
    "        q = self.dropout(self.w_fc(q))\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "        \n",
    "        return q, attn\n",
    "\n",
    "    def clear_agreement(self):\n",
    "        self.v_agreement = 0\n",
    "\n",
    "class NonlinearFF(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class TitleEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, d_model, num_heads, d_qk, d_v, d_hid=None, embeddings=None, track_agreement=False, padding_idx=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(num_embeddings, d_model, sparse=True, padding_idx=0)\n",
    "            \n",
    "        self.mh_attn = MultiHeadAttention(d_model, num_heads, d_qk, d_v, track_agreement=track_agreement, dropout=dropout)\n",
    "        self.nff = NonlinearFF(d_model, d_hid if d_hid is not None else d_model * 4, dropout=dropout)\n",
    "        self.add_attn = AdditiveSelfAttention(d_model, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "    def forward(self, title):    \n",
    "        mask = (title == self.padding_idx).byte()\n",
    "        \n",
    "        q = k = v = self.embeddings(title)\n",
    "        title, attn = self.mh_attn(q, k ,v, mask=mask)\n",
    "        \n",
    "        title = self.nff(title)\n",
    "        title, add_attn = self.add_attn(title, mask=mask)\n",
    "        \n",
    "        title = self.layer_norm(title)\n",
    "        \n",
    "        return title\n",
    "    \n",
    "    def load_embeddings(embeddings):\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "title_embedding = torch.load(r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_oooo.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultipleOptimizer:\n",
    "    def __init__(self, *op):\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "sparse_params = []\n",
    "dense_params = []\n",
    "\n",
    "for name, param in title_embedding.named_parameters():\n",
    "    if name == 'embeddings.weight':\n",
    "        sparse_params.append(param)\n",
    "    else:\n",
    "        dense_params.append(param)\n",
    "        \n",
    "opt_dense = torch.optim.Adam(dense_params, lr=1e-3)\n",
    "opt_sparse = torch.optim.SGD(sparse_params, lr=1e-3)\n",
    "\n",
    "optimizer = MultipleOptimizer(opt_sparse, opt_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "batch_size = 128\n",
    "num_samples = 32\n",
    "\n",
    "d_model = title_embedding.mh_attn.d_model\n",
    "reg_coeff = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset part 1\n",
      "avg loss at batch 1000: 3.275847445940599\n",
      "avg loss at batch 2000: 3.2701533045619726\n",
      "avg loss at batch 3000: 3.268993166508153\n",
      "avg loss at batch 4000: 3.250702240038663\n",
      "avg loss at batch 5000: 3.2502781853545457\n",
      "avg loss at batch 6000: 3.251233186107129\n",
      "avg loss at batch 7000: 3.256643380271271\n",
      "avg loss at batch 8000: 3.250356246251613\n",
      "avg loss at batch 9000: 3.251795514021069\n",
      "avg loss at batch 10000: 3.248294446617365\n",
      "loaded dataset part 2\n",
      "avg loss at batch 1000: 3.349393581971526\n",
      "avg loss at batch 2000: 3.3466336256824434\n",
      "avg loss at batch 3000: 3.338762879371643\n",
      "avg loss at batch 4000: 3.3353805989027023\n",
      "avg loss at batch 5000: 3.3269967027008533\n",
      "avg loss at batch 6000: 3.3195725665427744\n",
      "avg loss at batch 7000: 3.3331831267569214\n",
      "avg loss at batch 8000: 3.3159784404560924\n",
      "avg loss at batch 9000: 3.330627430928871\n",
      "avg loss at batch 10000: 3.3234239539597183\n",
      "loaded dataset part 3\n",
      "avg loss at batch 1000: 3.6315361335873604\n",
      "avg loss at batch 2000: 3.5987137421034276\n",
      "avg loss at batch 3000: 3.610265028430149\n",
      "avg loss at batch 4000: 3.5940242900978774\n",
      "avg loss at batch 5000: 3.586738067679107\n",
      "avg loss at batch 6000: 3.5873590889386833\n",
      "avg loss at batch 7000: 3.5965554222930223\n",
      "avg loss at batch 8000: 3.5791351080406457\n",
      "avg loss at batch 9000: 3.5847850404679775\n",
      "avg loss at batch 10000: 3.601804956095293\n",
      "loaded dataset part 4\n",
      "avg loss at batch 1000: 3.6425317514222115\n",
      "avg loss at batch 2000: 3.635471851332113\n",
      "avg loss at batch 3000: 3.6280103530734777\n",
      "avg loss at batch 4000: 3.622167847584933\n",
      "avg loss at batch 5000: 3.6185704115778208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-379436f2ab27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mtarget_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_embedding_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "avg_batch_loss = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    for dataset_part in range(0, 7):\n",
    "\n",
    "        dataset = torch.load(r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\dataset{}.pt'.format(dataset_part))\n",
    "\n",
    "        print('loaded dataset part {}'.format(dataset_part + 1))\n",
    "\n",
    "        len_dataset = len(dataset.tensors[1])\n",
    "        len_sequence = dataset.tensors[1].shape[1]\n",
    "\n",
    "        train_loader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)        \n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "\n",
    "            title_embedding.mh_attn.clear_agreement()\n",
    "\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            sz = x.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sample_indices = torch.FloatTensor(sz * num_samples).uniform_(0, len_dataset - 1).long()        \n",
    "            sample_indices, tmp = torch.broadcast_tensors(sample_indices.unsqueeze(1), \n",
    "                                                      torch.arange(sz * num_samples * len_sequence)\n",
    "                                                      .view(sz * num_samples, len_sequence))\n",
    "\n",
    "            n = torch.gather(dataset.tensors[1], 0, sample_indices).to(device)\n",
    "\n",
    "            x = title_embedding(x)\n",
    "            y = title_embedding(y)\n",
    "            n = title_embedding(n)\n",
    "\n",
    "            target_loss = F.cosine_embedding_loss(x, y, torch.Tensor([1]).to(device), margin=0.5)\n",
    "\n",
    "            x = torch.broadcast_tensors(x.unsqueeze(1), n.view(sz, num_samples, d_model))[0].flatten(0,1)\n",
    "\n",
    "            noise_loss = F.cosine_embedding_loss(x, n, torch.Tensor([-1]).to(device), margin=0.5, reduction='none')\n",
    "            noise_loss = noise_loss.view(sz, num_samples, 1).sum(1).mean()    \n",
    "\n",
    "            loss = target_loss + noise_loss + reg_coeff * title_embedding.mh_attn.v_agreement\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            title_embedding.mh_attn.clear_agreement()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_batch_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print('avg loss at batch {}: {}'.format((idx+1), avg_batch_loss / batch_size))\n",
    "                losses.append(avg_batch_loss / (1000 * (1 + num_samples)) )\n",
    "                avg_batch_loss = 0\n",
    "\n",
    "            if (idx + 1) == 10000:\n",
    "                break\n",
    "\n",
    "    #torch.save(title_embedding.to('cpu'), r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_{}.pt'.format(dataset_part))\n",
    "    #title_embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del title_embedding\n",
    "#title_embedding = torch.load(r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_ooo.pt', map_location='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitleEmbedding(\n",
       "  (embeddings): Embedding(3312341, 300, padding_idx=0, sparse=True)\n",
       "  (mh_attn): MultiHeadAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (w_q): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_k): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_v): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_fc): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (attention): MultiplicativeAttention(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (nff): NonlinearFF(\n",
       "    (w_1): Linear(in_features=300, out_features=1200, bias=True)\n",
       "    (w_2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (add_attn): AdditiveSelfAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "    (w): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(title_embedding.to('cpu'), r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_ft.pt'.format(0))\n",
    "title_embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(title_embedding.to('cpu'), r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_0_owo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(title_embedding, r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\model_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.plot(range(len(losses)), losses,'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_title = torch.LongTensor([1, 9, 17])\n",
    "padded_title = torch.LongTensor([1, 9, 17, 0])\n",
    "\n",
    "w_embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True, padding_idx=0)\n",
    "w_embeddings.weight.data[0] = torch.zeros(300)\n",
    "mult_attn = MultiplicativeAttention()\n",
    "add_attn = AdditiveSelfAttention(300)\n",
    "mh_attn = MultiHeadAttention(300, 12, 25, 25)\n",
    "nff = NonlinearFF(300, 25)\n",
    "\n",
    "torch.set_printoptions(linewidth=120,threshold=100)\n",
    "mult_attn.eval()\n",
    "mh_attn.eval()\n",
    "nff.eval()\n",
    "add_attn.eval()\n",
    "mult_attn.eval()\n",
    "title_embedding.eval()\n",
    "\n",
    "def transform(title, mask=None): \n",
    "    z = title\n",
    "    s = mask\n",
    "    print('title: {}'.format(title))\n",
    "    \n",
    "    q = k = v = w_embeddings(title)\n",
    "    \n",
    "    print('embedded title: \\n{}'.format(q))\n",
    "    print(q.shape)\n",
    "    \n",
    "    title, attn = mh_attn(q, k, v, mask)\n",
    "    \n",
    "    print('transformed title: \\n{}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    title = nff(title)\n",
    "    \n",
    "    print('title after nonlinear ff: \\n {}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    title, importance = add_attn(title, mask)\n",
    "    \n",
    "    print('importance of each word: \\n {}'.format(importance))\n",
    "    print(importance.shape)\n",
    "    \n",
    "    print('title after everything: \\n {}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    \n",
    "    y = title_embedding(z, mask=s)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "\n",
    "transform(torch.LongTensor([[1, 5, 7, 2, 9, 8, 0, 0], [2, 5, 5, 0, 0, 0, 0, 0]]),        \n",
    "                    mask= torch.ByteTensor([[0,0,0,0,0,0,1,1],[0,0,0,1,1,1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiracer",
   "language": "python",
   "name": "wikiracer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
