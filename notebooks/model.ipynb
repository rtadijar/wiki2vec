{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(fname, get_embeddings=True, get_w2i=False, get_i2w=False, skip_first_line=True):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    \n",
    "    if skip_first_line:\n",
    "        fin.readline()\n",
    "    \n",
    "    num_embeddings = 0\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.rstrip().split(' ')\n",
    "        \n",
    "        if get_w2i:\n",
    "            word2idx[line[0]] = num_embeddings\n",
    "        if get_i2w:\n",
    "            idx2word[num_embeddings] = line[0]\n",
    "        if get_embeddings:       \n",
    "            embeddings.append([float(num) for num in line[1:]])\n",
    "        \n",
    "        num_embeddings += 1\n",
    "        \n",
    "        \n",
    "    return torch.FloatTensor(embeddings), word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx = load_embeddings('../embeddings/wiki-news-300d-1M.vec', get_embeddings=False, get_w2i=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "      \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = torch.matmul(q , k.transpose(-2, -1) / math.sqrt(q.size(-1)))\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask.unsqueeze(1) == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))        \n",
    "        res = torch.matmul(attn, v)\n",
    "\n",
    "        return res, attn\n",
    "\n",
    "class AdditiveSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.w = nn.Linear(d_model, d_model)\n",
    "        self.q = torch.nn.Parameter(torch.FloatTensor(d_model).uniform_(-0.1, 0.1))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn = torch.tanh(self.dropout(self.w(x)))        \n",
    "        attn = torch.matmul(attn, self.q)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        \n",
    "        res = torch.einsum('ijk, ij->ik', x, attn)\n",
    "        return res, attn\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_qk, d_v, track_agreement=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_qk = d_qk\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, num_heads * d_v, bias=False)\n",
    "        \n",
    "        self.w_fc = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "        \n",
    "        self.attention = MultiplicativeAttention(dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "        self.track_agreement = track_agreement\n",
    "        self.v_agreement = 0\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):     \n",
    "        batch_size = q.shape[0]\n",
    "        seq_size = q.shape[1]\n",
    "        \n",
    "        q_proj = self.w_q(q).view(q.shape[0], q.shape[1], self.num_heads, self.d_qk)\n",
    "        k_proj = self.w_k(k).view(k.shape[0], k.shape[1], self.num_heads, self.d_qk)\n",
    "        v_proj = self.w_v(v).view(v.shape[0], v.shape[1], self.num_heads, self.d_v) \n",
    "\n",
    "        if self.track_agreement:\n",
    "            self.v_agreement += torch.einsum('bshd, bsnd->', F.normalize(v_proj, dim=3), F.normalize(v_proj, dim=3)) / self.num_heads**2\n",
    "\n",
    "        if mask is None:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2))\n",
    "        else:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2), mask.unsqueeze(1))\n",
    "        \n",
    "        q = q.transpose(1, 2).contiguous()\n",
    "        q = q.view(batch_size, seq_size, -1)\n",
    "\n",
    "        q = self.dropout(self.w_fc(q))\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "        \n",
    "        return q, attn\n",
    "\n",
    "    def clear_agreement(self):\n",
    "        self.v_agreement = 0\n",
    "\n",
    "class NonlinearFF(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class TitleEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, d_model, num_heads, d_qk, d_v, d_hid=None, embeddings=None, track_agreement=False, padding_idx=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(num_embeddings, d_model, sparse=True, padding_idx=0)\n",
    "            \n",
    "        self.mh_attn = MultiHeadAttention(d_model, num_heads, d_qk, d_v, track_agreement=track_agreement, dropout=dropout)\n",
    "        self.nff = NonlinearFF(d_model, d_hid if d_hid is not None else d_model * 4, dropout=dropout)\n",
    "        self.add_attn = AdditiveSelfAttention(d_model, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "    def forward(self, title):    \n",
    "        mask = (title == self.padding_idx).byte()\n",
    "        \n",
    "        q = k = v = self.embeddings(title)\n",
    "        title, attn = self.mh_attn(q, k ,v, mask=mask)\n",
    "        \n",
    "        title = self.nff(title)\n",
    "        title, add_attn = self.add_attn(title, mask=mask)\n",
    "        \n",
    "        title = self.layer_norm(title)\n",
    "        \n",
    "        return title\n",
    "    \n",
    "    def load_embeddings(embeddings):\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self, sample_pool, embedding, num_samples, device='cpu'):\n",
    "        super(NegativeSamplingLoss, self).__init__()\n",
    "        \n",
    "        self.sample_pool = sample_pool\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        sample_indices = torch.FloatTensor(batch_size * self.num_samples).uniform_(0, len(self.sample_pool) - 1).long()        \n",
    "        sample_indices, tmp = torch.broadcast_tensors(sample_indices.unsqueeze(1), \n",
    "                                                      torch.arange(batch_size * self.num_samples * self.sample_pool.shape[1])\n",
    "                                                      .view(batch_size * self.num_samples, self.sample_pool.shape[1]))\n",
    "        \n",
    "        n = torch.gather(self.sample_pool, 0, sample_indices).to(self.device)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        y = self.embedding(y)\n",
    "        n = self.embedding(n).neg()\n",
    "        \n",
    "        target_loss = (torch.einsum('ij,ij->i', x, y) / x.shape[-1]).sigmoid().mean() \n",
    "        noise_loss = (torch.einsum('ij, ikj->ik', x, n.view(batch_size, self.num_samples, n.shape[-1])) / (x.shape[-1])).sigmoid().log().sum(1).mean()\n",
    "        \n",
    "        return -(target_loss + noise_loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.load('../data/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "title_embedding = torch.load('../models/model_reg_init.pt')\n",
    "nss = NegativeSamplingLoss(dataset.tensors[1], title_embedding, 5, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(title_embedding, '../models/model_reg_init.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#title_embedding.to(torch.device('cpu'))\n",
    "#nss.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultipleOptimizer:\n",
    "    def __init__(self, *op):\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "sparse_params = []\n",
    "dense_params = []\n",
    "\n",
    "for name, param in title_embedding.named_parameters():\n",
    "    if name == 'embeddings.weight':\n",
    "        sparse_params.append(param)\n",
    "    else:\n",
    "        dense_params.append(param)\n",
    "        \n",
    "opt_dense = torch.optim.Adam(dense_params, lr=1e-4, weight_decay=0.0001)\n",
    "opt_sparse = torch.optim.SparseAdam(sparse_params, lr=1e-4)\n",
    "\n",
    "optimizer = MultipleOptimizer(opt_sparse, opt_dense)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 64: 66.75123596191406\n",
      "loss at batch 128: 46.99741744995117\n",
      "loss at batch 192: 33.35817337036133\n",
      "loss at batch 256: 26.671594619750977\n",
      "loss at batch 320: 21.124557495117188\n",
      "loss at batch 384: 17.891103744506836\n",
      "loss at batch 448: 14.759486198425293\n",
      "loss at batch 512: 13.05744457244873\n",
      "loss at batch 576: 11.62479019165039\n",
      "loss at batch 640: 10.634286880493164\n",
      "loss at batch 704: 9.653124809265137\n",
      "loss at batch 768: 8.771020889282227\n",
      "loss at batch 832: 8.396596908569336\n",
      "loss at batch 896: 7.720708847045898\n",
      "loss at batch 960: 7.804649353027344\n",
      "loss at batch 1024: 7.037797927856445\n",
      "loss at batch 1088: 6.838610649108887\n",
      "loss at batch 1152: 6.1069536209106445\n",
      "loss at batch 1216: 6.123982906341553\n",
      "loss at batch 1280: 5.810311794281006\n",
      "loss at batch 1344: 5.484839916229248\n",
      "loss at batch 1408: 5.290788173675537\n",
      "loss at batch 1472: 5.220921993255615\n",
      "loss at batch 1536: 5.000507354736328\n",
      "loss at batch 1600: 5.039775371551514\n",
      "loss at batch 1664: 4.806284427642822\n",
      "loss at batch 1728: 4.721552848815918\n",
      "loss at batch 1792: 4.475701332092285\n",
      "loss at batch 1856: 4.5712432861328125\n",
      "loss at batch 1920: 4.341878890991211\n",
      "loss at batch 1984: 4.301982879638672\n",
      "loss at batch 2048: 4.246316909790039\n",
      "loss at batch 2112: 4.07575798034668\n",
      "loss at batch 2176: 4.008073329925537\n",
      "loss at batch 2240: 4.118003845214844\n",
      "loss at batch 2304: 3.899005889892578\n",
      "loss at batch 2368: 3.9052481651306152\n",
      "loss at batch 2432: 3.953732967376709\n",
      "loss at batch 2496: 3.8819618225097656\n",
      "loss at batch 2560: 3.9047422409057617\n",
      "loss at batch 2624: 3.709566831588745\n",
      "loss at batch 2688: 3.6260478496551514\n",
      "loss at batch 2752: 3.525526762008667\n",
      "loss at batch 2816: 3.6104936599731445\n",
      "loss at batch 2880: 3.570736885070801\n",
      "loss at batch 2944: 3.557178020477295\n",
      "loss at batch 3008: 3.472064733505249\n",
      "loss at batch 3072: 3.514535427093506\n",
      "loss at batch 3136: 3.5340962409973145\n",
      "loss at batch 3200: 3.469458818435669\n",
      "loss at batch 3264: 3.4857308864593506\n",
      "loss at batch 3328: 3.3762435913085938\n",
      "loss at batch 3392: 3.4943912029266357\n",
      "loss at batch 3456: 3.343691825866699\n",
      "loss at batch 3520: 3.400756359100342\n",
      "loss at batch 3584: 3.3996901512145996\n",
      "loss at batch 3648: 3.3684046268463135\n",
      "loss at batch 3712: 3.307920217514038\n",
      "loss at batch 3776: 3.290264844894409\n",
      "loss at batch 3840: 3.308666944503784\n",
      "loss at batch 3904: 3.2862093448638916\n",
      "loss at batch 3968: 3.204564094543457\n",
      "loss at batch 4032: 3.219421863555908\n",
      "loss at batch 4096: 3.237508535385132\n",
      "loss at batch 4160: 3.156712293624878\n",
      "loss at batch 4224: 3.1601409912109375\n",
      "loss at batch 4288: 3.1709582805633545\n",
      "loss at batch 4352: 3.166367530822754\n",
      "loss at batch 4416: 3.1859517097473145\n",
      "loss at batch 4480: 3.101407051086426\n",
      "loss at batch 4544: 3.13431453704834\n",
      "loss at batch 4608: 3.136082887649536\n",
      "loss at batch 4672: 3.1398205757141113\n",
      "loss at batch 4736: 3.0423479080200195\n",
      "loss at batch 4800: 3.144763946533203\n",
      "loss at batch 4864: 3.131884813308716\n",
      "loss at batch 4928: 3.0242671966552734\n",
      "loss at batch 4992: 3.1594982147216797\n",
      "loss at batch 5056: 3.0946168899536133\n",
      "loss at batch 5120: 3.0659167766571045\n",
      "loss at batch 5184: 3.034719228744507\n",
      "loss at batch 5248: 3.1158463954925537\n",
      "loss at batch 5312: 3.041459560394287\n",
      "loss at batch 5376: 3.05085825920105\n",
      "loss at batch 5440: 3.0837979316711426\n",
      "loss at batch 5504: 3.0460972785949707\n",
      "loss at batch 5568: 3.093608856201172\n",
      "loss at batch 5632: 3.0080370903015137\n",
      "loss at batch 5696: 3.040807008743286\n",
      "loss at batch 5760: 3.083322763442993\n",
      "loss at batch 5824: 3.048954725265503\n",
      "loss at batch 5888: 2.99301815032959\n",
      "loss at batch 5952: 3.0282583236694336\n",
      "loss at batch 6016: 2.968787908554077\n",
      "loss at batch 6080: 2.989060640335083\n",
      "loss at batch 6144: 3.0422215461730957\n",
      "loss at batch 6208: 2.947441339492798\n",
      "loss at batch 6272: 2.9898273944854736\n",
      "loss at batch 6336: 2.9726505279541016\n",
      "loss at batch 6400: 2.954799175262451\n",
      "loss at batch 6464: 2.9457976818084717\n",
      "loss at batch 6528: 2.943328857421875\n",
      "loss at batch 6592: 3.001824378967285\n",
      "loss at batch 6656: 2.9755146503448486\n",
      "loss at batch 6720: 2.9499218463897705\n",
      "loss at batch 6784: 2.871840000152588\n",
      "loss at batch 6848: 3.0098190307617188\n",
      "loss at batch 6912: 3.0005381107330322\n",
      "loss at batch 6976: 2.946437120437622\n",
      "loss at batch 7040: 3.034959316253662\n",
      "loss at batch 7104: 2.9472806453704834\n",
      "loss at batch 7168: 2.9375932216644287\n",
      "loss at batch 7232: 2.920525074005127\n",
      "loss at batch 7296: 2.966153860092163\n",
      "loss at batch 7360: 2.9575998783111572\n",
      "loss at batch 7424: 3.032688617706299\n",
      "loss at batch 7488: 2.9736311435699463\n",
      "loss at batch 7552: 2.914367914199829\n",
      "loss at batch 7616: 2.899851083755493\n",
      "loss at batch 7680: 2.9117677211761475\n",
      "loss at batch 7744: 2.943696975708008\n",
      "loss at batch 7808: 2.959540843963623\n",
      "loss at batch 7872: 2.862236261367798\n",
      "loss at batch 7936: 2.9519002437591553\n",
      "loss at batch 8000: 2.9378066062927246\n",
      "loss at batch 8064: 2.9566915035247803\n",
      "loss at batch 8128: 2.9776690006256104\n",
      "loss at batch 8192: 2.9041106700897217\n",
      "loss at batch 8256: 2.985490322113037\n",
      "loss at batch 8320: 2.9782049655914307\n",
      "loss at batch 8384: 2.9176344871520996\n",
      "loss at batch 8448: 2.904169797897339\n",
      "loss at batch 8512: 2.9411463737487793\n",
      "loss at batch 8576: 2.900059461593628\n",
      "loss at batch 8640: 2.9608726501464844\n",
      "loss at batch 8704: 2.9317917823791504\n",
      "loss at batch 8768: 2.9379239082336426\n",
      "loss at batch 8832: 2.8640623092651367\n",
      "loss at batch 8896: 2.9198222160339355\n",
      "loss at batch 8960: 2.9880597591400146\n",
      "loss at batch 9024: 2.891418218612671\n",
      "loss at batch 9088: 2.8540172576904297\n",
      "loss at batch 9152: 2.865614414215088\n",
      "loss at batch 9216: 3.0067672729492188\n",
      "loss at batch 9280: 2.857085704803467\n",
      "loss at batch 9344: 3.003779172897339\n",
      "loss at batch 9408: 2.9030730724334717\n",
      "loss at batch 9472: 2.9530529975891113\n",
      "loss at batch 9536: 2.941135883331299\n",
      "loss at batch 9600: 2.9556455612182617\n",
      "loss at batch 9664: 2.9430296421051025\n",
      "loss at batch 9728: 2.896643877029419\n",
      "loss at batch 9792: 2.884793758392334\n",
      "loss at batch 9856: 2.863891363143921\n",
      "loss at batch 9920: 2.8448734283447266\n",
      "loss at batch 9984: 2.883716583251953\n",
      "loss at batch 10048: 2.9734134674072266\n",
      "loss at batch 10112: 2.911653757095337\n",
      "loss at batch 10176: 2.926306962966919\n",
      "loss at batch 10240: 2.934053659439087\n",
      "loss at batch 10304: 2.892934560775757\n",
      "loss at batch 10368: 3.005408763885498\n",
      "loss at batch 10432: 2.9604604244232178\n",
      "loss at batch 10496: 2.910914659500122\n",
      "loss at batch 10560: 2.897552251815796\n",
      "loss at batch 10624: 2.9110660552978516\n",
      "loss at batch 10688: 2.94146466255188\n",
      "loss at batch 10752: 2.8597936630249023\n",
      "loss at batch 10816: 2.935421943664551\n",
      "loss at batch 10880: 2.89583420753479\n",
      "loss at batch 10944: 2.8057749271392822\n",
      "loss at batch 11008: 2.84090256690979\n",
      "loss at batch 11072: 2.849504232406616\n",
      "loss at batch 11136: 2.8671624660491943\n",
      "loss at batch 11200: 2.8909096717834473\n",
      "loss at batch 11264: 2.8365190029144287\n",
      "loss at batch 11328: 2.9826629161834717\n",
      "loss at batch 11392: 2.930863857269287\n",
      "loss at batch 11456: 2.831523895263672\n",
      "loss at batch 11520: 2.944061517715454\n",
      "loss at batch 11584: 2.9723503589630127\n",
      "loss at batch 11648: 2.838902711868286\n",
      "loss at batch 11712: 2.8929646015167236\n",
      "loss at batch 11776: 2.930715799331665\n",
      "loss at batch 11840: 2.904308319091797\n",
      "loss at batch 11904: 2.8292038440704346\n",
      "loss at batch 11968: 2.9264116287231445\n",
      "loss at batch 12032: 2.8764493465423584\n",
      "loss at batch 12096: 2.834167242050171\n",
      "loss at batch 12160: 2.9152841567993164\n",
      "loss at batch 12224: 2.9189651012420654\n",
      "loss at batch 12288: 2.895308017730713\n",
      "loss at batch 12352: 2.9117484092712402\n",
      "loss at batch 12416: 2.9189376831054688\n",
      "loss at batch 12480: 2.8525450229644775\n",
      "loss at batch 12544: 2.860658884048462\n",
      "loss at batch 12608: 2.8664538860321045\n",
      "loss at batch 12672: 2.844944477081299\n",
      "loss at batch 12736: 2.8744466304779053\n",
      "loss at batch 12800: 2.8879551887512207\n",
      "loss at batch 12864: 2.830803394317627\n",
      "loss at batch 12928: 2.878628730773926\n",
      "loss at batch 12992: 2.9192748069763184\n",
      "loss at batch 13056: 2.829660654067993\n",
      "loss at batch 13120: 2.9229462146759033\n",
      "loss at batch 13184: 2.862213134765625\n",
      "loss at batch 13248: 2.878803253173828\n",
      "loss at batch 13312: 2.8517069816589355\n",
      "loss at batch 13376: 2.8562135696411133\n",
      "loss at batch 13440: 2.7944905757904053\n",
      "loss at batch 13504: 2.971189022064209\n",
      "loss at batch 13568: 2.8808438777923584\n",
      "loss at batch 13632: 2.870760440826416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 13696: 2.816551923751831\n",
      "loss at batch 13760: 2.8801352977752686\n",
      "loss at batch 13824: 2.9046051502227783\n",
      "loss at batch 13888: 2.9112181663513184\n",
      "loss at batch 13952: 2.855391502380371\n",
      "loss at batch 14016: 2.899574041366577\n",
      "loss at batch 14080: 2.8524420261383057\n",
      "loss at batch 14144: 2.88330340385437\n",
      "loss at batch 14208: 2.92085337638855\n",
      "loss at batch 14272: 2.88553524017334\n",
      "loss at batch 14336: 2.8869450092315674\n",
      "loss at batch 14400: 2.81593656539917\n",
      "loss at batch 14464: 2.8988704681396484\n",
      "loss at batch 14528: 2.7620739936828613\n",
      "loss at batch 14592: 2.8919174671173096\n",
      "loss at batch 14656: 3.039677619934082\n",
      "loss at batch 14720: 2.925189733505249\n",
      "loss at batch 14784: 2.9111697673797607\n",
      "loss at batch 14848: 2.8044509887695312\n",
      "loss at batch 14912: 2.9497694969177246\n",
      "loss at batch 14976: 2.844222068786621\n",
      "loss at batch 15040: 2.9476261138916016\n",
      "loss at batch 15104: 2.8522894382476807\n",
      "loss at batch 15168: 2.8199918270111084\n",
      "loss at batch 15232: 2.910844087600708\n",
      "loss at batch 15296: 2.9944002628326416\n",
      "loss at batch 15360: 2.8308839797973633\n",
      "loss at batch 15424: 2.887718915939331\n",
      "loss at batch 15488: 2.873684883117676\n",
      "loss at batch 15552: 2.7621443271636963\n",
      "loss at batch 15616: 2.9157347679138184\n",
      "loss at batch 15680: 2.887655735015869\n",
      "loss at batch 15744: 2.8688299655914307\n",
      "loss at batch 15808: 2.9281578063964844\n",
      "loss at batch 15872: 2.8565189838409424\n",
      "loss at batch 15936: 2.959913730621338\n",
      "loss at batch 16000: 2.790144205093384\n",
      "loss at batch 16064: 2.888078212738037\n",
      "loss at batch 16128: 2.9555110931396484\n",
      "loss at batch 16192: 2.948934555053711\n",
      "loss at batch 16256: 2.849907398223877\n",
      "loss at batch 16320: 2.8589537143707275\n",
      "loss at batch 16384: 2.844158172607422\n",
      "loss at batch 16448: 2.820310115814209\n",
      "loss at batch 16512: 2.828030824661255\n",
      "loss at batch 16576: 2.905163049697876\n",
      "loss at batch 16640: 2.794802188873291\n",
      "loss at batch 16704: 2.864806652069092\n",
      "loss at batch 16768: 2.808239459991455\n",
      "loss at batch 16832: 2.8951876163482666\n",
      "loss at batch 16896: 2.850667953491211\n",
      "loss at batch 16960: 2.7642784118652344\n",
      "loss at batch 17024: 2.9021494388580322\n",
      "loss at batch 17088: 2.8489859104156494\n",
      "loss at batch 17152: 2.8600010871887207\n",
      "loss at batch 17216: 2.8613734245300293\n",
      "loss at batch 17280: 2.935105800628662\n",
      "loss at batch 17344: 2.873183012008667\n",
      "loss at batch 17408: 2.7975516319274902\n",
      "loss at batch 17472: 2.8513622283935547\n",
      "loss at batch 17536: 2.8460683822631836\n",
      "loss at batch 17600: 2.8167572021484375\n",
      "loss at batch 17664: 2.8835482597351074\n",
      "loss at batch 17728: 2.742021322250366\n",
      "loss at batch 17792: 2.8748903274536133\n",
      "loss at batch 17856: 2.8635973930358887\n",
      "loss at batch 17920: 2.8341355323791504\n",
      "loss at batch 17984: 2.8665120601654053\n",
      "loss at batch 18048: 2.8236756324768066\n",
      "loss at batch 18112: 2.7890806198120117\n",
      "loss at batch 18176: 2.916550397872925\n",
      "loss at batch 18240: 2.8012466430664062\n",
      "loss at batch 18304: 2.856550931930542\n",
      "loss at batch 18368: 2.8434042930603027\n",
      "loss at batch 18432: 2.869936466217041\n",
      "loss at batch 18496: 2.8599472045898438\n",
      "loss at batch 18560: 2.988150119781494\n",
      "loss at batch 18624: 2.805781126022339\n",
      "loss at batch 18688: 2.854393482208252\n",
      "loss at batch 18752: 2.8353264331817627\n",
      "loss at batch 18816: 2.7587268352508545\n",
      "loss at batch 18880: 2.7068448066711426\n",
      "loss at batch 18944: 2.854214906692505\n",
      "loss at batch 19008: 2.842683792114258\n",
      "loss at batch 19072: 2.821335554122925\n",
      "loss at batch 19136: 2.9171535968780518\n",
      "loss at batch 19200: 2.8547604084014893\n",
      "loss at batch 19264: 2.8229427337646484\n",
      "loss at batch 19328: 2.751431465148926\n",
      "loss at batch 19392: 2.8002939224243164\n",
      "loss at batch 19456: 2.89487886428833\n",
      "loss at batch 19520: 2.8764407634735107\n",
      "loss at batch 19584: 2.8706235885620117\n",
      "loss at batch 19648: 2.917926788330078\n",
      "loss at batch 19712: 2.9142093658447266\n",
      "loss at batch 19776: 2.962247610092163\n",
      "loss at batch 19840: 2.9215140342712402\n",
      "loss at batch 19904: 2.8010191917419434\n",
      "loss at batch 19968: 2.907050848007202\n",
      "loss at batch 20032: 2.857112407684326\n",
      "loss at batch 20096: 2.857419490814209\n",
      "loss at batch 20160: 2.7891600131988525\n",
      "loss at batch 20224: 2.7953383922576904\n",
      "loss at batch 20288: 2.871147394180298\n",
      "loss at batch 20352: 2.809129238128662\n",
      "loss at batch 20416: 2.846222162246704\n",
      "loss at batch 20480: 2.8704066276550293\n",
      "loss at batch 20544: 2.8524718284606934\n",
      "loss at batch 20608: 2.8269295692443848\n",
      "loss at batch 20672: 2.8832201957702637\n",
      "loss at batch 20736: 2.8707637786865234\n",
      "loss at batch 20800: 2.9033780097961426\n",
      "loss at batch 20864: 2.848280668258667\n",
      "loss at batch 20928: 2.8409430980682373\n",
      "loss at batch 20992: 2.8239455223083496\n",
      "loss at batch 21056: 2.8820736408233643\n",
      "loss at batch 21120: 2.8459079265594482\n",
      "loss at batch 21184: 2.8421196937561035\n",
      "loss at batch 21248: 2.805060863494873\n",
      "loss at batch 21312: 2.87198805809021\n",
      "loss at batch 21376: 2.8973639011383057\n",
      "loss at batch 21440: 2.8625099658966064\n",
      "loss at batch 21504: 2.8637707233428955\n",
      "loss at batch 21568: 2.867482900619507\n",
      "loss at batch 21632: 2.9069857597351074\n",
      "loss at batch 21696: 2.8347291946411133\n",
      "loss at batch 21760: 2.826456308364868\n",
      "loss at batch 21824: 2.8653788566589355\n",
      "loss at batch 21888: 2.8226962089538574\n",
      "loss at batch 21952: 2.9235825538635254\n",
      "loss at batch 22016: 2.784821033477783\n",
      "loss at batch 22080: 2.8528566360473633\n",
      "loss at batch 22144: 2.862048864364624\n",
      "loss at batch 22208: 2.889601945877075\n",
      "loss at batch 22272: 2.8288304805755615\n",
      "loss at batch 22336: 2.765451669692993\n",
      "loss at batch 22400: 2.7780044078826904\n",
      "loss at batch 22464: 2.8400990962982178\n",
      "loss at batch 22528: 2.9034740924835205\n",
      "loss at batch 22592: 2.8485934734344482\n",
      "loss at batch 22656: 2.9425740242004395\n",
      "loss at batch 22720: 2.892972469329834\n",
      "loss at batch 22784: 2.7828402519226074\n",
      "loss at batch 22848: 2.827855110168457\n",
      "loss at batch 22912: 2.902745246887207\n",
      "loss at batch 22976: 2.8777031898498535\n",
      "loss at batch 23040: 2.861006259918213\n",
      "loss at batch 23104: 2.873798131942749\n",
      "loss at batch 23168: 2.877366542816162\n",
      "loss at batch 23232: 2.8143181800842285\n",
      "loss at batch 23296: 2.872119903564453\n",
      "loss at batch 23360: 2.857952356338501\n",
      "loss at batch 23424: 2.7993311882019043\n",
      "loss at batch 23488: 2.9226701259613037\n",
      "loss at batch 23552: 2.85929799079895\n",
      "loss at batch 23616: 2.8788607120513916\n",
      "loss at batch 23680: 2.912478446960449\n",
      "loss at batch 23744: 2.781205415725708\n",
      "loss at batch 23808: 2.8258492946624756\n",
      "loss at batch 23872: 2.9766833782196045\n",
      "loss at batch 23936: 2.813969373703003\n",
      "loss at batch 24000: 2.942720890045166\n",
      "loss at batch 24064: 2.877920150756836\n",
      "loss at batch 24128: 2.752432107925415\n",
      "loss at batch 24192: 2.894211530685425\n",
      "loss at batch 24256: 2.9083969593048096\n",
      "loss at batch 24320: 2.8409149646759033\n",
      "loss at batch 24384: 2.8689322471618652\n",
      "loss at batch 24448: 2.8794806003570557\n",
      "loss at batch 24512: 2.8258092403411865\n",
      "loss at batch 24576: 2.782680034637451\n",
      "loss at batch 24640: 2.777554988861084\n",
      "loss at batch 24704: 2.8024027347564697\n",
      "loss at batch 24768: 2.8041117191314697\n",
      "loss at batch 24832: 2.8351612091064453\n",
      "loss at batch 24896: 2.819563865661621\n",
      "loss at batch 24960: 2.8224294185638428\n",
      "loss at batch 25024: 2.7811572551727295\n",
      "loss at batch 25088: 2.8114280700683594\n",
      "loss at batch 25152: 2.930269241333008\n",
      "loss at batch 25216: 2.9279072284698486\n",
      "loss at batch 25280: 2.859964609146118\n",
      "loss at batch 25344: 2.8273117542266846\n",
      "loss at batch 25408: 2.814335823059082\n",
      "loss at batch 25472: 2.8255202770233154\n",
      "loss at batch 25536: 2.812669277191162\n",
      "loss at batch 25600: 2.8739664554595947\n",
      "loss at batch 25664: 2.839693069458008\n",
      "loss at batch 25728: 2.871358871459961\n",
      "loss at batch 25792: 2.8278942108154297\n",
      "loss at batch 25856: 2.8936564922332764\n",
      "loss at batch 25920: 2.816007137298584\n",
      "loss at batch 25984: 2.8610212802886963\n",
      "loss at batch 26048: 2.933647632598877\n",
      "loss at batch 26112: 2.8299307823181152\n",
      "loss at batch 26176: 2.7394614219665527\n",
      "loss at batch 26240: 2.8549551963806152\n",
      "loss at batch 26304: 2.917320966720581\n",
      "loss at batch 26368: 2.7946696281433105\n",
      "loss at batch 26432: 2.70609712600708\n",
      "loss at batch 26496: 2.906621217727661\n",
      "loss at batch 26560: 2.793769121170044\n",
      "loss at batch 26624: 2.812788963317871\n",
      "loss at batch 26688: 2.8788418769836426\n",
      "loss at batch 26752: 2.8955540657043457\n",
      "loss at batch 26816: 2.781420946121216\n",
      "loss at batch 26880: 2.74352765083313\n",
      "loss at batch 26944: 2.834529399871826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 27008: 2.7816121578216553\n",
      "loss at batch 27072: 2.727949857711792\n",
      "loss at batch 27136: 2.8570003509521484\n",
      "loss at batch 27200: 2.8516318798065186\n",
      "loss at batch 27264: 2.810133218765259\n",
      "loss at batch 27328: 2.765998125076294\n",
      "loss at batch 27392: 2.8323867321014404\n",
      "loss at batch 27456: 2.8149282932281494\n",
      "loss at batch 27520: 2.7756378650665283\n",
      "loss at batch 27584: 2.8505735397338867\n",
      "loss at batch 27648: 2.8033416271209717\n",
      "loss at batch 27712: 2.823866128921509\n",
      "loss at batch 27776: 2.848714590072632\n",
      "loss at batch 27840: 2.7931625843048096\n",
      "loss at batch 27904: 2.9010963439941406\n",
      "loss at batch 27968: 2.7272305488586426\n",
      "loss at batch 28032: 2.766465187072754\n",
      "loss at batch 28096: 2.894645929336548\n",
      "loss at batch 28160: 2.7393741607666016\n",
      "loss at batch 28224: 2.7678189277648926\n",
      "loss at batch 28288: 2.773531198501587\n",
      "loss at batch 28352: 2.8636231422424316\n",
      "loss at batch 28416: 2.844917058944702\n",
      "loss at batch 28480: 2.769022226333618\n",
      "loss at batch 28544: 2.847860336303711\n",
      "loss at batch 28608: 2.7838306427001953\n",
      "loss at batch 28672: 2.7992234230041504\n",
      "loss at batch 28736: 2.775782585144043\n",
      "loss at batch 28800: 2.8236119747161865\n",
      "loss at batch 28864: 2.736150026321411\n",
      "loss at batch 28928: 2.7849960327148438\n",
      "loss at batch 28992: 2.8106532096862793\n",
      "loss at batch 29056: 2.707226514816284\n",
      "loss at batch 29120: 2.879945993423462\n",
      "loss at batch 29184: 2.8495099544525146\n",
      "loss at batch 29248: 2.8246219158172607\n",
      "loss at batch 29312: 2.889263868331909\n",
      "loss at batch 29376: 2.957691192626953\n",
      "loss at batch 29440: 2.823133945465088\n",
      "loss at batch 29504: 2.8525307178497314\n",
      "loss at batch 29568: 2.845778465270996\n",
      "loss at batch 29632: 2.808685541152954\n",
      "loss at batch 29696: 2.8160529136657715\n",
      "loss at batch 29760: 2.947521448135376\n",
      "loss at batch 29824: 2.9270315170288086\n",
      "loss at batch 29888: 2.7523186206817627\n",
      "loss at batch 29952: 2.8271689414978027\n",
      "loss at batch 30016: 2.784345865249634\n",
      "loss at batch 30080: 2.822805166244507\n",
      "loss at batch 30144: 2.8279638290405273\n",
      "loss at batch 30208: 2.942194938659668\n",
      "loss at batch 30272: 2.7741615772247314\n",
      "loss at batch 30336: 2.783459424972534\n",
      "loss at batch 30400: 2.829148292541504\n",
      "loss at batch 30464: 2.8310282230377197\n",
      "loss at batch 30528: 2.8927247524261475\n",
      "loss at batch 30592: 2.7860822677612305\n",
      "loss at batch 30656: 2.829163074493408\n",
      "loss at batch 30720: 2.9768898487091064\n",
      "loss at batch 30784: 2.7662341594696045\n",
      "loss at batch 30848: 2.833705186843872\n",
      "loss at batch 30912: 2.8086113929748535\n",
      "loss at batch 30976: 2.749803304672241\n",
      "loss at batch 31040: 2.812885284423828\n",
      "loss at batch 31104: 2.783639907836914\n",
      "loss at batch 31168: 2.8456287384033203\n",
      "loss at batch 31232: 2.8471288681030273\n",
      "loss at batch 31296: 2.8251945972442627\n",
      "loss at batch 31360: 2.850844383239746\n",
      "loss at batch 31424: 2.82480525970459\n",
      "loss at batch 31488: 2.90878963470459\n",
      "loss at batch 31552: 2.919152021408081\n",
      "loss at batch 31616: 2.8697783946990967\n",
      "loss at batch 31680: 2.7585806846618652\n",
      "loss at batch 31744: 2.820546865463257\n",
      "loss at batch 31808: 2.8532257080078125\n",
      "loss at batch 31872: 2.7698559761047363\n",
      "loss at batch 31936: 2.8226828575134277\n",
      "loss at batch 32000: 2.835905075073242\n",
      "loss at batch 32064: 2.683285713195801\n",
      "loss at batch 32128: 2.721813917160034\n",
      "loss at batch 32192: 2.786102771759033\n",
      "loss at batch 32256: 2.908607244491577\n",
      "loss at batch 32320: 2.897844076156616\n",
      "loss at batch 32384: 2.7641866207122803\n",
      "loss at batch 32448: 2.7730300426483154\n",
      "loss at batch 32512: 2.866529703140259\n",
      "loss at batch 32576: 2.8658347129821777\n",
      "loss at batch 32640: 2.962143898010254\n",
      "loss at batch 32704: 2.8557353019714355\n",
      "loss at batch 32768: 2.8399484157562256\n",
      "loss at batch 32832: 2.788973808288574\n",
      "loss at batch 32896: 2.8105616569519043\n",
      "loss at batch 32960: 2.8059804439544678\n",
      "loss at batch 33024: 2.7049002647399902\n",
      "loss at batch 33088: 2.8717174530029297\n",
      "loss at batch 33152: 2.7543067932128906\n",
      "loss at batch 33216: 2.795778751373291\n",
      "loss at batch 33280: 2.792747735977173\n",
      "loss at batch 33344: 2.8814711570739746\n",
      "loss at batch 33408: 2.718364715576172\n",
      "loss at batch 33472: 2.801234245300293\n",
      "loss at batch 33536: 2.772366523742676\n",
      "loss at batch 33600: 2.8233320713043213\n",
      "loss at batch 33664: 2.8196640014648438\n",
      "loss at batch 33728: 2.8631772994995117\n",
      "loss at batch 33792: 2.750152349472046\n",
      "loss at batch 33856: 2.804504632949829\n",
      "loss at batch 33920: 2.782484292984009\n",
      "loss at batch 33984: 2.8956239223480225\n",
      "loss at batch 34048: 2.890484571456909\n",
      "loss at batch 34112: 2.763451099395752\n",
      "loss at batch 34176: 2.8137993812561035\n",
      "loss at batch 34240: 2.814194679260254\n",
      "loss at batch 34304: 2.846968173980713\n",
      "loss at batch 34368: 2.71418833732605\n",
      "loss at batch 34432: 2.8188271522521973\n",
      "loss at batch 34496: 2.882568836212158\n",
      "loss at batch 34560: 2.8161051273345947\n",
      "loss at batch 34624: 2.880289316177368\n",
      "loss at batch 34688: 2.7305586338043213\n",
      "loss at batch 34752: 2.8398420810699463\n",
      "loss at batch 34816: 2.8106448650360107\n",
      "loss at batch 34880: 2.820301055908203\n",
      "loss at batch 34944: 2.7898828983306885\n",
      "loss at batch 35008: 2.7774736881256104\n",
      "loss at batch 35072: 2.8787474632263184\n",
      "loss at batch 35136: 2.7152373790740967\n",
      "loss at batch 35200: 2.9038541316986084\n",
      "loss at batch 35264: 2.789616584777832\n",
      "loss at batch 35328: 2.8419065475463867\n",
      "loss at batch 35392: 2.880875825881958\n",
      "loss at batch 35456: 2.842895269393921\n",
      "loss at batch 35520: 2.8215231895446777\n",
      "loss at batch 35584: 2.7955148220062256\n",
      "loss at batch 35648: 2.900071620941162\n",
      "loss at batch 35712: 2.8529140949249268\n",
      "loss at batch 35776: 2.7597744464874268\n",
      "loss at batch 35840: 2.833652973175049\n",
      "loss at batch 35904: 2.6934847831726074\n",
      "loss at batch 35968: 2.7722442150115967\n",
      "loss at batch 36032: 2.9646799564361572\n",
      "loss at batch 36096: 2.789541006088257\n",
      "loss at batch 36160: 2.8565492630004883\n",
      "loss at batch 36224: 2.7562475204467773\n",
      "loss at batch 36288: 2.7483603954315186\n",
      "loss at batch 36352: 2.736423969268799\n",
      "loss at batch 36416: 2.743398427963257\n",
      "loss at batch 36480: 2.8427658081054688\n",
      "loss at batch 36544: 2.8236398696899414\n",
      "loss at batch 36608: 2.7485907077789307\n",
      "loss at batch 36672: 2.868704080581665\n",
      "loss at batch 36736: 2.8304061889648438\n",
      "loss at batch 36800: 2.7678112983703613\n",
      "loss at batch 36864: 2.7464914321899414\n",
      "loss at batch 36928: 2.7870025634765625\n",
      "loss at batch 36992: 2.87595272064209\n",
      "loss at batch 37056: 2.7991645336151123\n",
      "loss at batch 37120: 2.8366751670837402\n",
      "loss at batch 37184: 2.842921018600464\n",
      "loss at batch 37248: 2.70065975189209\n",
      "loss at batch 37312: 2.7646727561950684\n",
      "loss at batch 37376: 2.894657850265503\n",
      "loss at batch 37440: 2.8335018157958984\n",
      "loss at batch 37504: 2.778245449066162\n",
      "loss at batch 37568: 2.763246774673462\n",
      "loss at batch 37632: 2.82458233833313\n",
      "loss at batch 37696: 2.8389997482299805\n",
      "loss at batch 37760: 2.7789394855499268\n",
      "loss at batch 37824: 2.7190539836883545\n",
      "loss at batch 37888: 2.738088846206665\n",
      "loss at batch 37952: 2.87882924079895\n",
      "loss at batch 38016: 2.7386584281921387\n",
      "loss at batch 38080: 2.7868783473968506\n",
      "loss at batch 38144: 2.7234723567962646\n",
      "loss at batch 38208: 2.736506223678589\n",
      "loss at batch 38272: 2.7458953857421875\n",
      "loss at batch 38336: 2.7759852409362793\n",
      "loss at batch 38400: 2.821484088897705\n",
      "loss at batch 38464: 2.7818331718444824\n",
      "loss at batch 38528: 2.799237012863159\n",
      "loss at batch 38592: 2.781982660293579\n",
      "loss at batch 38656: 2.7556161880493164\n",
      "loss at batch 38720: 2.9262290000915527\n",
      "loss at batch 38784: 2.7643091678619385\n",
      "loss at batch 38848: 2.769320011138916\n",
      "loss at batch 38912: 2.898886203765869\n",
      "loss at batch 38976: 2.756171226501465\n",
      "loss at batch 39040: 2.7595062255859375\n",
      "loss at batch 39104: 2.8007335662841797\n",
      "loss at batch 39168: 2.778505325317383\n",
      "loss at batch 39232: 2.809936761856079\n",
      "loss at batch 39296: 2.813479423522949\n",
      "loss at batch 39360: 2.8724474906921387\n",
      "loss at batch 39424: 2.8528242111206055\n",
      "loss at batch 39488: 2.7543933391571045\n",
      "loss at batch 39552: 2.7318427562713623\n",
      "loss at batch 39616: 2.8822343349456787\n",
      "loss at batch 39680: 2.7293694019317627\n",
      "loss at batch 39744: 2.7300682067871094\n",
      "loss at batch 39808: 2.7450501918792725\n",
      "loss at batch 39872: 2.7626712322235107\n",
      "loss at batch 39936: 2.924996852874756\n",
      "loss at batch 40000: 2.83953595161438\n",
      "loss at batch 40064: 2.7970621585845947\n",
      "loss at batch 40128: 2.737748861312866\n",
      "loss at batch 40192: 2.7614636421203613\n",
      "loss at batch 40256: 2.8221826553344727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 40320: 2.8124825954437256\n",
      "loss at batch 40384: 2.8456873893737793\n",
      "loss at batch 40448: 2.7978808879852295\n",
      "loss at batch 40512: 2.763720750808716\n",
      "loss at batch 40576: 2.817284345626831\n",
      "loss at batch 40640: 2.7488749027252197\n",
      "loss at batch 40704: 2.700432062149048\n",
      "loss at batch 40768: 2.791775941848755\n",
      "loss at batch 40832: 2.9596965312957764\n",
      "loss at batch 40896: 2.7570412158966064\n",
      "loss at batch 40960: 2.791029214859009\n",
      "loss at batch 41024: 2.91772198677063\n",
      "loss at batch 41088: 2.717960834503174\n",
      "loss at batch 41152: 2.8026070594787598\n",
      "loss at batch 41216: 2.7738583087921143\n",
      "loss at batch 41280: 2.881950855255127\n",
      "loss at batch 41344: 2.7257463932037354\n",
      "loss at batch 41408: 2.7600693702697754\n",
      "loss at batch 41472: 2.782583236694336\n",
      "loss at batch 41536: 2.771139144897461\n",
      "loss at batch 41600: 2.9173591136932373\n",
      "loss at batch 41664: 2.7050440311431885\n",
      "loss at batch 41728: 2.819368362426758\n",
      "loss at batch 41792: 2.669616222381592\n",
      "loss at batch 41856: 2.784446954727173\n",
      "loss at batch 41920: 2.780710458755493\n",
      "loss at batch 41984: 2.9139211177825928\n",
      "loss at batch 42048: 2.844968318939209\n",
      "loss at batch 42112: 2.770293712615967\n",
      "loss at batch 42176: 2.8921549320220947\n",
      "loss at batch 42240: 2.9345505237579346\n",
      "loss at batch 42304: 2.781829833984375\n",
      "loss at batch 42368: 2.7776012420654297\n",
      "loss at batch 42432: 2.718759298324585\n",
      "loss at batch 42496: 2.890831708908081\n",
      "loss at batch 42560: 2.741516590118408\n",
      "loss at batch 42624: 2.85079288482666\n",
      "loss at batch 42688: 2.7214205265045166\n",
      "loss at batch 42752: 2.7825963497161865\n",
      "loss at batch 42816: 2.725642681121826\n",
      "loss at batch 42880: 2.816277027130127\n",
      "loss at batch 42944: 2.888845920562744\n",
      "loss at batch 43008: 2.726966142654419\n",
      "loss at batch 43072: 2.798180103302002\n",
      "loss at batch 43136: 2.721082925796509\n",
      "loss at batch 43200: 2.772848606109619\n",
      "loss at batch 43264: 2.6960182189941406\n",
      "loss at batch 43328: 2.8628783226013184\n",
      "loss at batch 43392: 2.6920230388641357\n",
      "loss at batch 43456: 2.897761583328247\n",
      "loss at batch 43520: 2.683189868927002\n",
      "loss at batch 43584: 2.7266600131988525\n",
      "loss at batch 43648: 2.790194034576416\n",
      "loss at batch 43712: 2.6996896266937256\n",
      "loss at batch 43776: 2.7255361080169678\n",
      "loss at batch 43840: 2.7219507694244385\n",
      "loss at batch 43904: 2.855482339859009\n",
      "loss at batch 43968: 2.7104852199554443\n",
      "loss at batch 44032: 2.732788562774658\n",
      "loss at batch 44096: 2.743196964263916\n",
      "loss at batch 44160: 2.698770761489868\n",
      "loss at batch 44224: 2.681765556335449\n",
      "loss at batch 44288: 2.695812940597534\n",
      "loss at batch 44352: 2.806472063064575\n",
      "loss at batch 44416: 2.769578456878662\n",
      "loss at batch 44480: 2.8665826320648193\n",
      "loss at batch 44544: 2.665898561477661\n",
      "loss at batch 44608: 2.6858508586883545\n",
      "loss at batch 44672: 2.770163059234619\n",
      "loss at batch 44736: 2.824228525161743\n",
      "loss at batch 44800: 2.8939318656921387\n",
      "loss at batch 44864: 2.8422329425811768\n",
      "loss at batch 44928: 2.8947057723999023\n",
      "loss at batch 44992: 2.842862367630005\n",
      "loss at batch 45056: 2.7958929538726807\n",
      "loss at batch 45120: 2.7146072387695312\n",
      "loss at batch 45184: 2.8987526893615723\n",
      "loss at batch 45248: 2.7867231369018555\n",
      "loss at batch 45312: 2.7352194786071777\n",
      "loss at batch 45376: 2.7548763751983643\n",
      "loss at batch 45440: 2.9402849674224854\n",
      "loss at batch 45504: 2.774773359298706\n",
      "loss at batch 45568: 2.7801108360290527\n",
      "loss at batch 45632: 2.7785427570343018\n",
      "loss at batch 45696: 2.874483108520508\n",
      "loss at batch 45760: 2.8983781337738037\n",
      "loss at batch 45824: 2.6857683658599854\n",
      "loss at batch 45888: 2.8089330196380615\n",
      "loss at batch 45952: 2.7855656147003174\n",
      "loss at batch 46016: 2.822439670562744\n",
      "loss at batch 46080: 2.749805212020874\n",
      "loss at batch 46144: 2.793950080871582\n",
      "loss at batch 46208: 2.7367894649505615\n",
      "loss at batch 46272: 2.722893238067627\n",
      "loss at batch 46336: 2.7939140796661377\n",
      "loss at batch 46400: 2.702511787414551\n",
      "loss at batch 46464: 2.8169922828674316\n",
      "loss at batch 46528: 2.8751120567321777\n",
      "loss at batch 46592: 2.8164901733398438\n",
      "loss at batch 46656: 2.6655850410461426\n",
      "loss at batch 46720: 2.741460084915161\n",
      "loss at batch 46784: 2.7397990226745605\n",
      "loss at batch 46848: 2.7619216442108154\n",
      "loss at batch 46912: 2.7082414627075195\n",
      "loss at batch 46976: 2.6978819370269775\n",
      "loss at batch 47040: 2.7340519428253174\n",
      "loss at batch 47104: 2.788405179977417\n",
      "loss at batch 47168: 2.7585291862487793\n",
      "loss at batch 47232: 2.8084299564361572\n",
      "loss at batch 47296: 2.912022352218628\n",
      "loss at batch 47360: 2.89986515045166\n",
      "loss at batch 47424: 2.8025894165039062\n",
      "loss at batch 47488: 2.757986307144165\n",
      "loss at batch 47552: 2.7771081924438477\n",
      "loss at batch 47616: 2.716552972793579\n",
      "loss at batch 47680: 2.6963701248168945\n",
      "loss at batch 47744: 2.770663022994995\n",
      "loss at batch 47808: 2.738145589828491\n",
      "loss at batch 47872: 2.6508376598358154\n",
      "loss at batch 47936: 2.842039108276367\n",
      "loss at batch 48000: 2.8592188358306885\n",
      "loss at batch 48064: 2.8018555641174316\n",
      "loss at batch 48128: 2.75216007232666\n",
      "loss at batch 48192: 2.7969536781311035\n",
      "loss at batch 48256: 2.8465845584869385\n",
      "loss at batch 48320: 2.683931827545166\n",
      "loss at batch 48384: 2.7841944694519043\n",
      "loss at batch 48448: 2.6999738216400146\n",
      "loss at batch 48512: 2.7168142795562744\n",
      "loss at batch 48576: 2.7765142917633057\n",
      "loss at batch 48640: 2.8246185779571533\n",
      "loss at batch 48704: 2.829636573791504\n",
      "loss at batch 48768: 2.856539487838745\n",
      "loss at batch 48832: 2.849543333053589\n",
      "loss at batch 48896: 2.8091912269592285\n",
      "loss at batch 48960: 2.7365658283233643\n",
      "loss at batch 49024: 2.776834726333618\n",
      "loss at batch 49088: 2.7671425342559814\n",
      "loss at batch 49152: 2.7917091846466064\n",
      "loss at batch 49216: 2.8098175525665283\n",
      "loss at batch 49280: 2.771418571472168\n",
      "loss at batch 49344: 2.8751606941223145\n",
      "loss at batch 49408: 2.7386324405670166\n",
      "loss at batch 49472: 2.7938039302825928\n",
      "loss at batch 49536: 2.7780885696411133\n",
      "loss at batch 49600: 2.767632007598877\n",
      "loss at batch 49664: 2.716719388961792\n",
      "loss at batch 49728: 2.755826950073242\n",
      "loss at batch 49792: 2.729759931564331\n",
      "loss at batch 49856: 2.80619215965271\n",
      "loss at batch 49920: 2.8311843872070312\n",
      "loss at batch 49984: 2.775202512741089\n",
      "loss at batch 50048: 2.77756404876709\n",
      "loss at batch 50112: 2.6718204021453857\n",
      "loss at batch 50176: 2.8352842330932617\n",
      "loss at batch 50240: 2.8372793197631836\n",
      "loss at batch 50304: 2.8302438259124756\n",
      "loss at batch 50368: 2.659323215484619\n",
      "loss at batch 50432: 2.6888091564178467\n",
      "loss at batch 50496: 2.6842453479766846\n",
      "loss at batch 50560: 2.8384206295013428\n",
      "loss at batch 50624: 2.7285637855529785\n",
      "loss at batch 50688: 2.7628846168518066\n",
      "loss at batch 50752: 2.8235056400299072\n",
      "loss at batch 50816: 2.782085418701172\n",
      "loss at batch 50880: 2.7823421955108643\n",
      "loss at batch 50944: 2.853403091430664\n",
      "loss at batch 51008: 2.883725643157959\n",
      "loss at batch 51072: 2.7482097148895264\n",
      "loss at batch 51136: 2.751569986343384\n",
      "loss at batch 51200: 2.7705252170562744\n",
      "loss at batch 51264: 2.7077150344848633\n",
      "loss at batch 51328: 2.8354427814483643\n",
      "loss at batch 51392: 2.85721755027771\n",
      "loss at batch 51456: 2.8796286582946777\n",
      "loss at batch 51520: 2.681708574295044\n",
      "loss at batch 51584: 2.7939069271087646\n",
      "loss at batch 51648: 2.7205166816711426\n",
      "loss at batch 51712: 2.686098575592041\n",
      "loss at batch 51776: 2.703227996826172\n",
      "loss at batch 51840: 2.788367509841919\n",
      "loss at batch 51904: 2.880978584289551\n",
      "loss at batch 51968: 2.65994930267334\n",
      "loss at batch 52032: 2.6107747554779053\n",
      "loss at batch 52096: 3.004805088043213\n",
      "loss at batch 52160: 2.69905161857605\n",
      "loss at batch 52224: 2.704543113708496\n",
      "loss at batch 52288: 2.8334052562713623\n",
      "loss at batch 52352: 2.82844614982605\n",
      "loss at batch 52416: 2.761018753051758\n",
      "loss at batch 52480: 2.7975871562957764\n",
      "loss at batch 52544: 2.6762659549713135\n",
      "loss at batch 52608: 2.8005268573760986\n",
      "loss at batch 52672: 2.8168540000915527\n",
      "loss at batch 52736: 2.714826822280884\n",
      "loss at batch 52800: 2.651205539703369\n",
      "loss at batch 52864: 2.858830451965332\n",
      "loss at batch 52928: 2.7259202003479004\n",
      "loss at batch 52992: 2.8034849166870117\n",
      "loss at batch 53056: 2.7797255516052246\n",
      "loss at batch 53120: 2.8108973503112793\n",
      "loss at batch 53184: 2.763991594314575\n",
      "loss at batch 53248: 2.713808298110962\n",
      "loss at batch 53312: 2.7994747161865234\n",
      "loss at batch 53376: 2.813136100769043\n",
      "loss at batch 53440: 2.7592551708221436\n",
      "loss at batch 53504: 2.859079360961914\n",
      "loss at batch 53568: 2.6935508251190186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 53632: 2.915766477584839\n",
      "loss at batch 53696: 2.766934633255005\n",
      "loss at batch 53760: 2.825538396835327\n",
      "loss at batch 53824: 2.7493653297424316\n",
      "loss at batch 53888: 2.7898037433624268\n",
      "loss at batch 53952: 2.7268104553222656\n",
      "loss at batch 54016: 2.7507729530334473\n",
      "loss at batch 54080: 2.773775100708008\n",
      "loss at batch 54144: 2.6594395637512207\n",
      "loss at batch 54208: 2.846226692199707\n",
      "loss at batch 54272: 2.761691093444824\n",
      "loss at batch 54336: 2.692112445831299\n",
      "loss at batch 54400: 2.6366827487945557\n",
      "loss at batch 54464: 2.8336589336395264\n",
      "loss at batch 54528: 2.802342414855957\n",
      "loss at batch 54592: 2.630979537963867\n",
      "loss at batch 54656: 2.6844379901885986\n",
      "loss at batch 54720: 2.6554057598114014\n",
      "loss at batch 54784: 2.784353256225586\n",
      "loss at batch 54848: 2.659458637237549\n",
      "loss at batch 54912: 2.7063894271850586\n",
      "loss at batch 54976: 2.699251651763916\n",
      "loss at batch 55040: 2.688809394836426\n",
      "loss at batch 55104: 2.715444564819336\n",
      "loss at batch 55168: 2.6963589191436768\n",
      "loss at batch 55232: 2.8124542236328125\n",
      "loss at batch 55296: 2.9112131595611572\n",
      "loss at batch 55360: 2.6552963256835938\n",
      "loss at batch 55424: 2.76348614692688\n",
      "loss at batch 55488: 2.6284053325653076\n",
      "loss at batch 55552: 2.7359931468963623\n",
      "loss at batch 55616: 2.7065463066101074\n",
      "loss at batch 55680: 2.8537404537200928\n",
      "loss at batch 55744: 2.799406051635742\n",
      "loss at batch 55808: 2.6827452182769775\n",
      "loss at batch 55872: 2.7487072944641113\n",
      "loss at batch 55936: 2.6734020709991455\n",
      "loss at batch 56000: 2.696411371231079\n",
      "loss at batch 56064: 2.771772623062134\n",
      "loss at batch 56128: 2.772843599319458\n",
      "loss at batch 56192: 2.687345266342163\n",
      "loss at batch 56256: 2.74664044380188\n",
      "loss at batch 56320: 2.7287824153900146\n",
      "loss at batch 56384: 2.7164292335510254\n",
      "loss at batch 56448: 2.8103573322296143\n",
      "loss at batch 56512: 2.7777161598205566\n",
      "loss at batch 56576: 2.689955472946167\n",
      "loss at batch 56640: 2.721869468688965\n",
      "loss at batch 56704: 2.7820944786071777\n",
      "loss at batch 56768: 2.77955961227417\n",
      "loss at batch 56832: 2.7243003845214844\n",
      "loss at batch 56896: 2.5685551166534424\n",
      "loss at batch 56960: 2.8460350036621094\n",
      "loss at batch 57024: 2.691744804382324\n",
      "loss at batch 57088: 2.727735996246338\n",
      "loss at batch 57152: 2.810300827026367\n",
      "loss at batch 57216: 2.805197238922119\n",
      "loss at batch 57280: 2.8853771686553955\n",
      "loss at batch 57344: 2.7426483631134033\n",
      "loss at batch 57408: 2.7743899822235107\n",
      "loss at batch 57472: 2.9188039302825928\n",
      "loss at batch 57536: 2.685373306274414\n",
      "loss at batch 57600: 2.800236225128174\n",
      "loss at batch 57664: 2.6525604724884033\n",
      "loss at batch 57728: 2.769362211227417\n",
      "loss at batch 57792: 2.75583553314209\n",
      "loss at batch 57856: 2.7380831241607666\n",
      "loss at batch 57920: 2.642087459564209\n",
      "loss at batch 57984: 2.7317965030670166\n",
      "loss at batch 58048: 2.6638309955596924\n",
      "loss at batch 58112: 2.7325899600982666\n",
      "loss at batch 58176: 2.6355698108673096\n",
      "loss at batch 58240: 2.726107120513916\n",
      "loss at batch 58304: 2.8641104698181152\n",
      "loss at batch 58368: 2.645163059234619\n",
      "loss at batch 58432: 2.775498151779175\n",
      "loss at batch 58496: 2.5932607650756836\n",
      "loss at batch 58560: 2.736107587814331\n",
      "loss at batch 58624: 2.725879192352295\n",
      "loss at batch 58688: 2.771824598312378\n",
      "loss at batch 58752: 2.810847282409668\n",
      "loss at batch 58816: 2.736694812774658\n",
      "loss at batch 58880: 2.760403633117676\n",
      "loss at batch 58944: 2.7437429428100586\n",
      "loss at batch 59008: 2.7130985260009766\n",
      "loss at batch 59072: 2.751981258392334\n",
      "loss at batch 59136: 2.708345413208008\n",
      "loss at batch 59200: 2.7840168476104736\n",
      "loss at batch 59264: 2.6767261028289795\n",
      "loss at batch 59328: 2.7345638275146484\n",
      "loss at batch 59392: 2.8018417358398438\n",
      "loss at batch 59456: 2.6354563236236572\n",
      "loss at batch 59520: 2.851297378540039\n",
      "loss at batch 59584: 2.7343928813934326\n",
      "loss at batch 59648: 2.7251973152160645\n",
      "loss at batch 59712: 2.5804755687713623\n",
      "loss at batch 59776: 2.647045135498047\n",
      "loss at batch 59840: 2.6345643997192383\n",
      "loss at batch 59904: 2.562868118286133\n",
      "loss at batch 59968: 2.716618299484253\n",
      "loss at batch 60032: 2.6923365592956543\n",
      "loss at batch 60096: 2.7588424682617188\n",
      "loss at batch 60160: 2.62481427192688\n",
      "loss at batch 60224: 2.6477084159851074\n",
      "loss at batch 60288: 2.688617706298828\n",
      "loss at batch 60352: 2.601912260055542\n",
      "loss at batch 60416: 2.811556816101074\n",
      "loss at batch 60480: 2.731734275817871\n",
      "loss at batch 60544: 2.674516201019287\n",
      "loss at batch 60608: 2.595186710357666\n",
      "loss at batch 60672: 2.8222880363464355\n",
      "loss at batch 60736: 2.6139776706695557\n",
      "loss at batch 60800: 2.7492518424987793\n",
      "loss at batch 60864: 2.8256285190582275\n",
      "loss at batch 60928: 2.5287439823150635\n",
      "loss at batch 60992: 2.720180034637451\n",
      "loss at batch 61056: 2.678267240524292\n",
      "loss at batch 61120: 2.7022781372070312\n",
      "loss at batch 61184: 2.591498613357544\n",
      "loss at batch 61248: 2.800748109817505\n",
      "loss at batch 61312: 2.661536931991577\n",
      "loss at batch 61376: 2.7584712505340576\n",
      "loss at batch 61440: 2.802541732788086\n",
      "loss at batch 61504: 2.628910779953003\n",
      "loss at batch 61568: 2.6998229026794434\n",
      "loss at batch 61632: 2.671860694885254\n",
      "loss at batch 61696: 2.782876491546631\n",
      "loss at batch 61760: 2.715914011001587\n",
      "loss at batch 61824: 2.5801916122436523\n",
      "loss at batch 61888: 2.6503515243530273\n",
      "loss at batch 61952: 2.786641836166382\n",
      "loss at batch 62016: 2.684380054473877\n",
      "loss at batch 62080: 2.727372884750366\n",
      "loss at batch 62144: 2.6732752323150635\n",
      "loss at batch 62208: 2.721285104751587\n",
      "loss at batch 62272: 2.7616708278656006\n",
      "loss at batch 62336: 2.6861019134521484\n",
      "loss at batch 62400: 2.576770067214966\n",
      "loss at batch 62464: 2.6718201637268066\n",
      "loss at batch 62528: 2.757934331893921\n",
      "loss at batch 62592: 2.7483654022216797\n",
      "loss at batch 62656: 2.738718271255493\n",
      "loss at batch 62720: 2.6404004096984863\n",
      "loss at batch 62784: 2.7205820083618164\n",
      "loss at batch 62848: 2.883096218109131\n",
      "loss at batch 62912: 2.7573869228363037\n",
      "loss at batch 62976: 2.67447566986084\n",
      "loss at batch 63040: 2.5783493518829346\n",
      "loss at batch 63104: 2.673586130142212\n",
      "loss at batch 63168: 2.581897497177124\n",
      "loss at batch 63232: 2.7957828044891357\n",
      "loss at batch 63296: 2.801499128341675\n",
      "loss at batch 63360: 2.600724458694458\n",
      "loss at batch 63424: 2.6160786151885986\n",
      "loss at batch 63488: 2.6652629375457764\n",
      "loss at batch 63552: 2.8255133628845215\n",
      "loss at batch 63616: 2.697593927383423\n",
      "loss at batch 63680: 2.8907179832458496\n",
      "loss at batch 63744: 2.7919042110443115\n",
      "loss at batch 63808: 2.8948802947998047\n",
      "loss at batch 63872: 2.921572208404541\n",
      "loss at batch 63936: 2.688619613647461\n",
      "loss at batch 64000: 2.530635118484497\n",
      "loss at batch 64064: 2.677644729614258\n",
      "loss at batch 64128: 2.8414199352264404\n",
      "loss at batch 64192: 2.9481382369995117\n",
      "loss at batch 64256: 2.69206166267395\n",
      "loss at batch 64320: 2.6346843242645264\n",
      "loss at batch 64384: 2.597670793533325\n",
      "loss at batch 64448: 2.573808193206787\n",
      "loss at batch 64512: 2.6466827392578125\n",
      "loss at batch 64576: 2.720904588699341\n",
      "loss at batch 64640: 2.6582794189453125\n",
      "loss at batch 64704: 2.6979920864105225\n",
      "loss at batch 64768: 2.7711362838745117\n",
      "loss at batch 64832: 2.708460807800293\n",
      "loss at batch 64896: 2.696667194366455\n",
      "loss at batch 64960: 2.714474678039551\n",
      "loss at batch 65024: 2.5222294330596924\n",
      "loss at batch 65088: 2.805833339691162\n",
      "loss at batch 65152: 2.64721417427063\n",
      "loss at batch 65216: 2.6449368000030518\n",
      "loss at batch 65280: 2.693997621536255\n",
      "loss at batch 65344: 2.5166585445404053\n",
      "loss at batch 65408: 2.8454885482788086\n",
      "loss at batch 65472: 2.79613995552063\n",
      "loss at batch 65536: 2.638322591781616\n",
      "loss at batch 65600: 2.623060941696167\n",
      "loss at batch 65664: 2.5957162380218506\n",
      "loss at batch 65728: 2.594696283340454\n",
      "loss at batch 65792: 2.817545175552368\n",
      "loss at batch 65856: 2.5771098136901855\n",
      "loss at batch 65920: 2.6700172424316406\n",
      "loss at batch 65984: 2.661836862564087\n",
      "loss at batch 66048: 2.683443307876587\n",
      "loss at batch 66112: 2.813804864883423\n",
      "loss at batch 66176: 2.8149311542510986\n",
      "loss at batch 66240: 2.5898101329803467\n",
      "loss at batch 66304: 2.6075732707977295\n",
      "loss at batch 66368: 2.740394353866577\n",
      "loss at batch 66432: 2.7621617317199707\n",
      "loss at batch 66496: 2.700294017791748\n",
      "loss at batch 66560: 2.6166036128997803\n",
      "loss at batch 66624: 2.561514377593994\n",
      "loss at batch 66688: 2.663861036300659\n",
      "loss at batch 66752: 2.5354223251342773\n",
      "loss at batch 66816: 2.695920467376709\n",
      "loss at batch 66880: 2.7566797733306885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 66944: 2.9473347663879395\n",
      "loss at batch 67008: 2.5864102840423584\n",
      "loss at batch 67072: 2.520852565765381\n",
      "loss at batch 67136: 2.799804210662842\n",
      "loss at batch 67200: 2.6862597465515137\n",
      "loss at batch 67264: 2.7933738231658936\n",
      "loss at batch 67328: 2.6698970794677734\n",
      "loss at batch 67392: 2.5951974391937256\n",
      "loss at batch 67456: 2.620413303375244\n",
      "loss at batch 67520: 2.779829740524292\n",
      "loss at batch 67584: 2.853372097015381\n",
      "loss at batch 67648: 2.632086992263794\n",
      "loss at batch 67712: 2.673696517944336\n",
      "loss at batch 67776: 2.6419601440429688\n",
      "loss at batch 67840: 2.661815881729126\n",
      "loss at batch 67904: 2.5982353687286377\n",
      "loss at batch 67968: 2.6739749908447266\n",
      "loss at batch 68032: 2.8772528171539307\n",
      "loss at batch 68096: 2.6666688919067383\n",
      "loss at batch 68160: 2.51983642578125\n",
      "loss at batch 68224: 2.785557985305786\n",
      "loss at batch 68288: 2.6238722801208496\n",
      "loss at batch 68352: 2.66142201423645\n",
      "loss at batch 68416: 2.566540241241455\n",
      "loss at batch 68480: 2.5835258960723877\n",
      "loss at batch 68544: 2.7415523529052734\n",
      "loss at batch 68608: 2.565128803253174\n",
      "loss at batch 68672: 2.742924690246582\n",
      "loss at batch 68736: 2.651836395263672\n",
      "loss at batch 68800: 2.659358024597168\n",
      "loss at batch 68864: 2.6528878211975098\n",
      "loss at batch 68928: 2.7296688556671143\n",
      "loss at batch 68992: 2.763345718383789\n",
      "loss at batch 69056: 2.7793712615966797\n",
      "loss at batch 69120: 2.4948649406433105\n",
      "loss at batch 69184: 2.6452629566192627\n",
      "loss at batch 69248: 2.688941717147827\n",
      "loss at batch 69312: 2.58691143989563\n",
      "loss at batch 69376: 2.6073269844055176\n",
      "loss at batch 69440: 2.6921615600585938\n",
      "loss at batch 69504: 2.652407169342041\n",
      "loss at batch 69568: 2.6410651206970215\n",
      "loss at batch 69632: 2.7541122436523438\n",
      "loss at batch 69696: 2.6875107288360596\n",
      "loss at batch 69760: 2.7460198402404785\n",
      "loss at batch 69824: 2.7743594646453857\n",
      "loss at batch 69888: 2.6148784160614014\n",
      "loss at batch 69952: 2.8475968837738037\n",
      "loss at batch 70016: 2.7064123153686523\n",
      "loss at batch 70080: 2.5490269660949707\n",
      "loss at batch 70144: 2.73547625541687\n",
      "loss at batch 70208: 2.5899155139923096\n",
      "loss at batch 70272: 2.5563371181488037\n",
      "loss at batch 70336: 2.5876593589782715\n",
      "loss at batch 70400: 2.666924238204956\n",
      "loss at batch 70464: 2.4294240474700928\n",
      "loss at batch 70528: 2.5039939880371094\n",
      "loss at batch 70592: 2.6937897205352783\n",
      "loss at batch 70656: 2.6288154125213623\n",
      "loss at batch 70720: 2.5810930728912354\n",
      "loss at batch 70784: 2.7904136180877686\n",
      "loss at batch 70848: 2.5304129123687744\n",
      "loss at batch 70912: 2.3432133197784424\n",
      "loss at batch 70976: 2.5534110069274902\n",
      "loss at batch 71040: 2.6592862606048584\n",
      "loss at batch 71104: 2.5894863605499268\n",
      "loss at batch 71168: 2.7041890621185303\n",
      "loss at batch 71232: 2.6889703273773193\n",
      "loss at batch 71296: 2.5212018489837646\n",
      "loss at batch 71360: 2.692228078842163\n",
      "loss at batch 71424: 2.4805638790130615\n",
      "loss at batch 71488: 2.3918449878692627\n",
      "loss at batch 71552: 2.4740631580352783\n",
      "loss at batch 71616: 2.580775022506714\n",
      "loss at batch 71680: 2.402162790298462\n",
      "loss at batch 71744: 2.4099838733673096\n",
      "loss at batch 71808: 2.5169355869293213\n",
      "loss at batch 71872: 2.538299560546875\n",
      "loss at batch 71936: 2.638230800628662\n",
      "loss at batch 72000: 2.6012775897979736\n",
      "loss at batch 72064: 2.629786252975464\n",
      "loss at batch 72128: 2.618119239807129\n",
      "loss at batch 72192: 2.51031756401062\n",
      "loss at batch 72256: 2.6944143772125244\n",
      "loss at batch 72320: 2.7574965953826904\n",
      "loss at batch 72384: 2.6108148097991943\n",
      "loss at batch 72448: 2.521547317504883\n",
      "loss at batch 72512: 2.552497386932373\n",
      "loss at batch 72576: 2.4286422729492188\n",
      "loss at batch 72640: 2.6493325233459473\n",
      "loss at batch 72704: 2.692941665649414\n",
      "loss at batch 72768: 2.691179037094116\n",
      "loss at batch 72832: 2.543761730194092\n",
      "loss at batch 72896: 2.5511550903320312\n",
      "loss at batch 72960: 2.7535171508789062\n",
      "loss at batch 73024: 2.7001843452453613\n",
      "loss at batch 73088: 2.624640464782715\n",
      "loss at batch 73152: 2.8576841354370117\n",
      "loss at batch 73216: 2.710245370864868\n",
      "loss at batch 73280: 2.7558462619781494\n",
      "loss at batch 73344: 2.6577577590942383\n",
      "loss at batch 73408: 2.6449804306030273\n",
      "loss at batch 73472: 2.512512445449829\n",
      "loss at batch 73536: 2.509929895401001\n",
      "loss at batch 73600: 2.4188003540039062\n",
      "loss at batch 73664: 2.8822832107543945\n",
      "loss at batch 73728: 2.434859037399292\n",
      "loss at batch 73792: 2.38482403755188\n",
      "loss at batch 73856: 2.706669807434082\n",
      "loss at batch 73920: 2.6028716564178467\n",
      "loss at batch 73984: 2.540595293045044\n",
      "loss at batch 74048: 2.566818952560425\n",
      "loss at batch 74112: 2.642340898513794\n",
      "loss at batch 74176: 2.5331804752349854\n",
      "loss at batch 74240: 2.5978755950927734\n",
      "loss at batch 74304: 2.6157407760620117\n",
      "loss at batch 74368: 2.6294212341308594\n",
      "loss at batch 74432: 2.6914117336273193\n",
      "loss at batch 74496: 2.451198101043701\n",
      "loss at batch 74560: 2.7710626125335693\n",
      "loss at batch 74624: 2.59997820854187\n",
      "loss at batch 74688: 2.4181299209594727\n",
      "loss at batch 74752: 2.7999682426452637\n",
      "loss at batch 74816: 2.5839500427246094\n",
      "loss at batch 74880: 2.510568618774414\n",
      "loss at batch 74944: 2.595106840133667\n",
      "loss at batch 75008: 2.776272773742676\n",
      "loss at batch 75072: 2.496216058731079\n",
      "loss at batch 75136: 2.5246822834014893\n",
      "loss at batch 75200: 2.4671342372894287\n",
      "loss at batch 75264: 2.6201539039611816\n",
      "loss at batch 75328: 2.7114856243133545\n",
      "loss at batch 75392: 2.4842045307159424\n",
      "loss at batch 75456: 2.4112162590026855\n",
      "loss at batch 75520: 2.5733113288879395\n",
      "loss at batch 75584: 2.404360294342041\n",
      "loss at batch 75648: 2.928574800491333\n",
      "loss at batch 75712: 2.399153470993042\n",
      "loss at batch 75776: 2.577174425125122\n",
      "loss at batch 75840: 2.705199956893921\n",
      "loss at batch 75904: 3.0478389263153076\n",
      "loss at batch 75968: 2.6255269050598145\n",
      "loss at batch 76032: 2.5345518589019775\n",
      "loss at batch 76096: 2.5863311290740967\n",
      "loss at batch 76160: 2.5880467891693115\n",
      "loss at batch 76224: 2.837972402572632\n",
      "loss at batch 76288: 2.6307220458984375\n",
      "loss at batch 76352: 2.495270252227783\n",
      "loss at batch 76416: 2.544982433319092\n",
      "loss at batch 76480: 2.5145766735076904\n",
      "loss at batch 76544: 2.5259389877319336\n",
      "loss at batch 76608: 2.3877248764038086\n",
      "loss at batch 76672: 2.6269338130950928\n",
      "loss at batch 76736: 2.9220986366271973\n",
      "loss at batch 76800: 2.6372549533843994\n",
      "loss at batch 76864: 2.5867631435394287\n",
      "loss at batch 76928: 2.544590950012207\n",
      "loss at batch 76992: 2.624234199523926\n",
      "loss at batch 77056: 2.6241095066070557\n",
      "loss at batch 77120: 2.667504072189331\n",
      "loss at batch 77184: 2.4875686168670654\n",
      "loss at batch 77248: 2.426774740219116\n",
      "loss at batch 77312: 2.381495952606201\n",
      "loss at batch 77376: 2.569521427154541\n",
      "loss at batch 77440: 2.5101280212402344\n",
      "loss at batch 77504: 2.415942907333374\n",
      "loss at batch 77568: 2.612309694290161\n",
      "loss at batch 77632: 2.5025978088378906\n",
      "loss at batch 77696: 2.7804200649261475\n",
      "loss at batch 77760: 2.403414726257324\n",
      "loss at batch 77824: 2.591954231262207\n",
      "loss at batch 77888: 2.483438730239868\n",
      "loss at batch 77952: 2.590355396270752\n",
      "loss at batch 78016: 2.479480504989624\n",
      "loss at batch 78080: 2.5305702686309814\n",
      "loss at batch 78144: 2.6700234413146973\n",
      "loss at batch 78208: 2.5529816150665283\n",
      "loss at batch 78272: 2.6203837394714355\n",
      "loss at batch 78336: 2.6262261867523193\n",
      "loss at batch 78400: 2.5097076892852783\n",
      "loss at batch 78464: 2.7337167263031006\n",
      "loss at batch 78528: 2.6277451515197754\n",
      "loss at batch 78592: 2.2998623847961426\n",
      "loss at batch 78656: 2.790510416030884\n",
      "loss at batch 78720: 2.522080183029175\n",
      "loss at batch 78784: 2.5193138122558594\n",
      "loss at batch 78848: 2.5677201747894287\n",
      "loss at batch 78912: 2.4778196811676025\n",
      "loss at batch 78976: 2.459540367126465\n",
      "loss at batch 79040: 2.459794282913208\n",
      "loss at batch 79104: 2.8458988666534424\n",
      "loss at batch 79168: 2.7679219245910645\n",
      "loss at batch 79232: 2.590942144393921\n",
      "loss at batch 79296: 2.7246391773223877\n",
      "loss at batch 79360: 2.392484664916992\n",
      "loss at batch 79424: 2.6345486640930176\n",
      "loss at batch 79488: 3.047074556350708\n",
      "loss at batch 79552: 2.8748903274536133\n",
      "loss at batch 79616: 2.7575361728668213\n",
      "loss at batch 79680: 2.8708078861236572\n",
      "loss at batch 79744: 2.384969472885132\n",
      "loss at batch 79808: 2.497875452041626\n",
      "loss at batch 79872: 2.4236865043640137\n",
      "loss at batch 79936: 2.3587846755981445\n",
      "loss at batch 80000: 2.4180595874786377\n",
      "loss at batch 80064: 2.3623900413513184\n",
      "loss at batch 80128: 2.46787691116333\n",
      "loss at batch 80192: 2.8282670974731445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 80256: 2.821998119354248\n",
      "loss at batch 80320: 2.7124440670013428\n",
      "loss at batch 80384: 2.6940081119537354\n",
      "loss at batch 80448: 2.4829349517822266\n",
      "loss at batch 80512: 2.4458892345428467\n",
      "loss at batch 80576: 2.5071804523468018\n",
      "loss at batch 80640: 2.664264440536499\n",
      "loss at batch 80704: 2.4588985443115234\n",
      "loss at batch 80768: 2.549326181411743\n",
      "loss at batch 80832: 2.6436219215393066\n",
      "loss at batch 80896: 2.617842674255371\n",
      "loss at batch 80960: 2.3521270751953125\n",
      "loss at batch 81024: 2.467338800430298\n",
      "loss at batch 81088: 2.5303940773010254\n",
      "loss at batch 81152: 2.782472610473633\n",
      "loss at batch 81216: 2.5059385299682617\n",
      "loss at batch 81280: 2.447650671005249\n",
      "loss at batch 81344: 2.5330820083618164\n",
      "loss at batch 81408: 2.523319959640503\n",
      "loss at batch 81472: 2.422126293182373\n",
      "loss at batch 81536: 2.5131874084472656\n",
      "loss at batch 81600: 2.5770139694213867\n",
      "loss at batch 81664: 2.3259060382843018\n",
      "loss at batch 81728: 2.388587236404419\n",
      "loss at batch 81792: 2.307211399078369\n",
      "loss at batch 81856: 2.7145843505859375\n",
      "loss at batch 81920: 2.3762683868408203\n",
      "loss at batch 81984: 2.5142765045166016\n",
      "loss at batch 82048: 2.3384783267974854\n",
      "loss at batch 82112: 2.416908025741577\n",
      "loss at batch 82176: 2.395282030105591\n",
      "loss at batch 82240: 2.5471298694610596\n",
      "loss at batch 82304: 2.3881428241729736\n",
      "loss at batch 82368: 2.5307514667510986\n",
      "loss at batch 82432: 2.44696307182312\n",
      "loss at batch 82496: 2.6728227138519287\n",
      "loss at batch 82560: 2.4679393768310547\n",
      "loss at batch 82624: 2.4263100624084473\n",
      "loss at batch 82688: 2.714569091796875\n",
      "loss at batch 82752: 2.5916013717651367\n",
      "loss at batch 82816: 2.4638125896453857\n",
      "loss at batch 82880: 2.524369239807129\n",
      "loss at batch 82944: 2.5298454761505127\n",
      "loss at batch 83008: 2.750239372253418\n",
      "loss at batch 83072: 2.404794454574585\n",
      "loss at batch 83136: 2.629794120788574\n",
      "loss at batch 83200: 2.3695383071899414\n",
      "loss at batch 83264: 2.7352585792541504\n",
      "loss at batch 83328: 2.459519863128662\n",
      "loss at batch 83392: 2.4055168628692627\n",
      "loss at batch 83456: 2.4574124813079834\n",
      "loss at batch 83520: 2.2661077976226807\n",
      "loss at batch 83584: 2.6074376106262207\n",
      "loss at batch 83648: 2.6208713054656982\n",
      "loss at batch 83712: 2.5609993934631348\n",
      "loss at batch 83776: 2.494751453399658\n",
      "loss at batch 83840: 2.65446138381958\n",
      "loss at batch 83904: 2.6830484867095947\n",
      "loss at batch 83968: 2.466782569885254\n",
      "loss at batch 84032: 2.413137674331665\n",
      "loss at batch 84096: 2.6583383083343506\n",
      "loss at batch 84160: 2.5886573791503906\n",
      "loss at batch 84224: 2.3357696533203125\n",
      "loss at batch 84288: 2.3914592266082764\n",
      "loss at batch 84352: 2.3937675952911377\n",
      "loss at batch 84416: 2.589054584503174\n",
      "loss at batch 84480: 2.560701370239258\n",
      "loss at batch 84544: 2.682943105697632\n",
      "loss at batch 84608: 2.3579235076904297\n",
      "loss at batch 84672: 2.3349387645721436\n",
      "loss at batch 84736: 2.4678494930267334\n",
      "loss at batch 84800: 2.6038825511932373\n",
      "loss at batch 84864: 2.5263559818267822\n",
      "loss at batch 84928: 2.598245143890381\n",
      "loss at batch 84992: 2.416480302810669\n",
      "loss at batch 85056: 2.4889650344848633\n",
      "loss at batch 85120: 2.4289376735687256\n",
      "loss at batch 85184: 2.3238017559051514\n",
      "loss at batch 85248: 2.4477479457855225\n",
      "loss at batch 85312: 2.3490803241729736\n",
      "loss at batch 85376: 2.565896511077881\n",
      "loss at batch 85440: 2.3918845653533936\n",
      "loss at batch 85504: 2.387282371520996\n",
      "loss at batch 85568: 2.4032955169677734\n",
      "loss at batch 85632: 2.526235580444336\n",
      "loss at batch 85696: 2.528127431869507\n",
      "loss at batch 85760: 2.160789728164673\n",
      "loss at batch 85824: 2.3958497047424316\n",
      "loss at batch 85888: 2.2926361560821533\n",
      "loss at batch 85952: 2.533954620361328\n",
      "loss at batch 86016: 2.6599245071411133\n",
      "loss at batch 86080: 2.5245614051818848\n",
      "loss at batch 86144: 2.5116782188415527\n",
      "loss at batch 86208: 2.4202451705932617\n",
      "loss at batch 86272: 2.233001708984375\n",
      "loss at batch 86336: 2.378950834274292\n",
      "loss at batch 86400: 2.483893632888794\n",
      "loss at batch 86464: 2.5838465690612793\n",
      "loss at batch 86528: 2.508345365524292\n",
      "loss at batch 86592: 2.423482894897461\n",
      "loss at batch 86656: 2.7737810611724854\n",
      "loss at batch 86720: 2.303414821624756\n",
      "loss at batch 86784: 2.6612699031829834\n",
      "loss at batch 86848: 2.3322629928588867\n",
      "loss at batch 86912: 2.2127625942230225\n",
      "loss at batch 86976: 2.313908576965332\n",
      "loss at batch 87040: 2.4431633949279785\n",
      "loss at batch 87104: 2.2780051231384277\n",
      "loss at batch 87168: 2.4701993465423584\n",
      "loss at batch 87232: 2.5589332580566406\n",
      "loss at batch 87296: 2.456106662750244\n",
      "loss at batch 87360: 2.521557331085205\n",
      "loss at batch 87424: 2.241671085357666\n",
      "loss at batch 87488: 2.387455463409424\n",
      "loss at batch 87552: 2.3119826316833496\n",
      "loss at batch 87616: 2.3112051486968994\n",
      "loss at batch 87680: 2.336026668548584\n",
      "loss at batch 87744: 2.343149423599243\n",
      "loss at batch 87808: 2.328507900238037\n",
      "loss at batch 87872: 2.327260971069336\n",
      "loss at batch 87936: 2.2132387161254883\n",
      "loss at batch 88000: 2.347907781600952\n",
      "loss at batch 88064: 2.588344097137451\n",
      "loss at batch 88128: 2.4427340030670166\n",
      "loss at batch 88192: 2.3290607929229736\n",
      "loss at batch 88256: 2.414224863052368\n",
      "loss at batch 88320: 2.324664831161499\n",
      "loss at batch 88384: 2.2172908782958984\n",
      "loss at batch 88448: 2.4734039306640625\n",
      "loss at batch 88512: 2.321000337600708\n",
      "loss at batch 88576: 2.3283538818359375\n",
      "loss at batch 88640: 2.756193161010742\n",
      "loss at batch 88704: 2.4255218505859375\n",
      "loss at batch 88768: 2.536559581756592\n",
      "loss at batch 88832: 2.4265730381011963\n",
      "loss at batch 88896: 2.4433045387268066\n",
      "loss at batch 88960: 2.5962836742401123\n",
      "loss at batch 89024: 2.5801239013671875\n",
      "loss at batch 89088: 2.334061861038208\n",
      "loss at batch 89152: 2.5468976497650146\n",
      "loss at batch 89216: 2.2467238903045654\n",
      "loss at batch 89280: 2.286304235458374\n",
      "loss at batch 89344: 2.3740193843841553\n",
      "loss at batch 89408: 2.311818838119507\n",
      "loss at batch 89472: 2.6515181064605713\n",
      "loss at batch 89536: 2.413559913635254\n",
      "loss at batch 89600: 2.544119119644165\n",
      "loss at batch 89664: 2.5599205493927\n",
      "loss at batch 89728: 2.281792640686035\n",
      "loss at batch 89792: 2.6988534927368164\n",
      "loss at batch 89856: 2.5261943340301514\n",
      "loss at batch 89920: 2.329756736755371\n",
      "loss at batch 89984: 2.248263359069824\n",
      "loss at batch 90048: 2.523987293243408\n",
      "loss at batch 90112: 2.093585252761841\n",
      "loss at batch 90176: 2.472196102142334\n",
      "loss at batch 90240: 2.109816551208496\n",
      "loss at batch 90304: 2.1026108264923096\n",
      "loss at batch 90368: 2.263406753540039\n",
      "loss at batch 90432: 2.407421350479126\n",
      "loss at batch 90496: 2.492064952850342\n",
      "loss at batch 90560: 2.4635672569274902\n",
      "loss at batch 90624: 2.701082229614258\n",
      "loss at batch 90688: 2.553658962249756\n",
      "loss at batch 90752: 2.3386330604553223\n",
      "loss at batch 90816: 2.1516313552856445\n",
      "loss at batch 90880: 2.3684356212615967\n",
      "loss at batch 90944: 2.5869688987731934\n",
      "loss at batch 91008: 2.164381265640259\n",
      "loss at batch 91072: 2.505913496017456\n",
      "loss at batch 91136: 2.431514024734497\n",
      "loss at batch 91200: 2.4092400074005127\n",
      "loss at batch 91264: 2.1851189136505127\n",
      "loss at batch 91328: 2.428327798843384\n",
      "loss at batch 91392: 2.1066300868988037\n",
      "loss at batch 91456: 2.453765392303467\n",
      "loss at batch 91520: 2.348123550415039\n",
      "loss at batch 91584: 2.525517702102661\n",
      "loss at batch 91648: 2.239962577819824\n",
      "loss at batch 91712: 2.1991567611694336\n",
      "loss at batch 91776: 2.5138638019561768\n",
      "loss at batch 91840: 2.1893599033355713\n",
      "loss at batch 91904: 2.1962504386901855\n",
      "loss at batch 91968: 2.4133529663085938\n",
      "loss at batch 92032: 2.23537540435791\n",
      "loss at batch 92096: 2.5125532150268555\n",
      "loss at batch 92160: 2.7296319007873535\n",
      "loss at batch 92224: 2.363407850265503\n",
      "loss at batch 92288: 2.2275943756103516\n",
      "loss at batch 92352: 2.376483917236328\n",
      "loss at batch 92416: 2.384702682495117\n",
      "loss at batch 92480: 2.2592403888702393\n",
      "loss at batch 92544: 2.2210192680358887\n",
      "loss at batch 92608: 2.0558104515075684\n",
      "loss at batch 92672: 2.2668042182922363\n",
      "loss at batch 92736: 2.4284002780914307\n",
      "loss at batch 92800: 2.3274502754211426\n",
      "loss at batch 92864: 2.1923723220825195\n",
      "loss at batch 92928: 2.415630578994751\n",
      "loss at batch 92992: 2.620152711868286\n",
      "loss at batch 93056: 2.3268494606018066\n",
      "loss at batch 93120: 2.4003822803497314\n",
      "loss at batch 93184: 2.505729913711548\n",
      "loss at batch 93248: 2.214677333831787\n",
      "loss at batch 93312: 2.58676815032959\n",
      "loss at batch 93376: 2.433046340942383\n",
      "loss at batch 93440: 2.146972417831421\n",
      "loss at batch 93504: 2.2449893951416016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 93568: 2.3390607833862305\n",
      "loss at batch 93632: 2.2263875007629395\n",
      "loss at batch 93696: 2.5165069103240967\n",
      "loss at batch 93760: 2.434410572052002\n",
      "loss at batch 93824: 2.813323974609375\n",
      "loss at batch 93888: 2.298464298248291\n",
      "loss at batch 93952: 2.735755443572998\n",
      "loss at batch 94016: 2.3553519248962402\n",
      "loss at batch 94080: 2.350369930267334\n",
      "loss at batch 94144: 2.3329994678497314\n",
      "loss at batch 94208: 2.192598342895508\n",
      "loss at batch 94272: 2.205881357192993\n",
      "loss at batch 94336: 2.3918635845184326\n",
      "loss at batch 94400: 2.5609264373779297\n",
      "loss at batch 94464: 2.3739256858825684\n",
      "loss at batch 94528: 2.577970504760742\n",
      "loss at batch 94592: 2.266463279724121\n",
      "loss at batch 94656: 2.2190585136413574\n",
      "loss at batch 94720: 2.466902732849121\n",
      "loss at batch 94784: 2.3105387687683105\n",
      "loss at batch 94848: 2.083522319793701\n",
      "loss at batch 94912: 2.5458762645721436\n",
      "loss at batch 94976: 2.2807235717773438\n",
      "loss at batch 95040: 2.379901647567749\n",
      "loss at batch 95104: 2.457388401031494\n",
      "loss at batch 95168: 2.3115925788879395\n",
      "loss at batch 95232: 2.233144521713257\n",
      "loss at batch 95296: 2.5891451835632324\n",
      "loss at batch 95360: 2.3751461505889893\n",
      "loss at batch 95424: 2.178617000579834\n",
      "loss at batch 95488: 2.1748077869415283\n",
      "loss at batch 95552: 2.350435972213745\n",
      "loss at batch 95616: 2.2672159671783447\n",
      "loss at batch 95680: 2.4520766735076904\n",
      "loss at batch 95744: 2.184217691421509\n",
      "loss at batch 95808: 2.0364441871643066\n",
      "loss at batch 95872: 2.4154553413391113\n",
      "loss at batch 95936: 2.603487253189087\n",
      "loss at batch 96000: 2.3633155822753906\n",
      "loss at batch 96064: 2.2330031394958496\n",
      "loss at batch 96128: 2.411705255508423\n",
      "loss at batch 96192: 2.099735736846924\n",
      "loss at batch 96256: 2.5981712341308594\n",
      "loss at batch 96320: 2.1857521533966064\n",
      "loss at batch 96384: 2.0716280937194824\n",
      "loss at batch 96448: 2.550992727279663\n",
      "loss at batch 96512: 2.5769550800323486\n",
      "loss at batch 96576: 2.1299362182617188\n",
      "loss at batch 96640: 2.244472026824951\n",
      "loss at batch 96704: 2.82967472076416\n",
      "loss at batch 96768: 2.2859082221984863\n",
      "loss at batch 96832: 2.667919158935547\n",
      "loss at batch 96896: 2.551426649093628\n",
      "loss at batch 96960: 2.3139984607696533\n",
      "loss at batch 97024: 2.5458920001983643\n",
      "loss at batch 97088: 2.3246967792510986\n",
      "loss at batch 97152: 2.1968905925750732\n",
      "loss at batch 97216: 2.1179215908050537\n",
      "loss at batch 97280: 2.21828031539917\n",
      "loss at batch 97344: 2.0974855422973633\n",
      "loss at batch 97408: 2.394968271255493\n",
      "loss at batch 97472: 3.257878303527832\n",
      "loss at batch 97536: 2.2148263454437256\n",
      "loss at batch 97600: 2.186203718185425\n",
      "loss at batch 97664: 2.174429178237915\n",
      "loss at batch 97728: 2.232428550720215\n",
      "loss at batch 97792: 2.4484002590179443\n",
      "loss at batch 97856: 2.3308756351470947\n",
      "loss at batch 97920: 2.290304660797119\n",
      "loss at batch 97984: 2.237990140914917\n",
      "loss at batch 98048: 2.413944959640503\n",
      "loss at batch 98112: 2.090935230255127\n",
      "loss at batch 98176: 2.2467100620269775\n",
      "loss at batch 98240: 2.1182661056518555\n",
      "loss at batch 98304: 2.29132342338562\n",
      "loss at batch 98368: 2.1887457370758057\n",
      "loss at batch 98432: 1.9913544654846191\n",
      "loss at batch 98496: 2.176485061645508\n",
      "loss at batch 98560: 2.2745862007141113\n",
      "loss at batch 98624: 2.3612613677978516\n",
      "loss at batch 98688: 2.395467758178711\n",
      "loss at batch 98752: 2.8783395290374756\n",
      "loss at batch 98816: 2.1481130123138428\n",
      "loss at batch 98880: 2.390608549118042\n",
      "loss at batch 98944: 2.499379873275757\n",
      "loss at batch 99008: 2.2562997341156006\n",
      "loss at batch 99072: 2.5110368728637695\n",
      "loss at batch 99136: 2.178654193878174\n",
      "loss at batch 99200: 2.3621716499328613\n",
      "loss at batch 99264: 2.203080892562866\n",
      "loss at batch 99328: 2.297689199447632\n",
      "loss at batch 99392: 2.112597942352295\n",
      "loss at batch 99456: 2.683969020843506\n",
      "loss at batch 99520: 1.9942450523376465\n",
      "loss at batch 99584: 2.071439266204834\n",
      "loss at batch 99648: 2.45078706741333\n",
      "loss at batch 99712: 2.345050573348999\n",
      "loss at batch 99776: 2.2197794914245605\n",
      "loss at batch 99840: 2.171980142593384\n",
      "loss at batch 99904: 2.3078134059906006\n",
      "loss at batch 99968: 2.154360055923462\n",
      "loss at batch 100032: 2.5214414596557617\n",
      "loss at batch 100096: 2.1235454082489014\n",
      "loss at batch 100160: 2.3325259685516357\n",
      "loss at batch 100224: 2.2918691635131836\n",
      "loss at batch 100288: 2.486593008041382\n",
      "loss at batch 100352: 2.184896469116211\n",
      "loss at batch 100416: 2.127117872238159\n",
      "loss at batch 100480: 1.9311057329177856\n",
      "loss at batch 100544: 2.5450985431671143\n",
      "loss at batch 100608: 2.1733808517456055\n",
      "loss at batch 100672: 2.3032116889953613\n",
      "loss at batch 100736: 2.3137762546539307\n",
      "loss at batch 100800: 2.3252131938934326\n",
      "loss at batch 100864: 2.3219618797302246\n",
      "loss at batch 100928: 2.2926366329193115\n",
      "loss at batch 100992: 2.680283546447754\n",
      "loss at batch 101056: 2.326680898666382\n",
      "loss at batch 101120: 3.0039913654327393\n",
      "loss at batch 101184: 2.1538426876068115\n",
      "loss at batch 101248: 2.1899726390838623\n",
      "loss at batch 101312: 2.2159481048583984\n",
      "loss at batch 101376: 2.4148287773132324\n",
      "loss at batch 101440: 2.2312591075897217\n",
      "loss at batch 101504: 2.2247586250305176\n",
      "loss at batch 101568: 2.413492202758789\n",
      "loss at batch 101632: 2.546504259109497\n",
      "loss at batch 101696: 2.237091302871704\n",
      "loss at batch 101760: 2.318674087524414\n",
      "loss at batch 101824: 2.33040452003479\n",
      "loss at batch 101888: 2.1607861518859863\n",
      "loss at batch 101952: 2.254882335662842\n",
      "loss at batch 102016: 2.3181393146514893\n",
      "loss at batch 102080: 1.8822933435440063\n",
      "loss at batch 102144: 2.222170829772949\n",
      "loss at batch 102208: 2.305217981338501\n",
      "loss at batch 102272: 2.312730073928833\n",
      "loss at batch 102336: 2.289463996887207\n",
      "loss at batch 102400: 2.046548366546631\n",
      "loss at batch 102464: 2.117400884628296\n",
      "loss at batch 102528: 2.215724229812622\n",
      "loss at batch 102592: 1.932380199432373\n",
      "loss at batch 102656: 2.2711801528930664\n",
      "loss at batch 102720: 2.289019823074341\n",
      "loss at batch 102784: 2.25610089302063\n",
      "loss at batch 102848: 2.263829231262207\n",
      "loss at batch 102912: 2.229827404022217\n",
      "loss at batch 102976: 2.4298181533813477\n",
      "loss at batch 103040: 2.1933488845825195\n",
      "loss at batch 103104: 2.1644890308380127\n",
      "loss at batch 103168: 2.1405463218688965\n",
      "loss at batch 103232: 2.3943400382995605\n",
      "loss at batch 103296: 2.108015537261963\n",
      "loss at batch 103360: 2.2493679523468018\n",
      "loss at batch 103424: 2.365935802459717\n",
      "loss at batch 103488: 2.0739223957061768\n",
      "loss at batch 103552: 2.321239948272705\n",
      "loss at batch 103616: 2.1993019580841064\n",
      "loss at batch 103680: 2.219613790512085\n",
      "loss at batch 103744: 2.262373924255371\n",
      "loss at batch 103808: 2.078246831893921\n",
      "loss at batch 103872: 2.096646785736084\n",
      "loss at batch 103936: 2.3132596015930176\n",
      "loss at batch 104000: 1.991421103477478\n",
      "loss at batch 104064: 2.2348828315734863\n",
      "loss at batch 104128: 2.3551201820373535\n",
      "loss at batch 104192: 2.1095850467681885\n",
      "loss at batch 104256: 2.2540030479431152\n",
      "loss at batch 104320: 2.3917200565338135\n",
      "loss at batch 104384: 2.2221524715423584\n",
      "loss at batch 104448: 2.4087891578674316\n",
      "loss at batch 104512: 2.249441146850586\n",
      "loss at batch 104576: 2.078117847442627\n",
      "loss at batch 104640: 2.1757540702819824\n",
      "loss at batch 104704: 2.222996234893799\n",
      "loss at batch 104768: 2.0949926376342773\n",
      "loss at batch 104832: 2.3175506591796875\n",
      "loss at batch 104896: 1.9692071676254272\n",
      "loss at batch 104960: 2.183987617492676\n",
      "loss at batch 105024: 2.771857500076294\n",
      "loss at batch 105088: 2.1627817153930664\n",
      "loss at batch 105152: 2.0067555904388428\n",
      "loss at batch 105216: 2.371539354324341\n",
      "loss at batch 105280: 2.2694461345672607\n",
      "loss at batch 105344: 2.4091901779174805\n",
      "loss at batch 105408: 2.0631635189056396\n",
      "loss at batch 105472: 2.3405685424804688\n",
      "loss at batch 105536: 2.3913402557373047\n",
      "loss at batch 105600: 2.280848264694214\n",
      "loss at batch 105664: 2.237502098083496\n",
      "loss at batch 105728: 2.1269924640655518\n",
      "loss at batch 105792: 2.131842613220215\n",
      "loss at batch 105856: 2.001793384552002\n",
      "loss at batch 105920: 2.0020477771759033\n",
      "loss at batch 105984: 2.313438892364502\n",
      "loss at batch 106048: 1.8683266639709473\n",
      "loss at batch 106112: 2.044740676879883\n",
      "loss at batch 106176: 2.0748536586761475\n",
      "loss at batch 106240: 2.151693820953369\n",
      "loss at batch 106304: 1.9218451976776123\n",
      "loss at batch 106368: 2.2802746295928955\n",
      "loss at batch 106432: 2.3932583332061768\n",
      "loss at batch 106496: 1.8607800006866455\n",
      "loss at batch 106560: 2.173022508621216\n",
      "loss at batch 106624: 2.071906089782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 106688: 2.1128644943237305\n",
      "loss at batch 106752: 2.2824926376342773\n",
      "loss at batch 106816: 1.9358755350112915\n",
      "loss at batch 106880: 2.1067512035369873\n",
      "loss at batch 106944: 2.314486503601074\n",
      "loss at batch 107008: 2.286648988723755\n",
      "loss at batch 107072: 2.3851191997528076\n",
      "loss at batch 107136: 2.2403063774108887\n",
      "loss at batch 107200: 2.407989025115967\n",
      "loss at batch 107264: 2.127560615539551\n",
      "loss at batch 107328: 2.0651285648345947\n",
      "loss at batch 107392: 2.1717376708984375\n",
      "loss at batch 107456: 2.3138022422790527\n",
      "loss at batch 107520: 2.3063132762908936\n",
      "loss at batch 107584: 2.1799936294555664\n",
      "loss at batch 107648: 1.9328861236572266\n",
      "loss at batch 107712: 2.312328577041626\n",
      "loss at batch 107776: 2.0110974311828613\n",
      "loss at batch 107840: 2.544574022293091\n",
      "loss at batch 107904: 2.4224960803985596\n",
      "loss at batch 107968: 2.0228137969970703\n",
      "loss at batch 108032: 2.2161800861358643\n",
      "loss at batch 108096: 2.0692782402038574\n",
      "loss at batch 108160: 2.179335832595825\n",
      "loss at batch 108224: 2.4514873027801514\n",
      "loss at batch 108288: 2.1248464584350586\n",
      "loss at batch 108352: 2.0804178714752197\n",
      "loss at batch 108416: 1.924856424331665\n",
      "loss at batch 108480: 2.1789543628692627\n",
      "loss at batch 108544: 1.9538522958755493\n",
      "loss at batch 108608: 2.0757219791412354\n",
      "loss at batch 108672: 2.1433188915252686\n",
      "loss at batch 108736: 2.111175775527954\n",
      "loss at batch 108800: 2.000810146331787\n",
      "loss at batch 108864: 2.7300524711608887\n",
      "loss at batch 108928: 2.222787857055664\n",
      "loss at batch 108992: 2.375501871109009\n",
      "loss at batch 109056: 2.042699098587036\n",
      "loss at batch 109120: 2.0932493209838867\n",
      "loss at batch 109184: 2.0218091011047363\n",
      "loss at batch 109248: 2.198245048522949\n",
      "loss at batch 109312: 2.1297647953033447\n",
      "loss at batch 109376: 2.159456968307495\n",
      "loss at batch 109440: 2.4027445316314697\n",
      "loss at batch 109504: 1.9099743366241455\n",
      "loss at batch 109568: 2.140828847885132\n",
      "loss at batch 109632: 2.12622332572937\n",
      "loss at batch 109696: 2.047682523727417\n",
      "loss at batch 109760: 2.4732677936553955\n",
      "loss at batch 109824: 2.142812967300415\n",
      "loss at batch 109888: 1.9279333353042603\n",
      "loss at batch 109952: 2.6366047859191895\n",
      "loss at batch 110016: 1.9077991247177124\n",
      "loss at batch 110080: 2.0210540294647217\n",
      "loss at batch 110144: 2.1848645210266113\n",
      "loss at batch 110208: 2.065699815750122\n",
      "loss at batch 110272: 2.0701003074645996\n",
      "loss at batch 110336: 1.9982500076293945\n",
      "loss at batch 110400: 2.490069627761841\n",
      "loss at batch 110464: 2.084501028060913\n",
      "loss at batch 110528: 2.290288209915161\n",
      "loss at batch 110592: 2.2996175289154053\n",
      "loss at batch 110656: 2.4698164463043213\n",
      "loss at batch 110720: 2.1690311431884766\n",
      "loss at batch 110784: 2.4571330547332764\n",
      "loss at batch 110848: 2.378154754638672\n",
      "loss at batch 110912: 2.100386619567871\n",
      "loss at batch 110976: 1.990259051322937\n",
      "loss at batch 111040: 2.401780605316162\n",
      "loss at batch 111104: 1.9598112106323242\n",
      "loss at batch 111168: 1.7659313678741455\n",
      "loss at batch 111232: 1.9453983306884766\n",
      "loss at batch 111296: 1.9955188035964966\n",
      "loss at batch 111360: 2.081254720687866\n",
      "loss at batch 111424: 2.069474458694458\n",
      "loss at batch 111488: 2.3038458824157715\n",
      "loss at batch 111552: 1.92954421043396\n",
      "loss at batch 111616: 2.1834042072296143\n",
      "loss at batch 111680: 2.0978329181671143\n",
      "loss at batch 111744: 2.1965508460998535\n",
      "loss at batch 111808: 2.5579092502593994\n",
      "loss at batch 111872: 2.2820913791656494\n",
      "loss at batch 111936: 2.0380303859710693\n",
      "loss at batch 112000: 2.1210553646087646\n",
      "loss at batch 112064: 2.085038661956787\n",
      "loss at batch 112128: 2.050070285797119\n",
      "loss at batch 112192: 2.1822266578674316\n",
      "loss at batch 112256: 2.055509567260742\n",
      "loss at batch 112320: 2.1577534675598145\n",
      "loss at batch 112384: 1.969916820526123\n",
      "loss at batch 112448: 2.060811758041382\n",
      "loss at batch 112512: 1.9245588779449463\n",
      "loss at batch 112576: 2.7126190662384033\n",
      "loss at batch 112640: 2.189445972442627\n",
      "loss at batch 112704: 1.8073023557662964\n",
      "loss at batch 112768: 2.615682363510132\n",
      "loss at batch 112832: 2.124422550201416\n",
      "loss at batch 112896: 2.3619494438171387\n",
      "loss at batch 112960: 2.314150094985962\n",
      "loss at batch 113024: 2.328871011734009\n",
      "loss at batch 113088: 2.1148552894592285\n",
      "loss at batch 113152: 2.3313777446746826\n",
      "loss at batch 113216: 2.4722559452056885\n",
      "loss at batch 113280: 2.1476540565490723\n",
      "loss at batch 113344: 2.2984676361083984\n",
      "loss at batch 113408: 2.107266664505005\n",
      "loss at batch 113472: 2.009788751602173\n",
      "loss at batch 113536: 2.6988296508789062\n",
      "loss at batch 113600: 2.2939488887786865\n",
      "loss at batch 113664: 2.052950143814087\n",
      "loss at batch 113728: 1.956047534942627\n",
      "loss at batch 113792: 1.896679162979126\n",
      "loss at batch 113856: 2.1103579998016357\n",
      "loss at batch 113920: 1.9300793409347534\n",
      "loss at batch 113984: 2.0537776947021484\n",
      "loss at batch 114048: 2.358752965927124\n",
      "loss at batch 114112: 2.019552230834961\n",
      "loss at batch 114176: 2.2986884117126465\n",
      "loss at batch 114240: 2.020202398300171\n",
      "loss at batch 114304: 2.312326431274414\n",
      "loss at batch 114368: 2.1281800270080566\n",
      "loss at batch 114432: 2.0643503665924072\n",
      "loss at batch 114496: 2.3962509632110596\n",
      "loss at batch 114560: 2.0572986602783203\n",
      "loss at batch 114624: 2.1296439170837402\n",
      "loss at batch 114688: 2.38486909866333\n",
      "loss at batch 114752: 1.8709617853164673\n",
      "loss at batch 114816: 2.411284923553467\n",
      "loss at batch 114880: 1.7406901121139526\n",
      "loss at batch 114944: 2.236128330230713\n",
      "loss at batch 115008: 2.211920976638794\n",
      "loss at batch 115072: 1.9006083011627197\n",
      "loss at batch 115136: 1.9630053043365479\n",
      "loss at batch 115200: 2.3988475799560547\n",
      "loss at batch 115264: 1.950146198272705\n",
      "loss at batch 115328: 2.194653272628784\n",
      "loss at batch 115392: 2.2666471004486084\n",
      "loss at batch 115456: 2.184582233428955\n",
      "loss at batch 115520: 2.232196569442749\n",
      "loss at batch 115584: 2.159963607788086\n",
      "loss at batch 115648: 1.8562538623809814\n",
      "loss at batch 115712: 2.176367998123169\n",
      "loss at batch 115776: 1.9612174034118652\n",
      "loss at batch 115840: 2.05094313621521\n",
      "loss at batch 115904: 1.8723727464675903\n",
      "loss at batch 115968: 2.0417094230651855\n",
      "loss at batch 116032: 1.8866684436798096\n",
      "loss at batch 116096: 2.0059382915496826\n",
      "loss at batch 116160: 2.036054849624634\n",
      "loss at batch 116224: 1.9852885007858276\n",
      "loss at batch 116288: 1.9741638898849487\n",
      "loss at batch 116352: 1.9863519668579102\n",
      "loss at batch 116416: 2.144510507583618\n",
      "loss at batch 116480: 1.9759429693222046\n",
      "loss at batch 116544: 1.860142469406128\n",
      "loss at batch 116608: 2.402405261993408\n",
      "loss at batch 116672: 1.9651292562484741\n",
      "loss at batch 116736: 1.9608631134033203\n",
      "loss at batch 116800: 1.9082221984863281\n",
      "loss at batch 116864: 1.9972755908966064\n",
      "loss at batch 116928: 2.203990936279297\n",
      "loss at batch 116992: 2.2170588970184326\n",
      "loss at batch 117056: 1.8650082349777222\n",
      "loss at batch 117120: 1.9913926124572754\n",
      "loss at batch 117184: 1.8340884447097778\n",
      "loss at batch 117248: 2.1762983798980713\n",
      "loss at batch 117312: 1.9523893594741821\n",
      "loss at batch 117376: 2.0414037704467773\n",
      "loss at batch 117440: 1.7187501192092896\n",
      "loss at batch 117504: 1.9435200691223145\n",
      "loss at batch 117568: 2.464850902557373\n",
      "loss at batch 117632: 1.8599148988723755\n",
      "loss at batch 117696: 2.1843199729919434\n",
      "loss at batch 117760: 2.120210647583008\n",
      "loss at batch 117824: 1.9465844631195068\n",
      "loss at batch 117888: 2.0455362796783447\n",
      "loss at batch 117952: 1.9394216537475586\n",
      "loss at batch 118016: 2.5651793479919434\n",
      "loss at batch 118080: 1.9123432636260986\n",
      "loss at batch 118144: 2.1199820041656494\n",
      "loss at batch 118208: 1.7887452840805054\n",
      "loss at batch 118272: 2.09696888923645\n",
      "loss at batch 118336: 2.0748424530029297\n",
      "loss at batch 118400: 2.188227415084839\n",
      "loss at batch 118464: 2.2721548080444336\n",
      "loss at batch 118528: 2.286764621734619\n",
      "loss at batch 118592: 2.141667127609253\n",
      "loss at batch 118656: 1.8863297700881958\n",
      "loss at batch 118720: 1.9831109046936035\n",
      "loss at batch 118784: 2.4429948329925537\n",
      "loss at batch 118848: 1.9325177669525146\n",
      "loss at batch 118912: 1.9198471307754517\n",
      "loss at batch 118976: 2.0749709606170654\n",
      "loss at batch 119040: 1.9638229608535767\n",
      "loss at batch 119104: 2.0531680583953857\n",
      "loss at batch 119168: 2.2036876678466797\n",
      "loss at batch 119232: 2.4572203159332275\n",
      "loss at batch 119296: 2.1418933868408203\n",
      "loss at batch 119360: 1.7588130235671997\n",
      "loss at batch 119424: 1.8567107915878296\n",
      "loss at batch 119488: 2.1081018447875977\n",
      "loss at batch 119552: 2.1200482845306396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 119616: 2.096625804901123\n",
      "loss at batch 119680: 2.1915876865386963\n",
      "loss at batch 119744: 2.0507328510284424\n",
      "loss at batch 119808: 1.7179436683654785\n",
      "loss at batch 119872: 1.9967347383499146\n",
      "loss at batch 119936: 2.739015579223633\n",
      "loss at batch 120000: 1.8611878156661987\n",
      "loss at batch 120064: 1.9733201265335083\n",
      "loss at batch 120128: 1.953633189201355\n",
      "loss at batch 120192: 2.1981959342956543\n",
      "loss at batch 120256: 2.1641950607299805\n",
      "loss at batch 120320: 1.9544620513916016\n",
      "loss at batch 120384: 1.7727874517440796\n",
      "loss at batch 120448: 2.3924829959869385\n",
      "loss at batch 120512: 1.8315553665161133\n",
      "loss at batch 120576: 2.380122661590576\n",
      "loss at batch 120640: 1.9153242111206055\n",
      "loss at batch 120704: 2.3052408695220947\n",
      "loss at batch 120768: 2.233654260635376\n",
      "loss at batch 120832: 1.915078043937683\n",
      "loss at batch 120896: 1.8165638446807861\n",
      "loss at batch 120960: 1.8588647842407227\n",
      "loss at batch 121024: 1.983673095703125\n",
      "loss at batch 121088: 2.417133092880249\n",
      "loss at batch 121152: 2.1946194171905518\n",
      "loss at batch 121216: 2.150841474533081\n",
      "loss at batch 121280: 1.9644118547439575\n",
      "loss at batch 121344: 2.0425257682800293\n",
      "loss at batch 121408: 1.7763153314590454\n",
      "loss at batch 121472: 2.0075886249542236\n",
      "loss at batch 121536: 1.9958196878433228\n",
      "loss at batch 121600: 1.8944745063781738\n",
      "loss at batch 121664: 2.0574147701263428\n",
      "loss at batch 121728: 1.9108363389968872\n",
      "loss at batch 121792: 2.443141222000122\n",
      "loss at batch 121856: 2.503190755844116\n",
      "loss at batch 121920: 1.9895799160003662\n",
      "loss at batch 121984: 1.993148684501648\n",
      "loss at batch 122048: 2.571159601211548\n",
      "loss at batch 122112: 1.9835535287857056\n",
      "loss at batch 122176: 1.9992741346359253\n",
      "loss at batch 122240: 1.9593349695205688\n",
      "loss at batch 122304: 1.9650605916976929\n",
      "loss at batch 122368: 2.4996869564056396\n",
      "loss at batch 122432: 2.3200795650482178\n",
      "loss at batch 122496: 1.807484745979309\n",
      "loss at batch 122560: 2.2033491134643555\n",
      "loss at batch 122624: 2.5116405487060547\n",
      "loss at batch 122688: 2.683089256286621\n",
      "loss at batch 122752: 1.9983246326446533\n",
      "loss at batch 122816: 1.8788448572158813\n",
      "loss at batch 122880: 2.0451107025146484\n",
      "loss at batch 122944: 1.999633550643921\n",
      "loss at batch 123008: 2.124752998352051\n",
      "loss at batch 123072: 2.005553960800171\n",
      "loss at batch 123136: 2.230349540710449\n",
      "loss at batch 123200: 1.9144622087478638\n",
      "loss at batch 123264: 2.0549113750457764\n",
      "loss at batch 123328: 2.1149675846099854\n",
      "loss at batch 123392: 2.550801992416382\n",
      "loss at batch 123456: 1.9561220407485962\n",
      "loss at batch 123520: 1.943873405456543\n",
      "loss at batch 123584: 2.3609139919281006\n",
      "loss at batch 123648: 1.871111273765564\n",
      "loss at batch 123712: 1.9457253217697144\n",
      "loss at batch 123776: 2.297940492630005\n",
      "loss at batch 123840: 2.5509350299835205\n",
      "loss at batch 123904: 2.0910468101501465\n",
      "loss at batch 123968: 1.8461145162582397\n",
      "loss at batch 124032: 1.929329514503479\n",
      "loss at batch 124096: 1.9995782375335693\n",
      "loss at batch 124160: 1.9045993089675903\n",
      "loss at batch 124224: 2.1119613647460938\n",
      "loss at batch 124288: 2.1477787494659424\n",
      "loss at batch 124352: 2.4566211700439453\n",
      "loss at batch 124416: 2.196563959121704\n",
      "loss at batch 124480: 1.8670507669448853\n",
      "loss at batch 124544: 2.0866172313690186\n",
      "loss at batch 124608: 1.8086262941360474\n",
      "loss at batch 124672: 2.170293092727661\n",
      "loss at batch 124736: 1.7153631448745728\n",
      "loss at batch 124800: 1.9742655754089355\n",
      "loss at batch 124864: 2.5168440341949463\n",
      "loss at batch 124928: 1.9273970127105713\n",
      "loss at batch 124992: 1.6700204610824585\n",
      "loss at batch 125056: 2.4223434925079346\n",
      "loss at batch 125120: 1.8237686157226562\n",
      "loss at batch 125184: 2.1211512088775635\n",
      "loss at batch 125248: 2.0177664756774902\n",
      "loss at batch 125312: 1.9914227724075317\n",
      "loss at batch 125376: 1.857070803642273\n",
      "loss at batch 125440: 1.9866900444030762\n",
      "loss at batch 125504: 2.0525548458099365\n",
      "loss at batch 125568: 2.118403911590576\n",
      "loss at batch 125632: 1.9581578969955444\n",
      "loss at batch 125696: 1.8035534620285034\n",
      "loss at batch 125760: 2.08758807182312\n",
      "loss at batch 125824: 2.392343759536743\n",
      "loss at batch 125888: 1.8345057964324951\n",
      "loss at batch 125952: 2.140306234359741\n",
      "loss at batch 126016: 2.2205450534820557\n",
      "loss at batch 126080: 1.749709129333496\n",
      "loss at batch 126144: 1.8417073488235474\n",
      "loss at batch 126208: 1.8100560903549194\n",
      "loss at batch 126272: 1.717106819152832\n",
      "loss at batch 126336: 2.03273606300354\n",
      "loss at batch 126400: 2.253248929977417\n",
      "loss at batch 126464: 1.893849492073059\n",
      "loss at batch 126528: 1.9958170652389526\n",
      "loss at batch 126592: 1.8129409551620483\n",
      "loss at batch 126656: 1.7884312868118286\n",
      "loss at batch 126720: 1.8512157201766968\n",
      "loss at batch 126784: 1.8650782108306885\n",
      "loss at batch 126848: 1.7539161443710327\n",
      "loss at batch 126912: 2.3200595378875732\n",
      "loss at batch 126976: 1.9611998796463013\n",
      "loss at batch 127040: 1.8546925783157349\n",
      "loss at batch 127104: 1.9771339893341064\n",
      "loss at batch 127168: 1.8266823291778564\n",
      "loss at batch 127232: 1.7324085235595703\n",
      "loss at batch 127296: 1.6217896938323975\n",
      "loss at batch 127360: 2.087148427963257\n",
      "loss at batch 127424: 1.9628833532333374\n",
      "loss at batch 127488: 1.9792345762252808\n",
      "loss at batch 127552: 1.7030107975006104\n",
      "loss at batch 127616: 2.1152682304382324\n",
      "loss at batch 127680: 1.8281656503677368\n",
      "loss at batch 127744: 1.9027163982391357\n",
      "loss at batch 127808: 2.7911720275878906\n",
      "loss at batch 127872: 1.8102134466171265\n",
      "loss at batch 127936: 1.8832601308822632\n",
      "loss at batch 128000: 1.9930944442749023\n",
      "loss at batch 128064: 1.7846343517303467\n",
      "loss at batch 128128: 2.240469217300415\n",
      "loss at batch 128192: 2.424818992614746\n",
      "loss at batch 128256: 2.2611680030822754\n",
      "loss at batch 128320: 2.0373411178588867\n",
      "loss at batch 128384: 1.690723180770874\n",
      "loss at batch 128448: 1.6666110754013062\n",
      "loss at batch 128512: 2.112811326980591\n",
      "loss at batch 128576: 1.95779287815094\n",
      "loss at batch 128640: 2.0749733448028564\n",
      "loss at batch 128704: 2.086461305618286\n",
      "loss at batch 128768: 1.7674938440322876\n",
      "loss at batch 128832: 2.036054849624634\n",
      "loss at batch 128896: 1.9239317178726196\n",
      "loss at batch 128960: 2.069514513015747\n",
      "loss at batch 129024: 1.8755865097045898\n",
      "loss at batch 129088: 1.7644091844558716\n",
      "loss at batch 129152: 2.46829891204834\n",
      "loss at batch 129216: 1.9490925073623657\n",
      "loss at batch 129280: 2.0069727897644043\n",
      "loss at batch 129344: 1.9686386585235596\n",
      "loss at batch 129408: 1.9268150329589844\n",
      "loss at batch 129472: 2.453239679336548\n",
      "loss at batch 129536: 1.9408187866210938\n",
      "loss at batch 129600: 1.880372166633606\n",
      "loss at batch 129664: 1.9652535915374756\n",
      "loss at batch 129728: 1.7440435886383057\n",
      "loss at batch 129792: 1.99759042263031\n",
      "loss at batch 129856: 1.9031484127044678\n",
      "loss at batch 129920: 1.7439628839492798\n",
      "loss at batch 129984: 1.896262288093567\n",
      "loss at batch 130048: 1.8193154335021973\n",
      "loss at batch 130112: 1.8153380155563354\n",
      "loss at batch 130176: 1.9923665523529053\n",
      "loss at batch 130240: 1.9045288562774658\n",
      "loss at batch 130304: 2.3761448860168457\n",
      "loss at batch 130368: 1.8673279285430908\n",
      "loss at batch 130432: 1.8079745769500732\n",
      "loss at batch 130496: 2.1367318630218506\n",
      "loss at batch 130560: 2.2289812564849854\n",
      "loss at batch 130624: 2.1137471199035645\n",
      "loss at batch 130688: 2.6147866249084473\n",
      "loss at batch 130752: 1.8169323205947876\n",
      "loss at batch 130816: 1.8134512901306152\n",
      "loss at batch 130880: 1.8280701637268066\n",
      "loss at batch 130944: 1.8769278526306152\n",
      "loss at batch 131008: 1.8451834917068481\n",
      "loss at batch 131072: 1.859477162361145\n",
      "loss at batch 131136: 1.9400173425674438\n",
      "loss at batch 131200: 2.2329041957855225\n",
      "loss at batch 131264: 2.5690672397613525\n",
      "loss at batch 131328: 1.434362530708313\n",
      "loss at batch 131392: 1.8807523250579834\n",
      "loss at batch 131456: 2.2224066257476807\n",
      "loss at batch 131520: 1.8590688705444336\n",
      "loss at batch 131584: 1.760672926902771\n",
      "loss at batch 131648: 1.7805473804473877\n",
      "loss at batch 131712: 1.9829899072647095\n",
      "loss at batch 131776: 1.6708627939224243\n",
      "loss at batch 131840: 2.312446355819702\n",
      "loss at batch 131904: 1.8258650302886963\n",
      "loss at batch 131968: 2.0436410903930664\n",
      "loss at batch 132032: 2.297983169555664\n",
      "loss at batch 132096: 1.8282086849212646\n",
      "loss at batch 132160: 1.8247994184494019\n",
      "loss at batch 132224: 1.8067033290863037\n",
      "loss at batch 132288: 1.9442909955978394\n",
      "loss at batch 132352: 1.7273716926574707\n",
      "loss at batch 132416: 1.9533164501190186\n",
      "loss at batch 132480: 1.9052612781524658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 132544: 2.203211784362793\n",
      "loss at batch 132608: 2.270289421081543\n",
      "loss at batch 132672: 1.9125767946243286\n",
      "loss at batch 132736: 1.8594121932983398\n",
      "loss at batch 132800: 1.983016014099121\n",
      "loss at batch 132864: 1.71877920627594\n",
      "loss at batch 132928: 2.2158360481262207\n",
      "loss at batch 132992: 2.111052989959717\n",
      "loss at batch 133056: 1.7610068321228027\n",
      "loss at batch 133120: 1.794886589050293\n",
      "loss at batch 133184: 1.9457365274429321\n",
      "loss at batch 133248: 1.9893457889556885\n",
      "loss at batch 133312: 1.8516536951065063\n",
      "loss at batch 133376: 1.6202583312988281\n",
      "loss at batch 133440: 1.915684461593628\n",
      "loss at batch 133504: 2.0485661029815674\n",
      "loss at batch 133568: 2.0632143020629883\n",
      "loss at batch 133632: 2.0346806049346924\n",
      "loss at batch 133696: 1.8127721548080444\n",
      "loss at batch 133760: 2.0226187705993652\n",
      "loss at batch 133824: 2.0884456634521484\n",
      "loss at batch 133888: 1.9393126964569092\n",
      "loss at batch 133952: 1.9001413583755493\n",
      "loss at batch 134016: 1.8320338726043701\n",
      "loss at batch 134080: 2.4411990642547607\n",
      "loss at batch 134144: 1.9474592208862305\n",
      "loss at batch 134208: 2.1023268699645996\n",
      "loss at batch 134272: 2.0387399196624756\n",
      "loss at batch 134336: 2.2023749351501465\n",
      "loss at batch 134400: 2.130990505218506\n",
      "loss at batch 134464: 1.6337424516677856\n",
      "loss at batch 134528: 2.1694116592407227\n",
      "loss at batch 134592: 1.9401359558105469\n",
      "loss at batch 134656: 1.9802615642547607\n",
      "loss at batch 134720: 2.43019700050354\n",
      "loss at batch 134784: 2.0494847297668457\n",
      "loss at batch 134848: 2.0378873348236084\n",
      "loss at batch 134912: 1.8055310249328613\n",
      "loss at batch 134976: 2.2304177284240723\n",
      "loss at batch 135040: 2.1195545196533203\n",
      "loss at batch 135104: 2.042579174041748\n",
      "loss at batch 135168: 1.7775087356567383\n",
      "loss at batch 135232: 2.2647180557250977\n",
      "loss at batch 135296: 1.7227470874786377\n",
      "loss at batch 135360: 1.4018795490264893\n",
      "loss at batch 135424: 2.1296558380126953\n",
      "loss at batch 135488: 1.8555749654769897\n",
      "loss at batch 135552: 2.088168144226074\n",
      "loss at batch 135616: 1.8334152698516846\n",
      "loss at batch 135680: 1.967819333076477\n",
      "loss at batch 135744: 1.9670466184616089\n",
      "loss at batch 135808: 2.6644961833953857\n",
      "loss at batch 135872: 1.9036338329315186\n",
      "loss at batch 135936: 1.92056143283844\n",
      "loss at batch 136000: 2.005319118499756\n",
      "loss at batch 136064: 2.077585458755493\n",
      "loss at batch 136128: 2.2533576488494873\n",
      "loss at batch 136192: 1.9199635982513428\n",
      "loss at batch 136256: 1.9943662881851196\n",
      "loss at batch 136320: 2.371371269226074\n",
      "loss at batch 136384: 1.9415196180343628\n",
      "loss at batch 136448: 2.1425817012786865\n",
      "loss at batch 136512: 1.9278671741485596\n",
      "loss at batch 136576: 1.83793306350708\n",
      "loss at batch 136640: 2.1213698387145996\n",
      "loss at batch 136704: 1.8639546632766724\n",
      "loss at batch 136768: 2.325565814971924\n",
      "loss at batch 136832: 1.6751112937927246\n",
      "loss at batch 136896: 2.0356483459472656\n",
      "loss at batch 136960: 1.8473325967788696\n",
      "loss at batch 137024: 1.644060492515564\n",
      "loss at batch 137088: 1.8157854080200195\n",
      "loss at batch 137152: 1.893224835395813\n",
      "loss at batch 137216: 2.587761640548706\n",
      "loss at batch 137280: 2.3622848987579346\n",
      "loss at batch 137344: 1.925069808959961\n",
      "loss at batch 137408: 2.1113290786743164\n",
      "loss at batch 137472: 2.082258701324463\n",
      "loss at batch 137536: 1.9679523706436157\n",
      "loss at batch 137600: 1.9165103435516357\n",
      "loss at batch 137664: 2.0564911365509033\n",
      "loss at batch 137728: 1.644719123840332\n",
      "loss at batch 137792: 2.0843422412872314\n",
      "loss at batch 137856: 1.9801958799362183\n",
      "loss at batch 137920: 1.4956780672073364\n",
      "loss at batch 137984: 2.267655849456787\n",
      "loss at batch 138048: 1.7816911935806274\n",
      "loss at batch 138112: 1.8826080560684204\n",
      "loss at batch 138176: 1.5770719051361084\n",
      "loss at batch 138240: 2.322971820831299\n",
      "loss at batch 138304: 2.2374253273010254\n",
      "loss at batch 138368: 1.7218397855758667\n",
      "loss at batch 138432: 1.8813358545303345\n",
      "loss at batch 138496: 1.7871958017349243\n",
      "loss at batch 138560: 1.9487638473510742\n",
      "loss at batch 138624: 2.126302719116211\n",
      "loss at batch 138688: 1.8859000205993652\n",
      "loss at batch 138752: 1.9003323316574097\n",
      "loss at batch 138816: 1.9819546937942505\n",
      "loss at batch 138880: 1.572070598602295\n",
      "loss at batch 138944: 2.191563844680786\n",
      "loss at batch 139008: 1.8453450202941895\n",
      "loss at batch 139072: 1.6724092960357666\n",
      "loss at batch 139136: 1.8360595703125\n",
      "loss at batch 139200: 1.919335126876831\n",
      "loss at batch 139264: 2.1463558673858643\n",
      "loss at batch 139328: 1.862837791442871\n",
      "loss at batch 139392: 1.778578281402588\n",
      "loss at batch 139456: 1.7668482065200806\n",
      "loss at batch 139520: 2.352640151977539\n",
      "loss at batch 139584: 1.9768834114074707\n",
      "loss at batch 139648: 1.683709979057312\n",
      "loss at batch 139712: 2.2348341941833496\n",
      "loss at batch 139776: 1.873896598815918\n",
      "loss at batch 139840: 2.1237995624542236\n",
      "loss at batch 139904: 1.7540208101272583\n",
      "loss at batch 139968: 1.6788640022277832\n",
      "loss at batch 140032: 1.8193659782409668\n",
      "loss at batch 140096: 2.1297547817230225\n",
      "loss at batch 140160: 1.8865808248519897\n",
      "loss at batch 140224: 2.038226366043091\n",
      "loss at batch 140288: 1.9339600801467896\n",
      "loss at batch 140352: 1.8328263759613037\n",
      "loss at batch 140416: 2.232048988342285\n",
      "loss at batch 140480: 1.7938785552978516\n",
      "loss at batch 140544: 1.645561695098877\n",
      "loss at batch 140608: 1.516394853591919\n",
      "loss at batch 140672: 2.0814342498779297\n",
      "loss at batch 140736: 2.4086713790893555\n",
      "loss at batch 140800: 1.663232684135437\n",
      "loss at batch 140864: 2.07499098777771\n",
      "loss at batch 140928: 1.7525404691696167\n",
      "loss at batch 140992: 1.6460506916046143\n",
      "loss at batch 141056: 1.6366227865219116\n",
      "loss at batch 141120: 2.0390236377716064\n",
      "loss at batch 141184: 2.277965545654297\n",
      "loss at batch 141248: 1.7089742422103882\n",
      "loss at batch 141312: 1.6091996431350708\n",
      "loss at batch 141376: 1.5882643461227417\n",
      "loss at batch 141440: 1.8307323455810547\n",
      "loss at batch 141504: 1.9559022188186646\n",
      "loss at batch 141568: 1.5772302150726318\n",
      "loss at batch 141632: 1.8097327947616577\n",
      "loss at batch 141696: 2.519911766052246\n",
      "loss at batch 141760: 1.9906995296478271\n",
      "loss at batch 141824: 1.957933783531189\n",
      "loss at batch 141888: 1.7750794887542725\n",
      "loss at batch 141952: 2.067861557006836\n",
      "loss at batch 142016: 2.0389018058776855\n",
      "loss at batch 142080: 1.7644931077957153\n",
      "loss at batch 142144: 1.8047728538513184\n",
      "loss at batch 142208: 1.7371199131011963\n",
      "loss at batch 142272: 1.8910099267959595\n",
      "loss at batch 142336: 1.837065577507019\n",
      "loss at batch 142400: 1.9820491075515747\n",
      "loss at batch 142464: 1.770052433013916\n",
      "loss at batch 142528: 1.872837781906128\n",
      "loss at batch 142592: 1.993739128112793\n",
      "loss at batch 142656: 2.2043519020080566\n",
      "loss at batch 142720: 2.1963083744049072\n",
      "loss at batch 142784: 1.7208787202835083\n",
      "loss at batch 142848: 2.2333474159240723\n",
      "loss at batch 142912: 1.9740395545959473\n",
      "loss at batch 142976: 1.8839106559753418\n",
      "loss at batch 143040: 2.678386688232422\n",
      "loss at batch 143104: 1.7517821788787842\n",
      "loss at batch 143168: 1.6919902563095093\n",
      "loss at batch 143232: 2.086397886276245\n",
      "loss at batch 143296: 1.6146695613861084\n",
      "loss at batch 143360: 1.8165268898010254\n",
      "loss at batch 143424: 2.7552287578582764\n",
      "loss at batch 143488: 1.7686668634414673\n",
      "loss at batch 143552: 1.6898274421691895\n",
      "loss at batch 143616: 1.7484712600708008\n",
      "loss at batch 143680: 2.0125601291656494\n",
      "loss at batch 143744: 2.1917173862457275\n",
      "loss at batch 143808: 1.7880032062530518\n",
      "loss at batch 143872: 2.1710619926452637\n",
      "loss at batch 143936: 1.8364883661270142\n",
      "loss at batch 144000: 2.103559732437134\n",
      "loss at batch 144064: 1.9902849197387695\n",
      "loss at batch 144128: 2.0206093788146973\n",
      "loss at batch 144192: 1.714347243309021\n",
      "loss at batch 144256: 1.9174474477767944\n",
      "loss at batch 144320: 1.87101149559021\n",
      "loss at batch 144384: 1.358747124671936\n",
      "loss at batch 144448: 1.7672643661499023\n",
      "loss at batch 144512: 2.644383192062378\n",
      "loss at batch 144576: 1.7473955154418945\n",
      "loss at batch 144640: 2.0898075103759766\n",
      "loss at batch 144704: 1.7070831060409546\n",
      "loss at batch 144768: 1.700784683227539\n",
      "loss at batch 144832: 1.7087323665618896\n",
      "loss at batch 144896: 2.0503389835357666\n",
      "loss at batch 144960: 1.8382048606872559\n",
      "loss at batch 145024: 1.9509328603744507\n",
      "loss at batch 145088: 1.7731417417526245\n",
      "loss at batch 145152: 2.2951138019561768\n",
      "loss at batch 145216: 1.7247326374053955\n",
      "loss at batch 145280: 1.978466272354126\n",
      "loss at batch 145344: 2.00767183303833\n",
      "loss at batch 145408: 1.9665437936782837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 145472: 2.134284019470215\n",
      "loss at batch 145536: 1.558081030845642\n",
      "loss at batch 145600: 1.8190181255340576\n",
      "loss at batch 145664: 1.8624484539031982\n",
      "loss at batch 145728: 1.7239240407943726\n",
      "loss at batch 145792: 1.5920964479446411\n",
      "loss at batch 145856: 2.508662223815918\n",
      "loss at batch 145920: 1.9139999151229858\n",
      "loss at batch 145984: 1.7023824453353882\n",
      "loss at batch 146048: 1.7131294012069702\n",
      "loss at batch 146112: 2.1901161670684814\n",
      "loss at batch 146176: 1.702165961265564\n",
      "loss at batch 146240: 1.7070679664611816\n",
      "loss at batch 146304: 1.655532956123352\n",
      "loss at batch 146368: 1.8542927503585815\n",
      "loss at batch 146432: 1.9908299446105957\n",
      "loss at batch 146496: 1.6107523441314697\n",
      "loss at batch 146560: 1.647210955619812\n",
      "loss at batch 146624: 1.5285272598266602\n",
      "loss at batch 146688: 1.6775530576705933\n",
      "loss at batch 146752: 1.9583704471588135\n",
      "loss at batch 146816: 2.596186876296997\n",
      "loss at batch 146880: 1.7386478185653687\n",
      "loss at batch 146944: 2.367631196975708\n",
      "loss at batch 147008: 2.05400013923645\n",
      "loss at batch 147072: 1.7689422369003296\n",
      "loss at batch 147136: 1.8391315937042236\n",
      "loss at batch 147200: 1.9865623712539673\n",
      "loss at batch 147264: 1.7184360027313232\n",
      "loss at batch 147328: 1.5008350610733032\n",
      "loss at batch 147392: 1.586233377456665\n",
      "loss at batch 147456: 1.8118417263031006\n",
      "loss at batch 147520: 1.5944490432739258\n",
      "loss at batch 147584: 1.6802464723587036\n",
      "loss at batch 147648: 1.7050387859344482\n",
      "loss at batch 147712: 1.6955347061157227\n",
      "loss at batch 147776: 1.9515035152435303\n",
      "loss at batch 147840: 1.9595645666122437\n",
      "loss at batch 147904: 2.3538413047790527\n",
      "loss at batch 147968: 1.7646682262420654\n",
      "loss at batch 148032: 1.7650184631347656\n",
      "loss at batch 148096: 1.9255530834197998\n",
      "loss at batch 148160: 1.6054044961929321\n",
      "loss at batch 148224: 1.935968279838562\n",
      "loss at batch 148288: 1.7074370384216309\n",
      "loss at batch 148352: 1.9304986000061035\n",
      "loss at batch 148416: 2.0957188606262207\n",
      "loss at batch 148480: 1.8179677724838257\n",
      "loss at batch 148544: 1.9796048402786255\n",
      "loss at batch 148608: 1.651549220085144\n",
      "loss at batch 148672: 2.380261182785034\n",
      "loss at batch 148736: 1.7244486808776855\n",
      "loss at batch 148800: 2.3056399822235107\n",
      "loss at batch 148864: 1.7038147449493408\n",
      "loss at batch 148928: 1.6350364685058594\n",
      "loss at batch 148992: 2.1710996627807617\n",
      "loss at batch 149056: 1.8977735042572021\n",
      "loss at batch 149120: 1.9000953435897827\n",
      "loss at batch 149184: 1.8887134790420532\n",
      "loss at batch 149248: 1.657285213470459\n",
      "loss at batch 149312: 1.9186553955078125\n",
      "loss at batch 149376: 1.8955992460250854\n",
      "loss at batch 149440: 2.0945770740509033\n",
      "loss at batch 149504: 2.349885940551758\n",
      "loss at batch 149568: 1.7985312938690186\n",
      "loss at batch 149632: 1.918355941772461\n",
      "loss at batch 149696: 1.7468867301940918\n",
      "loss at batch 149760: 3.1186318397521973\n",
      "loss at batch 149824: 1.6611515283584595\n",
      "loss at batch 149888: 1.7482843399047852\n",
      "loss at batch 149952: 1.822151780128479\n",
      "loss at batch 150016: 1.8966799974441528\n",
      "loss at batch 150080: 1.8469594717025757\n",
      "loss at batch 150144: 1.9604538679122925\n",
      "loss at batch 150208: 1.7038968801498413\n",
      "loss at batch 150272: 1.7559946775436401\n",
      "loss at batch 150336: 1.5764737129211426\n",
      "loss at batch 150400: 1.640503168106079\n",
      "loss at batch 150464: 1.774598479270935\n",
      "loss at batch 150528: 1.7579662799835205\n",
      "loss at batch 150592: 2.354820728302002\n",
      "loss at batch 150656: 1.9836318492889404\n",
      "loss at batch 150720: 1.6540495157241821\n",
      "loss at batch 150784: 1.6161361932754517\n",
      "loss at batch 150848: 1.9458383321762085\n",
      "loss at batch 150912: 1.465933084487915\n",
      "loss at batch 150976: 2.1203246116638184\n",
      "loss at batch 151040: 2.4086220264434814\n",
      "loss at batch 151104: 2.2998738288879395\n",
      "loss at batch 151168: 1.8440700769424438\n",
      "loss at batch 151232: 1.8884735107421875\n",
      "loss at batch 151296: 1.6032295227050781\n",
      "loss at batch 151360: 1.8536332845687866\n",
      "loss at batch 151424: 1.5683194398880005\n",
      "loss at batch 151488: 1.6005642414093018\n",
      "loss at batch 151552: 1.785563588142395\n",
      "loss at batch 151616: 2.349172592163086\n",
      "loss at batch 151680: 1.5945380926132202\n",
      "loss at batch 151744: 1.9956228733062744\n",
      "loss at batch 151808: 1.959225058555603\n",
      "loss at batch 151872: 1.5966428518295288\n",
      "loss at batch 151936: 1.7379556894302368\n",
      "loss at batch 152000: 1.6250191926956177\n",
      "loss at batch 152064: 2.5022664070129395\n",
      "loss at batch 152128: 2.278179168701172\n",
      "loss at batch 152192: 1.7079473733901978\n",
      "loss at batch 152256: 2.7754886150360107\n",
      "loss at batch 152320: 1.8034467697143555\n",
      "loss at batch 152384: 1.7657883167266846\n",
      "loss at batch 152448: 2.086303472518921\n",
      "loss at batch 152512: 1.5669100284576416\n",
      "loss at batch 152576: 1.6104577779769897\n",
      "loss at batch 152640: 1.7187343835830688\n",
      "loss at batch 152704: 2.8803305625915527\n",
      "loss at batch 152768: 2.015230894088745\n",
      "loss at batch 152832: 1.5121835470199585\n",
      "loss at batch 152896: 2.2477200031280518\n",
      "loss at batch 152960: 1.790226936340332\n",
      "loss at batch 153024: 2.070875883102417\n",
      "loss at batch 153088: 1.6132146120071411\n",
      "loss at batch 153152: 1.6276886463165283\n",
      "loss at batch 153216: 2.0067262649536133\n",
      "loss at batch 153280: 2.1296496391296387\n",
      "loss at batch 153344: 1.585696816444397\n",
      "loss at batch 153408: 1.7846307754516602\n",
      "loss at batch 153472: 1.9567309617996216\n",
      "loss at batch 153536: 1.6852200031280518\n",
      "loss at batch 153600: 1.8253462314605713\n",
      "loss at batch 153664: 1.9757139682769775\n",
      "loss at batch 153728: 1.9096404314041138\n",
      "loss at batch 153792: 1.6942516565322876\n",
      "loss at batch 153856: 1.495042324066162\n",
      "loss at batch 153920: 1.6228781938552856\n",
      "loss at batch 153984: 1.67926025390625\n",
      "loss at batch 154048: 1.9719094038009644\n",
      "loss at batch 154112: 1.7984451055526733\n",
      "loss at batch 154176: 1.7322304248809814\n",
      "loss at batch 154240: 1.6936975717544556\n",
      "loss at batch 154304: 2.0493221282958984\n",
      "loss at batch 154368: 2.6782162189483643\n",
      "loss at batch 154432: 1.5703303813934326\n",
      "loss at batch 154496: 1.7710298299789429\n",
      "loss at batch 154560: 1.692726492881775\n",
      "loss at batch 154624: 1.8637235164642334\n",
      "loss at batch 154688: 1.5918093919754028\n",
      "loss at batch 154752: 1.7913514375686646\n",
      "loss at batch 154816: 1.5578832626342773\n",
      "loss at batch 154880: 2.4408211708068848\n",
      "loss at batch 154944: 1.838350772857666\n",
      "loss at batch 155008: 1.7927192449569702\n",
      "loss at batch 155072: 1.609978199005127\n",
      "loss at batch 155136: 1.7578914165496826\n",
      "loss at batch 155200: 1.8280465602874756\n",
      "loss at batch 155264: 1.6189905405044556\n",
      "loss at batch 155328: 1.915351390838623\n",
      "loss at batch 155392: 1.871793508529663\n",
      "loss at batch 155456: 1.5865812301635742\n",
      "loss at batch 155520: 2.272435426712036\n",
      "loss at batch 155584: 1.8117767572402954\n",
      "loss at batch 155648: 1.9862499237060547\n",
      "loss at batch 155712: 1.7459126710891724\n",
      "loss at batch 155776: 1.8140623569488525\n",
      "loss at batch 155840: 2.2385499477386475\n",
      "loss at batch 155904: 2.4185190200805664\n",
      "loss at batch 155968: 1.6443156003952026\n",
      "loss at batch 156032: 1.9778400659561157\n",
      "loss at batch 156096: 1.3821659088134766\n",
      "loss at batch 156160: 1.690430760383606\n",
      "loss at batch 156224: 1.7321300506591797\n",
      "loss at batch 156288: 2.567203998565674\n",
      "loss at batch 156352: 2.0247550010681152\n",
      "loss at batch 156416: 1.792428731918335\n",
      "loss at batch 156480: 1.7425638437271118\n",
      "loss at batch 156544: 1.5539168119430542\n",
      "loss at batch 156608: 2.016362428665161\n",
      "loss at batch 156672: 1.7037559747695923\n",
      "loss at batch 156736: 1.856393814086914\n",
      "loss at batch 156800: 1.4438307285308838\n",
      "loss at batch 156864: 1.7001811265945435\n",
      "loss at batch 156928: 1.6379997730255127\n",
      "loss at batch 156992: 1.6091804504394531\n",
      "loss at batch 157056: 1.4743759632110596\n",
      "loss at batch 157120: 1.9628022909164429\n",
      "loss at batch 157184: 1.8526121377944946\n",
      "loss at batch 157248: 1.739406943321228\n",
      "loss at batch 157312: 1.5141503810882568\n",
      "loss at batch 157376: 2.122222900390625\n",
      "loss at batch 157440: 1.6675525903701782\n",
      "loss at batch 157504: 2.5405490398406982\n",
      "loss at batch 157568: 1.6001338958740234\n",
      "loss at batch 157632: 1.49332857131958\n",
      "loss at batch 157696: 1.5170609951019287\n",
      "loss at batch 157760: 2.03288197517395\n",
      "loss at batch 157824: 1.6898893117904663\n",
      "loss at batch 157888: 1.8983279466629028\n",
      "loss at batch 157952: 1.5970933437347412\n",
      "loss at batch 158016: 2.006582260131836\n",
      "loss at batch 158080: 1.8110392093658447\n",
      "loss at batch 158144: 1.500166893005371\n",
      "loss at batch 158208: 1.693943738937378\n",
      "loss at batch 158272: 1.3970723152160645\n",
      "loss at batch 158336: 1.8542438745498657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 158400: 1.601049780845642\n",
      "loss at batch 158464: 1.6048848628997803\n",
      "loss at batch 158528: 2.237887382507324\n",
      "loss at batch 158592: 2.1324288845062256\n",
      "loss at batch 158656: 1.8737506866455078\n",
      "loss at batch 158720: 2.015547037124634\n",
      "loss at batch 158784: 1.7053449153900146\n",
      "loss at batch 158848: 1.4822779893875122\n",
      "loss at batch 158912: 1.9389257431030273\n",
      "loss at batch 158976: 1.5690630674362183\n",
      "loss at batch 159040: 1.483041763305664\n",
      "loss at batch 159104: 1.8182108402252197\n",
      "loss at batch 159168: 1.5927214622497559\n",
      "loss at batch 159232: 2.195535182952881\n",
      "loss at batch 159296: 2.0783376693725586\n",
      "loss at batch 159360: 2.338857889175415\n",
      "loss at batch 159424: 1.3193624019622803\n",
      "loss at batch 159488: 1.6255762577056885\n",
      "loss at batch 159552: 1.4662278890609741\n",
      "loss at batch 159616: 1.8440414667129517\n",
      "loss at batch 159680: 1.3137773275375366\n",
      "loss at batch 159744: 1.8073910474777222\n",
      "loss at batch 159808: 1.5212979316711426\n",
      "loss at batch 159872: 1.5443812608718872\n",
      "loss at batch 159936: 1.783429741859436\n",
      "loss at batch 160000: 1.8323993682861328\n",
      "loss at batch 160064: 1.4651319980621338\n",
      "loss at batch 160128: 1.745517611503601\n",
      "loss at batch 160192: 1.6481988430023193\n",
      "loss at batch 160256: 1.6251516342163086\n",
      "loss at batch 160320: 1.7197626829147339\n",
      "loss at batch 160384: 1.8697550296783447\n",
      "loss at batch 160448: 1.8500841856002808\n",
      "loss at batch 160512: 1.6298316717147827\n",
      "loss at batch 160576: 2.538055419921875\n",
      "loss at batch 160640: 1.9368045330047607\n",
      "loss at batch 160704: 1.8408499956130981\n",
      "loss at batch 160768: 2.371898651123047\n",
      "loss at batch 160832: 1.5354453325271606\n",
      "loss at batch 160896: 1.6347577571868896\n",
      "loss at batch 160960: 1.8264847993850708\n",
      "loss at batch 161024: 1.642914891242981\n",
      "loss at batch 161088: 1.720424771308899\n",
      "loss at batch 161152: 1.9359265565872192\n",
      "loss at batch 161216: 1.640978455543518\n",
      "loss at batch 161280: 1.7438154220581055\n",
      "loss at batch 161344: 1.6409350633621216\n",
      "loss at batch 161408: 1.6809090375900269\n",
      "loss at batch 161472: 1.602125883102417\n",
      "loss at batch 161536: 1.4079880714416504\n",
      "loss at batch 161600: 2.025282144546509\n",
      "loss at batch 161664: 2.1254279613494873\n",
      "loss at batch 161728: 2.684671640396118\n",
      "loss at batch 161792: 1.8370965719223022\n",
      "loss at batch 161856: 1.5991241931915283\n",
      "loss at batch 161920: 2.6178505420684814\n",
      "loss at batch 161984: 2.010952949523926\n",
      "loss at batch 162048: 1.7408055067062378\n",
      "loss at batch 162112: 1.6974098682403564\n",
      "loss at batch 162176: 2.333951711654663\n",
      "loss at batch 162240: 1.6861709356307983\n",
      "loss at batch 162304: 1.4722843170166016\n",
      "loss at batch 162368: 1.6705968379974365\n",
      "loss at batch 162432: 1.7075731754302979\n",
      "loss at batch 162496: 1.8941752910614014\n",
      "loss at batch 162560: 1.7780367136001587\n",
      "loss at batch 162624: 1.5782994031906128\n",
      "loss at batch 162688: 1.9292428493499756\n",
      "loss at batch 162752: 2.1827337741851807\n",
      "loss at batch 162816: 1.708677887916565\n",
      "loss at batch 162880: 1.6676334142684937\n",
      "loss at batch 162944: 2.0949337482452393\n",
      "loss at batch 163008: 1.8992186784744263\n",
      "loss at batch 163072: 2.073273181915283\n",
      "loss at batch 163136: 1.9224456548690796\n",
      "loss at batch 163200: 1.9079573154449463\n",
      "loss at batch 163264: 1.6369853019714355\n",
      "loss at batch 163328: 1.7514945268630981\n",
      "loss at batch 163392: 1.563422441482544\n",
      "loss at batch 163456: 1.4617748260498047\n",
      "loss at batch 163520: 1.7610692977905273\n",
      "loss at batch 163584: 1.6909655332565308\n",
      "loss at batch 163648: 1.5378884077072144\n",
      "loss at batch 163712: 2.1381828784942627\n",
      "loss at batch 163776: 1.733751893043518\n",
      "loss at batch 163840: 1.518314242362976\n",
      "loss at batch 163904: 2.4244585037231445\n",
      "loss at batch 163968: 1.4840508699417114\n",
      "loss at batch 164032: 1.7433154582977295\n",
      "loss at batch 164096: 1.8550846576690674\n",
      "loss at batch 164160: 1.829453706741333\n",
      "loss at batch 164224: 1.557658314704895\n",
      "loss at batch 164288: 1.741241216659546\n",
      "loss at batch 164352: 1.4265836477279663\n",
      "loss at batch 164416: 1.8471108675003052\n",
      "loss at batch 164480: 1.5852328538894653\n",
      "loss at batch 164544: 1.7909938097000122\n",
      "loss at batch 164608: 1.4185559749603271\n",
      "loss at batch 164672: 1.8105089664459229\n",
      "loss at batch 164736: 1.7954177856445312\n",
      "loss at batch 164800: 1.3335906267166138\n",
      "loss at batch 164864: 2.1728403568267822\n",
      "loss at batch 164928: 2.0088462829589844\n",
      "loss at batch 164992: 1.738237977027893\n",
      "loss at batch 165056: 1.8610948324203491\n",
      "loss at batch 165120: 1.8821189403533936\n",
      "loss at batch 165184: 1.9256747961044312\n",
      "loss at batch 165248: 1.7553445100784302\n",
      "loss at batch 165312: 2.470358371734619\n",
      "loss at batch 165376: 1.6671414375305176\n",
      "loss at batch 165440: 1.7664557695388794\n",
      "loss at batch 165504: 1.78502357006073\n",
      "loss at batch 165568: 2.0697641372680664\n",
      "loss at batch 165632: 2.488509178161621\n",
      "loss at batch 165696: 1.9496803283691406\n",
      "loss at batch 165760: 1.891426920890808\n",
      "loss at batch 165824: 1.8471561670303345\n",
      "loss at batch 165888: 1.8377090692520142\n",
      "loss at batch 165952: 1.7215431928634644\n",
      "loss at batch 166016: 1.7575674057006836\n",
      "loss at batch 166080: 1.6305826902389526\n",
      "loss at batch 166144: 1.5216283798217773\n",
      "loss at batch 166208: 1.6103469133377075\n",
      "loss at batch 166272: 1.5318187475204468\n",
      "loss at batch 166336: 1.525359034538269\n",
      "loss at batch 166400: 2.0149190425872803\n",
      "loss at batch 166464: 1.9046021699905396\n",
      "loss at batch 166528: 1.5642834901809692\n",
      "loss at batch 166592: 2.4142777919769287\n",
      "loss at batch 166656: 2.049978494644165\n",
      "loss at batch 166720: 1.7413489818572998\n",
      "loss at batch 166784: 1.2710057497024536\n",
      "loss at batch 166848: 1.8954358100891113\n",
      "loss at batch 166912: 1.7739181518554688\n",
      "loss at batch 166976: 1.6901308298110962\n",
      "loss at batch 167040: 1.8388640880584717\n",
      "loss at batch 167104: 1.7083076238632202\n",
      "loss at batch 167168: 1.4874414205551147\n",
      "loss at batch 167232: 1.906913161277771\n",
      "loss at batch 167296: 1.6612812280654907\n",
      "loss at batch 167360: 2.2387921810150146\n",
      "loss at batch 167424: 2.155777931213379\n",
      "loss at batch 167488: 2.4744906425476074\n",
      "loss at batch 167552: 2.456165075302124\n",
      "loss at batch 167616: 1.8186519145965576\n",
      "loss at batch 167680: 1.324924349784851\n",
      "loss at batch 167744: 2.0046520233154297\n",
      "loss at batch 167808: 1.8031136989593506\n",
      "loss at batch 167872: 1.5848842859268188\n",
      "loss at batch 167936: 2.391650438308716\n",
      "loss at batch 168000: 1.8684989213943481\n",
      "loss at batch 168064: 1.7345479726791382\n",
      "loss at batch 168128: 1.838926911354065\n",
      "loss at batch 168192: 1.7773771286010742\n",
      "loss at batch 168256: 1.7393817901611328\n",
      "loss at batch 168320: 1.610567569732666\n",
      "loss at batch 168384: 1.9518073797225952\n",
      "loss at batch 168448: 1.6422395706176758\n",
      "loss at batch 168512: 1.8194162845611572\n",
      "loss at batch 168576: 1.4380135536193848\n",
      "loss at batch 168640: 1.711268663406372\n",
      "loss at batch 168704: 1.8009263277053833\n",
      "loss at batch 168768: 1.437090516090393\n",
      "loss at batch 168832: 1.5648326873779297\n",
      "loss at batch 168896: 1.589231252670288\n",
      "loss at batch 168960: 1.4364709854125977\n",
      "loss at batch 169024: 1.57310950756073\n",
      "loss at batch 169088: 1.4774378538131714\n",
      "loss at batch 169152: 1.4313031435012817\n",
      "loss at batch 169216: 2.0417699813842773\n",
      "loss at batch 169280: 1.986473798751831\n",
      "loss at batch 169344: 1.333211898803711\n",
      "loss at batch 169408: 1.7232272624969482\n",
      "loss at batch 169472: 1.7712091207504272\n",
      "loss at batch 169536: 1.6917120218276978\n",
      "loss at batch 169600: 1.7507308721542358\n",
      "loss at batch 169664: 1.5694974660873413\n",
      "loss at batch 169728: 1.4527214765548706\n",
      "loss at batch 169792: 1.8750135898590088\n",
      "loss at batch 169856: 1.9752122163772583\n",
      "loss at batch 169920: 1.4831043481826782\n",
      "loss at batch 169984: 1.6095778942108154\n",
      "loss at batch 170048: 1.6149928569793701\n",
      "loss at batch 170112: 1.432262897491455\n",
      "loss at batch 170176: 1.5999480485916138\n",
      "loss at batch 170240: 1.7527530193328857\n",
      "loss at batch 170304: 1.7548682689666748\n",
      "loss at batch 170368: 2.0670151710510254\n",
      "loss at batch 170432: 1.5163710117340088\n",
      "loss at batch 170496: 1.809623122215271\n",
      "loss at batch 170560: 1.5292996168136597\n",
      "loss at batch 170624: 1.6760414838790894\n",
      "loss at batch 170688: 1.9229731559753418\n",
      "loss at batch 170752: 1.8290067911148071\n",
      "loss at batch 170816: 1.6928201913833618\n",
      "loss at batch 170880: 1.3640213012695312\n",
      "loss at batch 170944: 1.708108901977539\n",
      "loss at batch 171008: 1.553121566772461\n",
      "loss at batch 171072: 1.6820818185806274\n",
      "loss at batch 171136: 1.8198868036270142\n",
      "loss at batch 171200: 1.7150832414627075\n",
      "loss at batch 171264: 1.525521159172058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 171328: 2.2494688034057617\n",
      "loss at batch 171392: 2.1444430351257324\n",
      "loss at batch 171456: 1.688788890838623\n",
      "loss at batch 171520: 1.535819411277771\n",
      "loss at batch 171584: 1.8298050165176392\n",
      "loss at batch 171648: 1.4294745922088623\n",
      "loss at batch 171712: 1.6886062622070312\n",
      "loss at batch 171776: 1.9141879081726074\n",
      "loss at batch 171840: 1.64595365524292\n",
      "loss at batch 171904: 1.5395158529281616\n",
      "loss at batch 171968: 1.827136516571045\n",
      "loss at batch 172032: 1.3391330242156982\n",
      "loss at batch 172096: 1.8637131452560425\n",
      "loss at batch 172160: 1.6028982400894165\n",
      "loss at batch 172224: 1.497110366821289\n",
      "loss at batch 172288: 1.5726869106292725\n",
      "loss at batch 172352: 1.5347509384155273\n",
      "loss at batch 172416: 1.7072923183441162\n",
      "loss at batch 172480: 1.4709020853042603\n",
      "loss at batch 172544: 1.6145151853561401\n",
      "loss at batch 172608: 2.2525761127471924\n",
      "loss at batch 172672: 1.5799810886383057\n",
      "loss at batch 172736: 1.9884024858474731\n",
      "loss at batch 172800: 1.9335769414901733\n",
      "loss at batch 172864: 2.866460084915161\n",
      "loss at batch 172928: 2.0700953006744385\n",
      "loss at batch 172992: 1.3431965112686157\n",
      "loss at batch 173056: 1.603922724723816\n",
      "loss at batch 173120: 1.7868297100067139\n",
      "loss at batch 173184: 1.4901756048202515\n",
      "loss at batch 173248: 1.517859697341919\n",
      "loss at batch 173312: 1.7492649555206299\n",
      "loss at batch 173376: 2.195324659347534\n",
      "loss at batch 173440: 1.6746975183486938\n",
      "loss at batch 173504: 1.657509446144104\n",
      "loss at batch 173568: 2.10723876953125\n",
      "loss at batch 173632: 1.6858971118927002\n",
      "loss at batch 173696: 1.760955810546875\n",
      "loss at batch 173760: 2.0331695079803467\n",
      "loss at batch 173824: 1.8918159008026123\n",
      "loss at batch 173888: 1.6325461864471436\n",
      "loss at batch 173952: 1.5045429468154907\n",
      "loss at batch 174016: 1.9716092348098755\n",
      "loss at batch 174080: 1.2857296466827393\n",
      "loss at batch 174144: 2.4652631282806396\n",
      "loss at batch 174208: 2.37396502494812\n",
      "loss at batch 174272: 1.529065728187561\n",
      "loss at batch 174336: 2.0982167720794678\n",
      "loss at batch 174400: 1.5037188529968262\n",
      "loss at batch 174464: 1.679849624633789\n",
      "loss at batch 174528: 1.4978301525115967\n",
      "loss at batch 174592: 1.4850047826766968\n",
      "loss at batch 174656: 1.868578553199768\n",
      "loss at batch 174720: 1.646461009979248\n",
      "loss at batch 174784: 1.3284677267074585\n",
      "loss at batch 174848: 1.527286171913147\n",
      "loss at batch 174912: 1.9620851278305054\n",
      "loss at batch 174976: 1.6073778867721558\n",
      "loss at batch 175040: 1.5202593803405762\n",
      "loss at batch 175104: 2.0425961017608643\n",
      "loss at batch 175168: 1.4285372495651245\n",
      "loss at batch 175232: 1.5066406726837158\n",
      "loss at batch 175296: 1.7264161109924316\n",
      "loss at batch 175360: 1.5818356275558472\n",
      "loss at batch 175424: 1.612646460533142\n",
      "loss at batch 175488: 2.754896879196167\n",
      "loss at batch 175552: 1.6145291328430176\n",
      "loss at batch 175616: 1.4771225452423096\n",
      "loss at batch 175680: 1.3264259099960327\n",
      "loss at batch 175744: 1.4091930389404297\n",
      "loss at batch 175808: 2.2599544525146484\n",
      "loss at batch 175872: 2.7302820682525635\n",
      "loss at batch 175936: 1.6475720405578613\n",
      "loss at batch 176000: 1.4850023984909058\n",
      "loss at batch 176064: 1.6226701736450195\n",
      "loss at batch 176128: 2.478898286819458\n",
      "loss at batch 176192: 1.715421438217163\n",
      "loss at batch 176256: 1.6171069145202637\n",
      "loss at batch 176320: 1.4814997911453247\n",
      "loss at batch 176384: 2.0886168479919434\n",
      "loss at batch 176448: 1.6966201066970825\n",
      "loss at batch 176512: 1.7749841213226318\n",
      "loss at batch 176576: 1.7877470254898071\n",
      "loss at batch 176640: 1.764628291130066\n",
      "loss at batch 176704: 1.721579670906067\n",
      "loss at batch 176768: 1.5724769830703735\n",
      "loss at batch 176832: 1.4901704788208008\n",
      "loss at batch 176896: 1.5600460767745972\n",
      "loss at batch 176960: 1.7274450063705444\n",
      "loss at batch 177024: 1.5100940465927124\n",
      "loss at batch 177088: 1.6466859579086304\n",
      "loss at batch 177152: 1.3041939735412598\n",
      "loss at batch 177216: 1.7021448612213135\n",
      "loss at batch 177280: 1.4259257316589355\n",
      "loss at batch 177344: 1.5193449258804321\n",
      "loss at batch 177408: 2.033371686935425\n",
      "loss at batch 177472: 1.5702747106552124\n",
      "loss at batch 177536: 1.403017520904541\n",
      "loss at batch 177600: 1.7036840915679932\n",
      "loss at batch 177664: 1.69334876537323\n",
      "loss at batch 177728: 1.8671232461929321\n",
      "loss at batch 177792: 2.468900203704834\n",
      "loss at batch 177856: 1.7402403354644775\n",
      "loss at batch 177920: 2.035454273223877\n",
      "loss at batch 177984: 1.6667221784591675\n",
      "loss at batch 178048: 1.4553472995758057\n",
      "loss at batch 178112: 1.62150239944458\n",
      "loss at batch 178176: 1.9398137331008911\n",
      "loss at batch 178240: 1.6694732904434204\n",
      "loss at batch 178304: 1.5586354732513428\n",
      "loss at batch 178368: 1.4494932889938354\n",
      "loss at batch 178432: 1.6565035581588745\n",
      "loss at batch 178496: 1.664581298828125\n",
      "loss at batch 178560: 1.5982224941253662\n",
      "loss at batch 178624: 1.721453070640564\n",
      "loss at batch 178688: 1.5994595289230347\n",
      "loss at batch 178752: 1.5168797969818115\n",
      "loss at batch 178816: 1.8247184753417969\n",
      "loss at batch 178880: 3.076572895050049\n",
      "loss at batch 178944: 1.8164794445037842\n",
      "loss at batch 179008: 1.846400499343872\n",
      "loss at batch 179072: 1.9394971132278442\n",
      "loss at batch 179136: 1.5918902158737183\n",
      "loss at batch 179200: 1.5753642320632935\n",
      "loss at batch 179264: 1.857425332069397\n",
      "loss at batch 179328: 2.275388479232788\n",
      "loss at batch 179392: 1.7275502681732178\n",
      "loss at batch 179456: 1.5504026412963867\n",
      "loss at batch 179520: 1.851186990737915\n",
      "loss at batch 179584: 1.9741318225860596\n",
      "loss at batch 179648: 1.752890706062317\n",
      "loss at batch 179712: 2.2668399810791016\n",
      "loss at batch 179776: 1.8434243202209473\n",
      "loss at batch 179840: 1.473198652267456\n",
      "loss at batch 179904: 1.532488465309143\n",
      "loss at batch 179968: 2.137970447540283\n",
      "loss at batch 180032: 1.893046259880066\n",
      "loss at batch 180096: 2.4045395851135254\n",
      "loss at batch 180160: 1.526376724243164\n",
      "loss at batch 180224: 1.6399989128112793\n",
      "loss at batch 180288: 1.8016916513442993\n",
      "loss at batch 180352: 1.312496542930603\n",
      "loss at batch 180416: 1.54009211063385\n",
      "loss at batch 180480: 2.054091215133667\n",
      "loss at batch 180544: 1.5722196102142334\n",
      "loss at batch 180608: 1.4107317924499512\n",
      "loss at batch 180672: 1.6954866647720337\n",
      "loss at batch 180736: 1.6359308958053589\n",
      "loss at batch 180800: 1.8930068016052246\n",
      "loss at batch 180864: 1.7694929838180542\n",
      "loss at batch 180928: 1.6132229566574097\n",
      "loss at batch 180992: 1.3246855735778809\n",
      "loss at batch 181056: 1.669377088546753\n",
      "loss at batch 181120: 1.6077443361282349\n",
      "loss at batch 181184: 1.7237138748168945\n",
      "loss at batch 181248: 1.5373936891555786\n",
      "loss at batch 181312: 1.6325446367263794\n",
      "loss at batch 181376: 1.433605432510376\n",
      "loss at batch 181440: 1.2783416509628296\n",
      "loss at batch 181504: 1.654447317123413\n",
      "loss at batch 181568: 1.7357932329177856\n",
      "loss at batch 181632: 2.0521368980407715\n",
      "loss at batch 181696: 1.6910227537155151\n",
      "loss at batch 181760: 1.7029848098754883\n",
      "loss at batch 181824: 1.4096686840057373\n",
      "loss at batch 181888: 1.5793901681900024\n",
      "loss at batch 181952: 1.4075062274932861\n",
      "loss at batch 182016: 1.6260422468185425\n",
      "loss at batch 182080: 1.4115575551986694\n",
      "loss at batch 182144: 1.170849323272705\n",
      "loss at batch 182208: 1.2802073955535889\n",
      "loss at batch 182272: 1.616440773010254\n",
      "loss at batch 182336: 1.8593119382858276\n",
      "loss at batch 182400: 1.465812087059021\n",
      "loss at batch 182464: 1.4899977445602417\n",
      "loss at batch 182528: 1.5203925371170044\n",
      "loss at batch 182592: 1.5927224159240723\n",
      "loss at batch 182656: 1.4237797260284424\n",
      "loss at batch 182720: 1.6937541961669922\n",
      "loss at batch 182784: 1.5741281509399414\n",
      "loss at batch 182848: 1.4465477466583252\n",
      "loss at batch 182912: 2.075697422027588\n",
      "loss at batch 182976: 1.6488419771194458\n",
      "loss at batch 183040: 1.5966978073120117\n",
      "loss at batch 183104: 1.6742064952850342\n",
      "loss at batch 183168: 2.064664363861084\n",
      "loss at batch 183232: 1.4537838697433472\n",
      "loss at batch 183296: 2.0884149074554443\n",
      "loss at batch 183360: 1.4921056032180786\n",
      "loss at batch 183424: 1.4553682804107666\n",
      "loss at batch 183488: 2.0121617317199707\n",
      "loss at batch 183552: 1.4146735668182373\n",
      "loss at batch 183616: 1.4937046766281128\n",
      "loss at batch 183680: 1.6101360321044922\n",
      "loss at batch 183744: 1.7818386554718018\n",
      "loss at batch 183808: 2.223174810409546\n",
      "loss at batch 183872: 2.281890869140625\n",
      "loss at batch 183936: 1.5527065992355347\n",
      "loss at batch 184000: 1.6740891933441162\n",
      "loss at batch 184064: 1.8131489753723145\n",
      "loss at batch 184128: 1.7576537132263184\n",
      "loss at batch 184192: 1.7212212085723877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 184256: 1.6337573528289795\n",
      "loss at batch 184320: 1.3104822635650635\n",
      "loss at batch 184384: 1.3708858489990234\n",
      "loss at batch 184448: 1.5700057744979858\n",
      "loss at batch 184512: 1.5503966808319092\n",
      "loss at batch 184576: 1.646257996559143\n",
      "loss at batch 184640: 1.6448359489440918\n",
      "loss at batch 184704: 1.593392014503479\n",
      "loss at batch 184768: 1.3351023197174072\n",
      "loss at batch 184832: 2.191152334213257\n",
      "loss at batch 184896: 1.4691681861877441\n",
      "loss at batch 184960: 2.429070234298706\n",
      "loss at batch 185024: 1.4211026430130005\n",
      "loss at batch 185088: 1.9899530410766602\n",
      "loss at batch 185152: 1.8489041328430176\n",
      "loss at batch 185216: 1.5203841924667358\n",
      "loss at batch 185280: 1.7534191608428955\n",
      "loss at batch 185344: 1.4878567457199097\n",
      "loss at batch 185408: 1.450958490371704\n",
      "loss at batch 185472: 1.2689753770828247\n",
      "loss at batch 185536: 2.232100248336792\n",
      "loss at batch 185600: 1.4098191261291504\n",
      "loss at batch 185664: 2.07075834274292\n",
      "loss at batch 185728: 1.4546904563903809\n",
      "loss at batch 185792: 2.3204755783081055\n",
      "loss at batch 185856: 1.5018620491027832\n",
      "loss at batch 185920: 1.784782886505127\n",
      "loss at batch 185984: 1.6022576093673706\n",
      "loss at batch 186048: 1.780158281326294\n",
      "loss at batch 186112: 1.4696885347366333\n",
      "loss at batch 186176: 2.165314197540283\n",
      "loss at end of epoch 0: 2.512410998503827\n",
      "loss at batch 64: 1.3080097436904907\n",
      "loss at batch 128: 1.5248215198516846\n",
      "loss at batch 192: 2.033142566680908\n",
      "loss at batch 256: 1.6154414415359497\n",
      "loss at batch 320: 1.48476243019104\n",
      "loss at batch 384: 1.5989078283309937\n",
      "loss at batch 448: 1.6399319171905518\n",
      "loss at batch 512: 1.658286690711975\n",
      "loss at batch 576: 1.8128855228424072\n",
      "loss at batch 640: 1.3998134136199951\n",
      "loss at batch 704: 1.4167945384979248\n",
      "loss at batch 768: 1.4078174829483032\n",
      "loss at batch 832: 1.6790350675582886\n",
      "loss at batch 896: 1.2554513216018677\n",
      "loss at batch 960: 1.3411755561828613\n",
      "loss at batch 1024: 1.7116267681121826\n",
      "loss at batch 1088: 1.789031982421875\n",
      "loss at batch 1152: 1.654035210609436\n",
      "loss at batch 1216: 1.4708151817321777\n",
      "loss at batch 1280: 2.3998401165008545\n",
      "loss at batch 1344: 1.4438495635986328\n",
      "loss at batch 1408: 1.6769859790802002\n",
      "loss at batch 1472: 1.5148441791534424\n",
      "loss at batch 1536: 1.9541443586349487\n",
      "loss at batch 1600: 2.0733232498168945\n",
      "loss at batch 1664: 1.779024600982666\n",
      "loss at batch 1728: 1.3406693935394287\n",
      "loss at batch 1792: 1.86915123462677\n",
      "loss at batch 1856: 1.8608615398406982\n",
      "loss at batch 1920: 1.5525074005126953\n",
      "loss at batch 1984: 2.0592381954193115\n",
      "loss at batch 2048: 1.7070568799972534\n",
      "loss at batch 2112: 1.4937129020690918\n",
      "loss at batch 2176: 2.1042563915252686\n",
      "loss at batch 2240: 1.7292640209197998\n",
      "loss at batch 2304: 1.5593726634979248\n",
      "loss at batch 2368: 1.2293953895568848\n",
      "loss at batch 2432: 1.3973991870880127\n",
      "loss at batch 2496: 1.5838654041290283\n",
      "loss at batch 2560: 2.0178091526031494\n",
      "loss at batch 2624: 1.6484733819961548\n",
      "loss at batch 2688: 1.480423927307129\n",
      "loss at batch 2752: 1.5494897365570068\n",
      "loss at batch 2816: 1.4708096981048584\n",
      "loss at batch 2880: 2.2872889041900635\n",
      "loss at batch 2944: 1.5397878885269165\n",
      "loss at batch 3008: 1.487657070159912\n",
      "loss at batch 3072: 1.9746202230453491\n",
      "loss at batch 3136: 1.2947924137115479\n",
      "loss at batch 3200: 1.9084861278533936\n",
      "loss at batch 3264: 1.3706921339035034\n",
      "loss at batch 3328: 1.2703678607940674\n",
      "loss at batch 3392: 1.378648281097412\n",
      "loss at batch 3456: 1.2196581363677979\n",
      "loss at batch 3520: 1.6583505868911743\n",
      "loss at batch 3584: 1.1735804080963135\n",
      "loss at batch 3648: 1.4520174264907837\n",
      "loss at batch 3712: 1.4160081148147583\n",
      "loss at batch 3776: 1.5658950805664062\n",
      "loss at batch 3840: 1.554521083831787\n",
      "loss at batch 3904: 1.6498337984085083\n",
      "loss at batch 3968: 1.654444932937622\n",
      "loss at batch 4032: 1.7658132314682007\n",
      "loss at batch 4096: 1.5630295276641846\n",
      "loss at batch 4160: 2.23531174659729\n",
      "loss at batch 4224: 1.477224349975586\n",
      "loss at batch 4288: 1.651165246963501\n",
      "loss at batch 4352: 1.6522797346115112\n",
      "loss at batch 4416: 1.6352663040161133\n",
      "loss at batch 4480: 2.0117149353027344\n",
      "loss at batch 4544: 1.7341843843460083\n",
      "loss at batch 4608: 1.6863354444503784\n",
      "loss at batch 4672: 1.6748541593551636\n",
      "loss at batch 4736: 1.437637209892273\n",
      "loss at batch 4800: 1.2585258483886719\n",
      "loss at batch 4864: 1.4100370407104492\n",
      "loss at batch 4928: 1.7283488512039185\n",
      "loss at batch 4992: 1.402642011642456\n",
      "loss at batch 5056: 1.6113463640213013\n",
      "loss at batch 5120: 1.965654730796814\n",
      "loss at batch 5184: 1.3769092559814453\n",
      "loss at batch 5248: 1.3290311098098755\n",
      "loss at batch 5312: 1.781909465789795\n",
      "loss at batch 5376: 1.4124064445495605\n",
      "loss at batch 5440: 2.0490517616271973\n",
      "loss at batch 5504: 1.5822676420211792\n",
      "loss at batch 5568: 2.0133235454559326\n",
      "loss at batch 5632: 1.40580153465271\n",
      "loss at batch 5696: 1.4357855319976807\n",
      "loss at batch 5760: 1.4936209917068481\n",
      "loss at batch 5824: 1.4612685441970825\n",
      "loss at batch 5888: 1.7038109302520752\n",
      "loss at batch 5952: 2.304133892059326\n",
      "loss at batch 6016: 1.3765406608581543\n",
      "loss at batch 6080: 1.4019896984100342\n",
      "loss at batch 6144: 1.920546054840088\n",
      "loss at batch 6208: 1.2685940265655518\n",
      "loss at batch 6272: 1.6411250829696655\n",
      "loss at batch 6336: 2.1423470973968506\n",
      "loss at batch 6400: 1.9858462810516357\n",
      "loss at batch 6464: 1.4166312217712402\n",
      "loss at batch 6528: 1.4498592615127563\n",
      "loss at batch 6592: 1.773255467414856\n",
      "loss at batch 6656: 2.6495039463043213\n",
      "loss at batch 6720: 1.5856528282165527\n",
      "loss at batch 6784: 1.4716877937316895\n",
      "loss at batch 6848: 1.4510242938995361\n",
      "loss at batch 6912: 1.9338876008987427\n",
      "loss at batch 6976: 2.7678465843200684\n",
      "loss at batch 7040: 1.7101970911026\n",
      "loss at batch 7104: 1.646108627319336\n",
      "loss at batch 7168: 1.639020562171936\n",
      "loss at batch 7232: 1.3099923133850098\n",
      "loss at batch 7296: 1.4806005954742432\n",
      "loss at batch 7360: 1.4773913621902466\n",
      "loss at batch 7424: 1.5413202047348022\n",
      "loss at batch 7488: 1.5505235195159912\n",
      "loss at batch 7552: 1.957274317741394\n",
      "loss at batch 7616: 1.7233625650405884\n",
      "loss at batch 7680: 1.4236329793930054\n",
      "loss at batch 7744: 1.6837433576583862\n",
      "loss at batch 7808: 1.1987112760543823\n",
      "loss at batch 7872: 1.7420765161514282\n",
      "loss at batch 7936: 2.116793155670166\n",
      "loss at batch 8000: 1.1886069774627686\n",
      "loss at batch 8064: 1.4804229736328125\n",
      "loss at batch 8128: 1.5253658294677734\n",
      "loss at batch 8192: 1.3354014158248901\n",
      "loss at batch 8256: 1.5237305164337158\n",
      "loss at batch 8320: 2.0354371070861816\n",
      "loss at batch 8384: 1.471110463142395\n",
      "loss at batch 8448: 1.5276119709014893\n",
      "loss at batch 8512: 1.5590543746948242\n",
      "loss at batch 8576: 1.7094545364379883\n",
      "loss at batch 8640: 1.7757056951522827\n",
      "loss at batch 8704: 1.388295292854309\n",
      "loss at batch 8768: 1.2283315658569336\n",
      "loss at batch 8832: 1.3688775300979614\n",
      "loss at batch 8896: 1.4731292724609375\n",
      "loss at batch 8960: 1.3458420038223267\n",
      "loss at batch 9024: 1.3652023077011108\n",
      "loss at batch 9088: 1.4790595769882202\n",
      "loss at batch 9152: 1.4734156131744385\n",
      "loss at batch 9216: 1.4452275037765503\n",
      "loss at batch 9280: 1.4041866064071655\n",
      "loss at batch 9344: 1.852281093597412\n",
      "loss at batch 9408: 1.5575834512710571\n",
      "loss at batch 9472: 1.8629037141799927\n",
      "loss at batch 9536: 1.4873640537261963\n",
      "loss at batch 9600: 1.2405610084533691\n",
      "loss at batch 9664: 2.0007243156433105\n",
      "loss at batch 9728: 1.4735643863677979\n",
      "loss at batch 9792: 1.7644286155700684\n",
      "loss at batch 9856: 2.6099185943603516\n",
      "loss at batch 9920: 1.6739323139190674\n",
      "loss at batch 9984: 1.5497411489486694\n",
      "loss at batch 10048: 1.6049822568893433\n",
      "loss at batch 10112: 1.4590338468551636\n",
      "loss at batch 10176: 1.7080767154693604\n",
      "loss at batch 10240: 1.5291752815246582\n",
      "loss at batch 10304: 1.3900246620178223\n",
      "loss at batch 10368: 1.46145498752594\n",
      "loss at batch 10432: 1.3408833742141724\n",
      "loss at batch 10496: 1.5395739078521729\n",
      "loss at batch 10560: 1.3887375593185425\n",
      "loss at batch 10624: 1.7383126020431519\n",
      "loss at batch 10688: 1.5870959758758545\n",
      "loss at batch 10752: 1.68118417263031\n",
      "loss at batch 10816: 1.4071767330169678\n",
      "loss at batch 10880: 1.55884850025177\n",
      "loss at batch 10944: 1.6575995683670044\n",
      "loss at batch 11008: 1.4005807638168335\n",
      "loss at batch 11072: 1.1449273824691772\n",
      "loss at batch 11136: 1.4774320125579834\n",
      "loss at batch 11200: 1.420501708984375\n",
      "loss at batch 11264: 1.3075426816940308\n",
      "loss at batch 11328: 1.70643949508667\n",
      "loss at batch 11392: 1.3852503299713135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 11456: 2.6859188079833984\n",
      "loss at batch 11520: 1.4592647552490234\n",
      "loss at batch 11584: 1.6462191343307495\n",
      "loss at batch 11648: 1.4173026084899902\n",
      "loss at batch 11712: 1.6707391738891602\n",
      "loss at batch 11776: 2.1782760620117188\n",
      "loss at batch 11840: 1.6148937940597534\n",
      "loss at batch 11904: 1.6781636476516724\n",
      "loss at batch 11968: 1.910778522491455\n",
      "loss at batch 12032: 1.3070650100708008\n",
      "loss at batch 12096: 2.03185772895813\n",
      "loss at batch 12160: 2.0678298473358154\n",
      "loss at batch 12224: 1.408552885055542\n",
      "loss at batch 12288: 1.2818409204483032\n",
      "loss at batch 12352: 1.3263415098190308\n",
      "loss at batch 12416: 1.4318764209747314\n",
      "loss at batch 12480: 1.425008773803711\n",
      "loss at batch 12544: 1.6922088861465454\n",
      "loss at batch 12608: 2.13870906829834\n",
      "loss at batch 12672: 1.4728742837905884\n",
      "loss at batch 12736: 1.2752351760864258\n",
      "loss at batch 12800: 1.548811674118042\n",
      "loss at batch 12864: 1.4488898515701294\n",
      "loss at batch 12928: 2.2755630016326904\n",
      "loss at batch 12992: 1.5152071714401245\n",
      "loss at batch 13056: 1.5515084266662598\n",
      "loss at batch 13120: 1.5490667819976807\n",
      "loss at batch 13184: 2.1849210262298584\n",
      "loss at batch 13248: 1.3083783388137817\n",
      "loss at batch 13312: 1.6498271226882935\n",
      "loss at batch 13376: 1.4110618829727173\n",
      "loss at batch 13440: 1.703755259513855\n",
      "loss at batch 13504: 1.8305846452713013\n",
      "loss at batch 13568: 2.4981024265289307\n",
      "loss at batch 13632: 1.7188844680786133\n",
      "loss at batch 13696: 1.6570799350738525\n",
      "loss at batch 13760: 1.493582010269165\n",
      "loss at batch 13824: 1.192942500114441\n",
      "loss at batch 13888: 1.730523705482483\n",
      "loss at batch 13952: 1.6820906400680542\n",
      "loss at batch 14016: 1.857810139656067\n",
      "loss at batch 14080: 1.6173335313796997\n",
      "loss at batch 14144: 1.4811269044876099\n",
      "loss at batch 14208: 1.5674726963043213\n",
      "loss at batch 14272: 1.4333075284957886\n",
      "loss at batch 14336: 1.712967038154602\n",
      "loss at batch 14400: 1.5243809223175049\n",
      "loss at batch 14464: 2.1896090507507324\n",
      "loss at batch 14528: 1.4917001724243164\n",
      "loss at batch 14592: 1.528002381324768\n",
      "loss at batch 14656: 1.3591904640197754\n",
      "loss at batch 14720: 1.8535231351852417\n",
      "loss at batch 14784: 1.6384323835372925\n",
      "loss at batch 14848: 2.2885594367980957\n",
      "loss at batch 14912: 1.6932958364486694\n",
      "loss at batch 14976: 1.489558458328247\n",
      "loss at batch 15040: 1.277422308921814\n",
      "loss at batch 15104: 1.496870994567871\n",
      "loss at batch 15168: 1.5998955965042114\n",
      "loss at batch 15232: 1.568188190460205\n",
      "loss at batch 15296: 1.263968825340271\n",
      "loss at batch 15360: 1.604056477546692\n",
      "loss at batch 15424: 1.636389136314392\n",
      "loss at batch 15488: 1.4220716953277588\n",
      "loss at batch 15552: 1.3255010843276978\n",
      "loss at batch 15616: 1.435689091682434\n",
      "loss at batch 15680: 1.4015177488327026\n",
      "loss at batch 15744: 1.9614875316619873\n",
      "loss at batch 15808: 1.5884981155395508\n",
      "loss at batch 15872: 1.5353777408599854\n",
      "loss at batch 15936: 1.6028058528900146\n",
      "loss at batch 16000: 1.3960083723068237\n",
      "loss at batch 16064: 1.273783564567566\n",
      "loss at batch 16128: 2.2596352100372314\n",
      "loss at batch 16192: 1.8362120389938354\n",
      "loss at batch 16256: 1.6917556524276733\n",
      "loss at batch 16320: 1.755773901939392\n",
      "loss at batch 16384: 1.4430198669433594\n",
      "loss at batch 16448: 1.3536357879638672\n",
      "loss at batch 16512: 1.2759313583374023\n",
      "loss at batch 16576: 1.5860613584518433\n",
      "loss at batch 16640: 1.4058526754379272\n",
      "loss at batch 16704: 1.5025681257247925\n",
      "loss at batch 16768: 1.708282232284546\n",
      "loss at batch 16832: 1.3327723741531372\n",
      "loss at batch 16896: 1.315314531326294\n",
      "loss at batch 16960: 1.3824243545532227\n",
      "loss at batch 17024: 1.4257311820983887\n",
      "loss at batch 17088: 2.284644603729248\n",
      "loss at batch 17152: 1.398638367652893\n",
      "loss at batch 17216: 1.8350684642791748\n",
      "loss at batch 17280: 1.5840924978256226\n",
      "loss at batch 17344: 1.1994115114212036\n",
      "loss at batch 17408: 1.4315745830535889\n",
      "loss at batch 17472: 1.252497911453247\n",
      "loss at batch 17536: 1.682917833328247\n",
      "loss at batch 17600: 1.316421627998352\n",
      "loss at batch 17664: 1.4615728855133057\n",
      "loss at batch 17728: 1.4068251848220825\n",
      "loss at batch 17792: 1.4740980863571167\n",
      "loss at batch 17856: 1.5200518369674683\n",
      "loss at batch 17920: 2.3205442428588867\n",
      "loss at batch 17984: 1.5692600011825562\n",
      "loss at batch 18048: 1.2515827417373657\n",
      "loss at batch 18112: 1.8157854080200195\n",
      "loss at batch 18176: 2.1196765899658203\n",
      "loss at batch 18240: 1.3584423065185547\n",
      "loss at batch 18304: 1.271363615989685\n",
      "loss at batch 18368: 1.531373143196106\n",
      "loss at batch 18432: 1.381459355354309\n",
      "loss at batch 18496: 1.7227667570114136\n",
      "loss at batch 18560: 1.7410402297973633\n",
      "loss at batch 18624: 1.2253191471099854\n",
      "loss at batch 18688: 1.4049171209335327\n",
      "loss at batch 18752: 1.4473209381103516\n",
      "loss at batch 18816: 1.38673734664917\n",
      "loss at batch 18880: 1.9783329963684082\n",
      "loss at batch 18944: 1.2929612398147583\n",
      "loss at batch 19008: 1.5184611082077026\n",
      "loss at batch 19072: 1.3566956520080566\n",
      "loss at batch 19136: 2.084657669067383\n",
      "loss at batch 19200: 1.5542625188827515\n",
      "loss at batch 19264: 1.5264606475830078\n",
      "loss at batch 19328: 1.411942958831787\n",
      "loss at batch 19392: 1.7688528299331665\n",
      "loss at batch 19456: 2.1363258361816406\n",
      "loss at batch 19520: 1.2590773105621338\n",
      "loss at batch 19584: 1.4858187437057495\n",
      "loss at batch 19648: 1.9281408786773682\n",
      "loss at batch 19712: 1.5985150337219238\n",
      "loss at batch 19776: 1.0324937105178833\n",
      "loss at batch 19840: 2.0006463527679443\n",
      "loss at batch 19904: 1.4879662990570068\n",
      "loss at batch 19968: 1.449103593826294\n",
      "loss at batch 20032: 1.0982651710510254\n",
      "loss at batch 20096: 1.5003706216812134\n",
      "loss at batch 20160: 1.59050452709198\n",
      "loss at batch 20224: 1.4576513767242432\n",
      "loss at batch 20288: 1.679088830947876\n",
      "loss at batch 20352: 1.587182879447937\n",
      "loss at batch 20416: 1.3321623802185059\n",
      "loss at batch 20480: 1.5006039142608643\n",
      "loss at batch 20544: 1.7421669960021973\n",
      "loss at batch 20608: 1.360794186592102\n",
      "loss at batch 20672: 1.5840389728546143\n",
      "loss at batch 20736: 1.4979857206344604\n",
      "loss at batch 20800: 1.6274113655090332\n",
      "loss at batch 20864: 1.2442442178726196\n",
      "loss at batch 20928: 1.9387694597244263\n",
      "loss at batch 20992: 1.6135793924331665\n",
      "loss at batch 21056: 1.9644020795822144\n",
      "loss at batch 21120: 1.550121784210205\n",
      "loss at batch 21184: 1.798960566520691\n",
      "loss at batch 21248: 1.8531429767608643\n",
      "loss at batch 21312: 1.0862305164337158\n",
      "loss at batch 21376: 1.568868637084961\n",
      "loss at batch 21440: 1.273622751235962\n",
      "loss at batch 21504: 1.7544972896575928\n",
      "loss at batch 21568: 1.4600560665130615\n",
      "loss at batch 21632: 1.1921993494033813\n",
      "loss at batch 21696: 1.4360275268554688\n",
      "loss at batch 21760: 1.4595741033554077\n",
      "loss at batch 21824: 1.2186764478683472\n",
      "loss at batch 21888: 1.4967385530471802\n",
      "loss at batch 21952: 1.7361782789230347\n",
      "loss at batch 22016: 1.6434717178344727\n",
      "loss at batch 22080: 1.417582392692566\n",
      "loss at batch 22144: 1.3011902570724487\n",
      "loss at batch 22208: 1.4959862232208252\n",
      "loss at batch 22272: 1.6902904510498047\n",
      "loss at batch 22336: 1.5188859701156616\n",
      "loss at batch 22400: 1.7599711418151855\n",
      "loss at batch 22464: 1.623672366142273\n",
      "loss at batch 22528: 1.1127591133117676\n",
      "loss at batch 22592: 1.3082144260406494\n",
      "loss at batch 22656: 1.185823917388916\n",
      "loss at batch 22720: 1.926276683807373\n",
      "loss at batch 22784: 1.3260246515274048\n",
      "loss at batch 22848: 1.4870328903198242\n",
      "loss at batch 22912: 1.5255634784698486\n",
      "loss at batch 22976: 1.4727188348770142\n",
      "loss at batch 23040: 1.7391561269760132\n",
      "loss at batch 23104: 1.6994560956954956\n",
      "loss at batch 23168: 1.386460304260254\n",
      "loss at batch 23232: 1.59824538230896\n",
      "loss at batch 23296: 1.4263465404510498\n",
      "loss at batch 23360: 1.4197280406951904\n",
      "loss at batch 23424: 1.5407534837722778\n",
      "loss at batch 23488: 1.2556016445159912\n",
      "loss at batch 23552: 1.3264715671539307\n",
      "loss at batch 23616: 1.5466177463531494\n",
      "loss at batch 23680: 1.273748517036438\n",
      "loss at batch 23744: 2.0541954040527344\n",
      "loss at batch 23808: 1.541619896888733\n",
      "loss at batch 23872: 1.3629637956619263\n",
      "loss at batch 23936: 1.46682870388031\n",
      "loss at batch 24000: 1.624320387840271\n",
      "loss at batch 24064: 1.0846073627471924\n",
      "loss at batch 24128: 1.4953577518463135\n",
      "loss at batch 24192: 1.603538990020752\n",
      "loss at batch 24256: 1.5290591716766357\n",
      "loss at batch 24320: 1.4569412469863892\n",
      "loss at batch 24384: 1.225905179977417\n",
      "loss at batch 24448: 1.696840763092041\n",
      "loss at batch 24512: 2.0409770011901855\n",
      "loss at batch 24576: 1.4885478019714355\n",
      "loss at batch 24640: 1.525999665260315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 24704: 1.3474465608596802\n",
      "loss at batch 24768: 1.7657803297042847\n",
      "loss at batch 24832: 1.4091780185699463\n",
      "loss at batch 24896: 1.3706903457641602\n",
      "loss at batch 24960: 1.499097466468811\n",
      "loss at batch 25024: 1.295345664024353\n",
      "loss at batch 25088: 1.6962002515792847\n",
      "loss at batch 25152: 1.3895957469940186\n",
      "loss at batch 25216: 1.2811427116394043\n",
      "loss at batch 25280: 1.246339201927185\n",
      "loss at batch 25344: 1.293062448501587\n",
      "loss at batch 25408: 1.5050016641616821\n",
      "loss at batch 25472: 1.4188240766525269\n",
      "loss at batch 25536: 1.4606834650039673\n",
      "loss at batch 25600: 1.4388186931610107\n",
      "loss at batch 25664: 1.3177781105041504\n",
      "loss at batch 25728: 1.785177230834961\n",
      "loss at batch 25792: 1.5129873752593994\n",
      "loss at batch 25856: 1.3706518411636353\n",
      "loss at batch 25920: 2.4910531044006348\n",
      "loss at batch 25984: 1.231197476387024\n",
      "loss at batch 26048: 1.6261509656906128\n",
      "loss at batch 26112: 1.51939058303833\n",
      "loss at batch 26176: 1.4553358554840088\n",
      "loss at batch 26240: 1.5003184080123901\n",
      "loss at batch 26304: 1.3812013864517212\n",
      "loss at batch 26368: 1.8112846612930298\n",
      "loss at batch 26432: 1.3763508796691895\n",
      "loss at batch 26496: 1.459349274635315\n",
      "loss at batch 26560: 1.8241076469421387\n",
      "loss at batch 26624: 1.2596895694732666\n",
      "loss at batch 26688: 1.4231350421905518\n",
      "loss at batch 26752: 1.6413204669952393\n",
      "loss at batch 26816: 1.4079794883728027\n",
      "loss at batch 26880: 2.170391321182251\n",
      "loss at batch 26944: 1.5071170330047607\n",
      "loss at batch 27008: 1.406118392944336\n",
      "loss at batch 27072: 1.5979290008544922\n",
      "loss at batch 27136: 1.0658577680587769\n",
      "loss at batch 27200: 1.6817762851715088\n",
      "loss at batch 27264: 1.112663745880127\n",
      "loss at batch 27328: 1.5106252431869507\n",
      "loss at batch 27392: 1.4541468620300293\n",
      "loss at batch 27456: 1.6261228322982788\n",
      "loss at batch 27520: 1.6761049032211304\n",
      "loss at batch 27584: 1.4073903560638428\n",
      "loss at batch 27648: 1.2702584266662598\n",
      "loss at batch 27712: 1.878324270248413\n",
      "loss at batch 27776: 1.2791852951049805\n",
      "loss at batch 27840: 1.7868518829345703\n",
      "loss at batch 27904: 1.5705044269561768\n",
      "loss at batch 27968: 1.5840895175933838\n",
      "loss at batch 28032: 1.9814071655273438\n",
      "loss at batch 28096: 1.169887661933899\n",
      "loss at batch 28160: 2.0349700450897217\n",
      "loss at batch 28224: 1.3609973192214966\n",
      "loss at batch 28288: 1.153744101524353\n",
      "loss at batch 28352: 1.7348964214324951\n",
      "loss at batch 28416: 1.5421903133392334\n",
      "loss at batch 28480: 1.8464938402175903\n",
      "loss at batch 28544: 1.3731728792190552\n",
      "loss at batch 28608: 1.2365508079528809\n",
      "loss at batch 28672: 1.488328218460083\n",
      "loss at batch 28736: 1.4743006229400635\n",
      "loss at batch 28800: 1.3000503778457642\n",
      "loss at batch 28864: 1.500120759010315\n",
      "loss at batch 28928: 1.4455928802490234\n",
      "loss at batch 28992: 1.5206964015960693\n",
      "loss at batch 29056: 1.0969586372375488\n",
      "loss at batch 29120: 1.3376410007476807\n",
      "loss at batch 29184: 1.2344077825546265\n",
      "loss at batch 29248: 1.571666955947876\n",
      "loss at batch 29312: 1.344504952430725\n",
      "loss at batch 29376: 1.6633254289627075\n",
      "loss at batch 29440: 1.6742148399353027\n",
      "loss at batch 29504: 1.7395986318588257\n",
      "loss at batch 29568: 1.5880017280578613\n",
      "loss at batch 29632: 1.1417667865753174\n",
      "loss at batch 29696: 1.2259552478790283\n",
      "loss at batch 29760: 1.3525813817977905\n",
      "loss at batch 29824: 1.355238437652588\n",
      "loss at batch 29888: 1.5980759859085083\n",
      "loss at batch 29952: 1.2983416318893433\n",
      "loss at batch 30016: 1.353580355644226\n",
      "loss at batch 30080: 1.3444061279296875\n",
      "loss at batch 30144: 1.859740972518921\n",
      "loss at batch 30208: 1.923124074935913\n",
      "loss at batch 30272: 1.3157471418380737\n",
      "loss at batch 30336: 1.1688603162765503\n",
      "loss at batch 30400: 1.2375526428222656\n",
      "loss at batch 30464: 1.1783405542373657\n",
      "loss at batch 30528: 1.3800504207611084\n",
      "loss at batch 30592: 1.7549792528152466\n",
      "loss at batch 30656: 1.4023014307022095\n",
      "loss at batch 30720: 1.5531306266784668\n",
      "loss at batch 30784: 1.4112154245376587\n",
      "loss at batch 30848: 1.3206173181533813\n",
      "loss at batch 30912: 1.492673397064209\n",
      "loss at batch 30976: 2.0028109550476074\n",
      "loss at batch 31040: 1.4549050331115723\n",
      "loss at batch 31104: 1.5429491996765137\n",
      "loss at batch 31168: 1.2495015859603882\n",
      "loss at batch 31232: 1.2658758163452148\n",
      "loss at batch 31296: 1.4793224334716797\n",
      "loss at batch 31360: 1.392046332359314\n",
      "loss at batch 31424: 1.1879326105117798\n",
      "loss at batch 31488: 1.4450149536132812\n",
      "loss at batch 31552: 1.681283950805664\n",
      "loss at batch 31616: 2.960517406463623\n",
      "loss at batch 31680: 1.7669743299484253\n",
      "loss at batch 31744: 1.8580968379974365\n",
      "loss at batch 31808: 1.2835371494293213\n",
      "loss at batch 31872: 2.0628185272216797\n",
      "loss at batch 31936: 1.4124139547348022\n",
      "loss at batch 32000: 1.4377877712249756\n",
      "loss at batch 32064: 1.775042176246643\n",
      "loss at batch 32128: 1.3822760581970215\n",
      "loss at batch 32192: 1.5416600704193115\n",
      "loss at batch 32256: 1.5337849855422974\n",
      "loss at batch 32320: 1.3510938882827759\n",
      "loss at batch 32384: 1.3714802265167236\n",
      "loss at batch 32448: 1.2807906866073608\n",
      "loss at batch 32512: 1.6723982095718384\n",
      "loss at batch 32576: 1.4310991764068604\n",
      "loss at batch 32640: 1.161915898323059\n",
      "loss at batch 32704: 1.5870381593704224\n",
      "loss at batch 32768: 1.3334904909133911\n",
      "loss at batch 32832: 1.258169412612915\n",
      "loss at batch 32896: 1.1624902486801147\n",
      "loss at batch 32960: 1.4013471603393555\n",
      "loss at batch 33024: 1.4858295917510986\n",
      "loss at batch 33088: 1.160526990890503\n",
      "loss at batch 33152: 1.5833848714828491\n",
      "loss at batch 33216: 1.401655673980713\n",
      "loss at batch 33280: 1.4071595668792725\n",
      "loss at batch 33344: 2.0716512203216553\n",
      "loss at batch 33408: 2.753052234649658\n",
      "loss at batch 33472: 1.4048004150390625\n",
      "loss at batch 33536: 1.4426863193511963\n",
      "loss at batch 33600: 1.4891366958618164\n",
      "loss at batch 33664: 1.6493077278137207\n",
      "loss at batch 33728: 1.4966413974761963\n",
      "loss at batch 33792: 1.610180139541626\n",
      "loss at batch 33856: 1.3850256204605103\n",
      "loss at batch 33920: 1.634080410003662\n",
      "loss at batch 33984: 1.4708529710769653\n",
      "loss at batch 34048: 1.598771572113037\n",
      "loss at batch 34112: 1.434205412864685\n",
      "loss at batch 34176: 1.723029613494873\n",
      "loss at batch 34240: 1.408624529838562\n",
      "loss at batch 34304: 1.4276049137115479\n",
      "loss at batch 34368: 1.6197088956832886\n",
      "loss at batch 34432: 1.3908793926239014\n",
      "loss at batch 34496: 1.850017786026001\n",
      "loss at batch 34560: 1.4461383819580078\n",
      "loss at batch 34624: 1.4694652557373047\n",
      "loss at batch 34688: 1.2738953828811646\n",
      "loss at batch 34752: 1.2988988161087036\n",
      "loss at batch 34816: 1.4711977243423462\n",
      "loss at batch 34880: 1.6050217151641846\n",
      "loss at batch 34944: 1.2879949808120728\n",
      "loss at batch 35008: 1.4044214487075806\n",
      "loss at batch 35072: 1.5401194095611572\n",
      "loss at batch 35136: 1.9282991886138916\n",
      "loss at batch 35200: 1.5506503582000732\n",
      "loss at batch 35264: 1.3868755102157593\n",
      "loss at batch 35328: 1.417725682258606\n",
      "loss at batch 35392: 1.4138519763946533\n",
      "loss at batch 35456: 1.6003168821334839\n",
      "loss at batch 35520: 1.430658221244812\n",
      "loss at batch 35584: 2.7566750049591064\n",
      "loss at batch 35648: 1.6158243417739868\n",
      "loss at batch 35712: 1.4388753175735474\n",
      "loss at batch 35776: 1.5033026933670044\n",
      "loss at batch 35840: 1.3399080038070679\n",
      "loss at batch 35904: 1.6760026216506958\n",
      "loss at batch 35968: 1.7331900596618652\n",
      "loss at batch 36032: 1.3450554609298706\n",
      "loss at batch 36096: 1.7569029331207275\n",
      "loss at batch 36160: 1.4512752294540405\n",
      "loss at batch 36224: 1.786841630935669\n",
      "loss at batch 36288: 2.1161820888519287\n",
      "loss at batch 36352: 1.1737027168273926\n",
      "loss at batch 36416: 1.2984530925750732\n",
      "loss at batch 36480: 1.3952178955078125\n",
      "loss at batch 36544: 2.3193764686584473\n",
      "loss at batch 36608: 1.5921498537063599\n",
      "loss at batch 36672: 1.2759816646575928\n",
      "loss at batch 36736: 1.534162998199463\n",
      "loss at batch 36800: 1.3901355266571045\n",
      "loss at batch 36864: 1.3648781776428223\n",
      "loss at batch 36928: 1.374639868736267\n",
      "loss at batch 36992: 1.981669306755066\n",
      "loss at batch 37056: 1.3184821605682373\n",
      "loss at batch 37120: 1.5495113134384155\n",
      "loss at batch 37184: 1.3837487697601318\n",
      "loss at batch 37248: 2.360199451446533\n",
      "loss at batch 37312: 1.4214123487472534\n",
      "loss at batch 37376: 1.967907428741455\n",
      "loss at batch 37440: 1.4731303453445435\n",
      "loss at batch 37504: 2.334567070007324\n",
      "loss at batch 37568: 1.7788015604019165\n",
      "loss at batch 37632: 1.9473161697387695\n",
      "loss at batch 37696: 1.4100321531295776\n",
      "loss at batch 37760: 1.3591701984405518\n",
      "loss at batch 37824: 1.9229962825775146\n",
      "loss at batch 37888: 1.5479891300201416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 37952: 1.1831907033920288\n",
      "loss at batch 38016: 1.4776099920272827\n",
      "loss at batch 38080: 1.510644555091858\n",
      "loss at batch 38144: 2.16936993598938\n",
      "loss at batch 38208: 1.5486377477645874\n",
      "loss at batch 38272: 1.2005616426467896\n",
      "loss at batch 38336: 1.4735326766967773\n",
      "loss at batch 38400: 1.6698955297470093\n",
      "loss at batch 38464: 1.4448119401931763\n",
      "loss at batch 38528: 1.4077224731445312\n",
      "loss at batch 38592: 1.3983525037765503\n",
      "loss at batch 38656: 1.2728406190872192\n",
      "loss at batch 38720: 1.3324402570724487\n",
      "loss at batch 38784: 1.3455169200897217\n",
      "loss at batch 38848: 1.5780043601989746\n",
      "loss at batch 38912: 1.3684868812561035\n",
      "loss at batch 38976: 1.534103274345398\n",
      "loss at batch 39040: 1.6419296264648438\n",
      "loss at batch 39104: 1.5289592742919922\n",
      "loss at batch 39168: 1.364574909210205\n",
      "loss at batch 39232: 1.5066109895706177\n",
      "loss at batch 39296: 1.451624870300293\n",
      "loss at batch 39360: 1.3010140657424927\n",
      "loss at batch 39424: 1.3691118955612183\n",
      "loss at batch 39488: 1.9857182502746582\n",
      "loss at batch 39552: 1.3383182287216187\n",
      "loss at batch 39616: 1.4991402626037598\n",
      "loss at batch 39680: 1.469266414642334\n",
      "loss at batch 39744: 1.4079194068908691\n",
      "loss at batch 39808: 1.4370723962783813\n",
      "loss at batch 39872: 1.107909083366394\n",
      "loss at batch 39936: 1.4271198511123657\n",
      "loss at batch 40000: 2.2006542682647705\n",
      "loss at batch 40064: 1.2416399717330933\n",
      "loss at batch 40128: 1.0998125076293945\n",
      "loss at batch 40192: 1.590778112411499\n",
      "loss at batch 40256: 1.4491068124771118\n",
      "loss at batch 40320: 1.5330740213394165\n",
      "loss at batch 40384: 1.25404953956604\n",
      "loss at batch 40448: 1.481737494468689\n",
      "loss at batch 40512: 1.2190306186676025\n",
      "loss at batch 40576: 1.7642720937728882\n",
      "loss at batch 40640: 2.1640243530273438\n",
      "loss at batch 40704: 1.3774232864379883\n",
      "loss at batch 40768: 1.5909453630447388\n",
      "loss at batch 40832: 1.6700074672698975\n",
      "loss at batch 40896: 1.851026177406311\n",
      "loss at batch 40960: 2.4461333751678467\n",
      "loss at batch 41024: 1.49491286277771\n",
      "loss at batch 41088: 1.3705034255981445\n",
      "loss at batch 41152: 1.3357844352722168\n",
      "loss at batch 41216: 1.4015432596206665\n",
      "loss at batch 41280: 1.326931118965149\n",
      "loss at batch 41344: 1.5863878726959229\n",
      "loss at batch 41408: 1.2076178789138794\n",
      "loss at batch 41472: 1.262140154838562\n",
      "loss at batch 41536: 1.254658579826355\n",
      "loss at batch 41600: 1.079253077507019\n",
      "loss at batch 41664: 1.775904655456543\n",
      "loss at batch 41728: 1.3084328174591064\n",
      "loss at batch 41792: 1.711602807044983\n",
      "loss at batch 41856: 1.3718551397323608\n",
      "loss at batch 41920: 1.3308519124984741\n",
      "loss at batch 41984: 1.3184431791305542\n",
      "loss at batch 42048: 1.737866997718811\n",
      "loss at batch 42112: 1.272828459739685\n",
      "loss at batch 42176: 1.1631510257720947\n",
      "loss at batch 42240: 1.3399657011032104\n",
      "loss at batch 42304: 1.1494172811508179\n",
      "loss at batch 42368: 2.047257900238037\n",
      "loss at batch 42432: 1.4269130229949951\n",
      "loss at batch 42496: 1.8599895238876343\n",
      "loss at batch 42560: 1.2208603620529175\n",
      "loss at batch 42624: 1.585280418395996\n",
      "loss at batch 42688: 1.287423014640808\n",
      "loss at batch 42752: 2.2030725479125977\n",
      "loss at batch 42816: 1.3561807870864868\n",
      "loss at batch 42880: 1.3441963195800781\n",
      "loss at batch 42944: 1.6778956651687622\n",
      "loss at batch 43008: 1.1828809976577759\n",
      "loss at batch 43072: 1.5113874673843384\n",
      "loss at batch 43136: 1.584116816520691\n",
      "loss at batch 43200: 1.2071788311004639\n",
      "loss at batch 43264: 1.2711583375930786\n",
      "loss at batch 43328: 1.1960424184799194\n",
      "loss at batch 43392: 1.3369213342666626\n",
      "loss at batch 43456: 1.3000977039337158\n",
      "loss at batch 43520: 1.2717455625534058\n",
      "loss at batch 43584: 1.4162684679031372\n",
      "loss at batch 43648: 1.8631188869476318\n",
      "loss at batch 43712: 1.605418086051941\n",
      "loss at batch 43776: 1.2353179454803467\n",
      "loss at batch 43840: 1.6947330236434937\n",
      "loss at batch 43904: 1.1110591888427734\n",
      "loss at batch 43968: 1.368196725845337\n",
      "loss at batch 44032: 1.663131833076477\n",
      "loss at batch 44096: 1.3699313402175903\n",
      "loss at batch 44160: 1.8234765529632568\n",
      "loss at batch 44224: 1.6765689849853516\n",
      "loss at batch 44288: 1.6283341646194458\n",
      "loss at batch 44352: 3.129389524459839\n",
      "loss at batch 44416: 1.2852369546890259\n",
      "loss at batch 44480: 1.6296091079711914\n",
      "loss at batch 44544: 1.3357864618301392\n",
      "loss at batch 44608: 1.2370615005493164\n",
      "loss at batch 44672: 1.6439539194107056\n",
      "loss at batch 44736: 1.1418203115463257\n",
      "loss at batch 44800: 1.3588263988494873\n",
      "loss at batch 44864: 1.4038422107696533\n",
      "loss at batch 44928: 1.9145630598068237\n",
      "loss at batch 44992: 1.664365530014038\n",
      "loss at batch 45056: 1.131201148033142\n",
      "loss at batch 45120: 1.563858151435852\n",
      "loss at batch 45184: 1.445797324180603\n",
      "loss at batch 45248: 1.727782964706421\n",
      "loss at batch 45312: 1.3580645322799683\n",
      "loss at batch 45376: 1.4008595943450928\n",
      "loss at batch 45440: 1.3759522438049316\n",
      "loss at batch 45504: 1.1306087970733643\n",
      "loss at batch 45568: 1.8757878541946411\n",
      "loss at batch 45632: 1.2583823204040527\n",
      "loss at batch 45696: 1.434985637664795\n",
      "loss at batch 45760: 1.2271887063980103\n",
      "loss at batch 45824: 1.1712267398834229\n",
      "loss at batch 45888: 1.3344449996948242\n",
      "loss at batch 45952: 1.2790571451187134\n",
      "loss at batch 46016: 1.3403061628341675\n",
      "loss at batch 46080: 1.8826160430908203\n",
      "loss at batch 46144: 1.2643719911575317\n",
      "loss at batch 46208: 1.1255617141723633\n",
      "loss at batch 46272: 1.9313126802444458\n",
      "loss at batch 46336: 1.6969295740127563\n",
      "loss at batch 46400: 1.2378593683242798\n",
      "loss at batch 46464: 1.5931209325790405\n",
      "loss at batch 46528: 1.490149736404419\n",
      "loss at batch 46592: 1.4933760166168213\n",
      "loss at batch 46656: 1.439794659614563\n",
      "loss at batch 46720: 1.4507826566696167\n",
      "loss at batch 46784: 1.290907859802246\n",
      "loss at batch 46848: 1.4521514177322388\n",
      "loss at batch 46912: 2.132741689682007\n",
      "loss at batch 46976: 1.5460153818130493\n",
      "loss at batch 47040: 1.1196701526641846\n",
      "loss at batch 47104: 1.179233431816101\n",
      "loss at batch 47168: 1.3743475675582886\n",
      "loss at batch 47232: 1.6924140453338623\n",
      "loss at batch 47296: 1.4989840984344482\n",
      "loss at batch 47360: 1.5201693773269653\n",
      "loss at batch 47424: 1.362163782119751\n",
      "loss at batch 47488: 1.374611496925354\n",
      "loss at batch 47552: 1.21593177318573\n",
      "loss at batch 47616: 1.457546353340149\n",
      "loss at batch 47680: 1.1588451862335205\n",
      "loss at batch 47744: 1.3738923072814941\n",
      "loss at batch 47808: 1.4430369138717651\n",
      "loss at batch 47872: 1.049891710281372\n",
      "loss at batch 47936: 2.0507216453552246\n",
      "loss at batch 48000: 1.3874176740646362\n",
      "loss at batch 48064: 1.599409580230713\n",
      "loss at batch 48128: 1.5384403467178345\n",
      "loss at batch 48192: 1.440983533859253\n",
      "loss at batch 48256: 1.4364930391311646\n",
      "loss at batch 48320: 1.4376749992370605\n",
      "loss at batch 48384: 1.1338560581207275\n",
      "loss at batch 48448: 1.980643391609192\n",
      "loss at batch 48512: 1.170100450515747\n",
      "loss at batch 48576: 1.827365517616272\n",
      "loss at batch 48640: 1.2800480127334595\n",
      "loss at batch 48704: 1.2929713726043701\n",
      "loss at batch 48768: 1.641506314277649\n",
      "loss at batch 48832: 1.6112511157989502\n",
      "loss at batch 48896: 1.3393335342407227\n",
      "loss at batch 48960: 1.3454854488372803\n",
      "loss at batch 49024: 1.8632898330688477\n",
      "loss at batch 49088: 1.3349597454071045\n",
      "loss at batch 49152: 1.3410351276397705\n",
      "loss at batch 49216: 1.510565161705017\n",
      "loss at batch 49280: 1.3911089897155762\n",
      "loss at batch 49344: 1.8360410928726196\n",
      "loss at batch 49408: 1.0342442989349365\n",
      "loss at batch 49472: 1.694988489151001\n",
      "loss at batch 49536: 1.8928782939910889\n",
      "loss at batch 49600: 1.560991644859314\n",
      "loss at batch 49664: 1.2280235290527344\n",
      "loss at batch 49728: 1.1239691972732544\n",
      "loss at batch 49792: 2.0773375034332275\n",
      "loss at batch 49856: 1.0563007593154907\n",
      "loss at batch 49920: 1.143432378768921\n",
      "loss at batch 49984: 1.560120940208435\n",
      "loss at batch 50048: 1.3111157417297363\n",
      "loss at batch 50112: 1.4264203310012817\n",
      "loss at batch 50176: 1.3355292081832886\n",
      "loss at batch 50240: 1.306427240371704\n",
      "loss at batch 50304: 1.3713836669921875\n",
      "loss at batch 50368: 1.2135463953018188\n",
      "loss at batch 50432: 1.4034576416015625\n",
      "loss at batch 50496: 1.4389272928237915\n",
      "loss at batch 50560: 1.6816215515136719\n",
      "loss at batch 50624: 1.3984653949737549\n",
      "loss at batch 50688: 1.4329752922058105\n",
      "loss at batch 50752: 1.68293297290802\n",
      "loss at batch 50816: 1.5294551849365234\n",
      "loss at batch 50880: 1.4945369958877563\n",
      "loss at batch 50944: 1.62848961353302\n",
      "loss at batch 51008: 1.354697585105896\n",
      "loss at batch 51072: 1.289199948310852\n",
      "loss at batch 51136: 1.3456624746322632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 51200: 2.1877524852752686\n",
      "loss at batch 51264: 1.3580201864242554\n",
      "loss at batch 51328: 1.2527623176574707\n",
      "loss at batch 51392: 1.2133424282073975\n",
      "loss at batch 51456: 1.1486387252807617\n",
      "loss at batch 51520: 1.2204854488372803\n",
      "loss at batch 51584: 2.4216394424438477\n",
      "loss at batch 51648: 1.610790491104126\n",
      "loss at batch 51712: 1.5114918947219849\n",
      "loss at batch 51776: 0.9596057534217834\n",
      "loss at batch 51840: 1.315780758857727\n",
      "loss at batch 51904: 1.2247287034988403\n",
      "loss at batch 51968: 1.555325984954834\n",
      "loss at batch 52032: 1.3328242301940918\n",
      "loss at batch 52096: 1.6013100147247314\n",
      "loss at batch 52160: 1.442153811454773\n",
      "loss at batch 52224: 1.6122807264328003\n",
      "loss at batch 52288: 1.3478187322616577\n",
      "loss at batch 52352: 1.7553346157073975\n",
      "loss at batch 52416: 1.3088953495025635\n",
      "loss at batch 52480: 2.396237850189209\n",
      "loss at batch 52544: 1.535386562347412\n",
      "loss at batch 52608: 1.3732677698135376\n",
      "loss at batch 52672: 1.096889853477478\n",
      "loss at batch 52736: 1.2309954166412354\n",
      "loss at batch 52800: 1.1828572750091553\n",
      "loss at batch 52864: 1.175092339515686\n",
      "loss at batch 52928: 1.5235522985458374\n",
      "loss at batch 52992: 1.4029226303100586\n",
      "loss at batch 53056: 1.6670953035354614\n",
      "loss at batch 53120: 1.387437105178833\n",
      "loss at batch 53184: 1.1930491924285889\n",
      "loss at batch 53248: 1.4029268026351929\n",
      "loss at batch 53312: 1.4162763357162476\n",
      "loss at batch 53376: 1.1694515943527222\n",
      "loss at batch 53440: 1.0826319456100464\n",
      "loss at batch 53504: 1.4405090808868408\n",
      "loss at batch 53568: 1.538277506828308\n",
      "loss at batch 53632: 1.2315741777420044\n",
      "loss at batch 53696: 1.2537708282470703\n",
      "loss at batch 53760: 1.8230892419815063\n",
      "loss at batch 53824: 2.0378665924072266\n",
      "loss at batch 53888: 1.949224591255188\n",
      "loss at batch 53952: 1.3191795349121094\n",
      "loss at batch 54016: 1.1784340143203735\n",
      "loss at batch 54080: 1.4476743936538696\n",
      "loss at batch 54144: 1.1732065677642822\n",
      "loss at batch 54208: 0.9554389119148254\n",
      "loss at batch 54272: 1.334352731704712\n",
      "loss at batch 54336: 1.2181484699249268\n",
      "loss at batch 54400: 1.0207189321517944\n",
      "loss at batch 54464: 1.4073363542556763\n",
      "loss at batch 54528: 1.5579636096954346\n",
      "loss at batch 54592: 1.6052581071853638\n",
      "loss at batch 54656: 1.3521634340286255\n",
      "loss at batch 54720: 1.9706730842590332\n",
      "loss at batch 54784: 1.291488766670227\n",
      "loss at batch 54848: 1.2142674922943115\n",
      "loss at batch 54912: 2.5330111980438232\n",
      "loss at batch 54976: 1.6497113704681396\n",
      "loss at batch 55040: 1.8869459629058838\n",
      "loss at batch 55104: 1.2673804759979248\n",
      "loss at batch 55168: 2.037691831588745\n",
      "loss at batch 55232: 1.4227968454360962\n",
      "loss at batch 55296: 1.4081107378005981\n",
      "loss at batch 55360: 1.368228793144226\n",
      "loss at batch 55424: 1.1494346857070923\n",
      "loss at batch 55488: 1.5705819129943848\n",
      "loss at batch 55552: 1.3581125736236572\n",
      "loss at batch 55616: 1.5275607109069824\n",
      "loss at batch 55680: 1.209059476852417\n",
      "loss at batch 55744: 1.1989187002182007\n",
      "loss at batch 55808: 1.2993638515472412\n",
      "loss at batch 55872: 1.3150913715362549\n",
      "loss at batch 55936: 1.3472073078155518\n",
      "loss at batch 56000: 1.755128264427185\n",
      "loss at batch 56064: 1.504133701324463\n",
      "loss at batch 56128: 1.3956513404846191\n",
      "loss at batch 56192: 1.6385091543197632\n",
      "loss at batch 56256: 1.3954507112503052\n",
      "loss at batch 56320: 1.2708081007003784\n",
      "loss at batch 56384: 1.305539846420288\n",
      "loss at batch 56448: 1.519840121269226\n",
      "loss at batch 56512: 1.5233503580093384\n",
      "loss at batch 56576: 1.413068175315857\n",
      "loss at batch 56640: 1.49459969997406\n",
      "loss at batch 56704: 2.538349151611328\n",
      "loss at batch 56768: 1.1312960386276245\n",
      "loss at batch 56832: 1.3387516736984253\n",
      "loss at batch 56896: 1.5403772592544556\n",
      "loss at batch 56960: 1.4704965353012085\n",
      "loss at batch 57024: 1.5843548774719238\n",
      "loss at batch 57088: 1.5657705068588257\n",
      "loss at batch 57152: 1.1140708923339844\n",
      "loss at batch 57216: 1.3624248504638672\n",
      "loss at batch 57280: 1.4322419166564941\n",
      "loss at batch 57344: 1.316054105758667\n",
      "loss at batch 57408: 2.02205491065979\n",
      "loss at batch 57472: 1.3238298892974854\n",
      "loss at batch 57536: 1.277969241142273\n",
      "loss at batch 57600: 1.343004822731018\n",
      "loss at batch 57664: 1.4199877977371216\n",
      "loss at batch 57728: 1.0728635787963867\n",
      "loss at batch 57792: 1.4931343793869019\n",
      "loss at batch 57856: 1.069664716720581\n",
      "loss at batch 57920: 1.498064637184143\n",
      "loss at batch 57984: 1.4404786825180054\n",
      "loss at batch 58048: 1.2446202039718628\n",
      "loss at batch 58112: 1.3591901063919067\n",
      "loss at batch 58176: 1.4158456325531006\n",
      "loss at batch 58240: 1.6466656923294067\n",
      "loss at batch 58304: 1.156040072441101\n",
      "loss at batch 58368: 2.1551129817962646\n",
      "loss at batch 58432: 1.3903110027313232\n",
      "loss at batch 58496: 1.717971920967102\n",
      "loss at batch 58560: 1.2191160917282104\n",
      "loss at batch 58624: 1.3505892753601074\n",
      "loss at batch 58688: 1.3364427089691162\n",
      "loss at batch 58752: 1.1958414316177368\n",
      "loss at batch 58816: 1.3937751054763794\n",
      "loss at batch 58880: 1.5225611925125122\n",
      "loss at batch 58944: 1.2857263088226318\n",
      "loss at batch 59008: 1.315049171447754\n",
      "loss at batch 59072: 1.524828314781189\n",
      "loss at batch 59136: 1.351442813873291\n",
      "loss at batch 59200: 1.0984456539154053\n",
      "loss at batch 59264: 1.3913519382476807\n",
      "loss at batch 59328: 1.3876070976257324\n",
      "loss at batch 59392: 1.4740784168243408\n",
      "loss at batch 59456: 1.29313325881958\n",
      "loss at batch 59520: 1.161413311958313\n",
      "loss at batch 59584: 1.666107177734375\n",
      "loss at batch 59648: 1.3237061500549316\n",
      "loss at batch 59712: 1.1973706483840942\n",
      "loss at batch 59776: 1.432897686958313\n",
      "loss at batch 59840: 1.7107359170913696\n",
      "loss at batch 59904: 1.2180533409118652\n",
      "loss at batch 59968: 1.2326436042785645\n",
      "loss at batch 60032: 1.4994488954544067\n",
      "loss at batch 60096: 1.3415106534957886\n",
      "loss at batch 60160: 1.0118285417556763\n",
      "loss at batch 60224: 1.2472580671310425\n",
      "loss at batch 60288: 1.4044190645217896\n",
      "loss at batch 60352: 1.2822039127349854\n",
      "loss at batch 60416: 1.3114738464355469\n",
      "loss at batch 60480: 1.2173645496368408\n",
      "loss at batch 60544: 1.464290976524353\n",
      "loss at batch 60608: 1.3322454690933228\n",
      "loss at batch 60672: 1.4918906688690186\n",
      "loss at batch 60736: 1.4211986064910889\n",
      "loss at batch 60800: 1.7732371091842651\n",
      "loss at batch 60864: 1.579685926437378\n",
      "loss at batch 60928: 1.2873746156692505\n",
      "loss at batch 60992: 1.2531577348709106\n",
      "loss at batch 61056: 1.3787636756896973\n",
      "loss at batch 61120: 1.3978705406188965\n",
      "loss at batch 61184: 1.6725713014602661\n",
      "loss at batch 61248: 1.8116859197616577\n",
      "loss at batch 61312: 1.4066625833511353\n",
      "loss at batch 61376: 1.4268306493759155\n",
      "loss at batch 61440: 1.5452039241790771\n",
      "loss at batch 61504: 1.4448397159576416\n",
      "loss at batch 61568: 1.4580330848693848\n",
      "loss at batch 61632: 1.3002465963363647\n",
      "loss at batch 61696: 1.3620489835739136\n",
      "loss at batch 61760: 1.4137271642684937\n",
      "loss at batch 61824: 1.0483940839767456\n",
      "loss at batch 61888: 1.127805471420288\n",
      "loss at batch 61952: 1.5778112411499023\n",
      "loss at batch 62016: 1.2403371334075928\n",
      "loss at batch 62080: 1.8903307914733887\n",
      "loss at batch 62144: 1.4022847414016724\n",
      "loss at batch 62208: 1.5088750123977661\n",
      "loss at batch 62272: 1.4687532186508179\n",
      "loss at batch 62336: 1.5203843116760254\n",
      "loss at batch 62400: 1.776013970375061\n",
      "loss at batch 62464: 1.1720662117004395\n",
      "loss at batch 62528: 1.5644447803497314\n",
      "loss at batch 62592: 1.5380038022994995\n",
      "loss at batch 62656: 1.3642487525939941\n",
      "loss at batch 62720: 1.333624005317688\n",
      "loss at batch 62784: 1.584096074104309\n",
      "loss at batch 62848: 1.3281700611114502\n",
      "loss at batch 62912: 1.3947887420654297\n",
      "loss at batch 62976: 1.715044379234314\n",
      "loss at batch 63040: 1.636715054512024\n",
      "loss at batch 63104: 1.710882544517517\n",
      "loss at batch 63168: 1.583613395690918\n",
      "loss at batch 63232: 1.2527916431427002\n",
      "loss at batch 63296: 2.309492349624634\n",
      "loss at batch 63360: 1.3495811223983765\n",
      "loss at batch 63424: 1.5238549709320068\n",
      "loss at batch 63488: 1.2953474521636963\n",
      "loss at batch 63552: 1.5836855173110962\n",
      "loss at batch 63616: 1.2946438789367676\n",
      "loss at batch 63680: 1.5175284147262573\n",
      "loss at batch 63744: 1.290374755859375\n",
      "loss at batch 63808: 1.3115320205688477\n",
      "loss at batch 63872: 1.2779672145843506\n",
      "loss at batch 63936: 1.9018791913986206\n",
      "loss at batch 64000: 1.4181517362594604\n",
      "loss at batch 64064: 1.2719870805740356\n",
      "loss at batch 64128: 1.5969973802566528\n",
      "loss at batch 64192: 1.2828346490859985\n",
      "loss at batch 64256: 1.453896164894104\n",
      "loss at batch 64320: 1.7658162117004395\n",
      "loss at batch 64384: 1.5396703481674194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 64448: 1.0541563034057617\n",
      "loss at batch 64512: 1.0542476177215576\n",
      "loss at batch 64576: 1.4600673913955688\n",
      "loss at batch 64640: 1.3554391860961914\n",
      "loss at batch 64704: 1.3420755863189697\n",
      "loss at batch 64768: 1.3126546144485474\n",
      "loss at batch 64832: 1.5440335273742676\n",
      "loss at batch 64896: 1.6679800748825073\n",
      "loss at batch 64960: 1.2868483066558838\n",
      "loss at batch 65024: 1.3626399040222168\n",
      "loss at batch 65088: 1.1217142343521118\n",
      "loss at batch 65152: 1.021314024925232\n",
      "loss at batch 65216: 1.5684555768966675\n",
      "loss at batch 65280: 1.5149649381637573\n",
      "loss at batch 65344: 1.1531236171722412\n",
      "loss at batch 65408: 1.2148257493972778\n",
      "loss at batch 65472: 1.2140721082687378\n",
      "loss at batch 65536: 1.2466192245483398\n",
      "loss at batch 65600: 1.4432514905929565\n",
      "loss at batch 65664: 1.4813655614852905\n",
      "loss at batch 65728: 1.1811723709106445\n",
      "loss at batch 65792: 1.2218519449234009\n",
      "loss at batch 65856: 1.2808061838150024\n",
      "loss at batch 65920: 1.310281753540039\n",
      "loss at batch 65984: 1.2263480424880981\n",
      "loss at batch 66048: 1.1426770687103271\n",
      "loss at batch 66112: 1.671134352684021\n",
      "loss at batch 66176: 1.3357770442962646\n",
      "loss at batch 66240: 1.1420642137527466\n",
      "loss at batch 66304: 1.1484260559082031\n",
      "loss at batch 66368: 1.0760186910629272\n",
      "loss at batch 66432: 2.6264238357543945\n",
      "loss at batch 66496: 1.1524142026901245\n",
      "loss at batch 66560: 1.2645868062973022\n",
      "loss at batch 66624: 1.3486391305923462\n",
      "loss at batch 66688: 1.5552059412002563\n",
      "loss at batch 66752: 1.3330495357513428\n",
      "loss at batch 66816: 1.7861088514328003\n",
      "loss at batch 66880: 1.2867417335510254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b18f228c3505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wikiracer/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wikiracer/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "reg_coeff = 1.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        epoch_loss = 0        \n",
    "        \n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = nss(x, y)\n",
    "            \n",
    "            loss += reg_coeff * title_embedding.mh_attn.v_agreement\n",
    "            title_embedding.mh_attn.clear_agreement()\n",
    "            \n",
    "            if loss.isnan() or loss.isinf():\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                print('loss at batch {}: {}'.format((idx+1), loss.item()))\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            \n",
    "            if (idx + 1) % 16384 == 0:\n",
    "                torch.save(title_embedding, '../models/model{}_{}.pt'.format(epoch, (int)(idx+1) / 16384))\n",
    "            \n",
    "        print('loss at end of epoch {}: {}'.format(epoch, epoch_loss/len(train_loader)))\n",
    "        \n",
    "        with open('../models/model{}.pt'.format(epoch+1), 'wb') as of:\n",
    "            torch.save(title_embedding, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.plot(range(len(losses)), losses,'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_embedding.mh_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_title = torch.LongTensor([1, 9, 17])\n",
    "padded_title = torch.LongTensor([1, 9, 17, 0])\n",
    "\n",
    "w_embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True, padding_idx=0)\n",
    "w_embeddings.weight.data[0] = torch.zeros(300)\n",
    "mult_attn = MultiplicativeAttention()\n",
    "add_attn = AdditiveSelfAttention(300)\n",
    "mh_attn = MultiHeadAttention(300, 12, 25, 25)\n",
    "nff = NonlinearFF(300, 25)\n",
    "\n",
    "torch.set_printoptions(linewidth=120,threshold=100)\n",
    "mult_attn.eval()\n",
    "mh_attn.eval()\n",
    "nff.eval()\n",
    "add_attn.eval()\n",
    "mult_attn.eval()\n",
    "title_embedding.eval()\n",
    "\n",
    "def transform(title, mask=None): \n",
    "    z = title\n",
    "    s = mask\n",
    "    print('title: {}'.format(title))\n",
    "    \n",
    "    q = k = v = w_embeddings(title)\n",
    "    \n",
    "    print('embedded title: \\n{}'.format(q))\n",
    "    print(q.shape)\n",
    "    \n",
    "    title, attn = mh_attn(q, k, v, mask)\n",
    "    \n",
    "    print('transformed title: \\n{}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    title = nff(title)\n",
    "    \n",
    "    print('title after nonlinear ff: \\n {}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    title, importance = add_attn(title, mask)\n",
    "    \n",
    "    print('importance of each word: \\n {}'.format(importance))\n",
    "    print(importance.shape)\n",
    "    \n",
    "    print('title after everything: \\n {}'.format(title))\n",
    "    print(title.shape)\n",
    "    \n",
    "    \n",
    "    y = title_embedding(z, mask=s)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "\n",
    "transform(torch.LongTensor([[1, 5, 7, 2, 9, 8, 0, 0], [2, 5, 5, 0, 0, 0, 0, 0]]),        \n",
    "                    mask= torch.ByteTensor([[0,0,0,0,0,0,1,1],[0,0,0,1,1,1,1,1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiracer",
   "language": "python",
   "name": "wikiracer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
