{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname, get_embeddings=True, get_w2i=False, get_i2w=False, skip_first_line=True):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    \n",
    "    if skip_first_line:\n",
    "        fin.readline()\n",
    "    \n",
    "    num_embeddings = 0\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.rstrip().split(' ')\n",
    "        \n",
    "        if get_w2i:\n",
    "            word2idx[line[0]] = num_embeddings\n",
    "        if get_i2w:\n",
    "            idx2word[num_embeddings] = line[0]\n",
    "        if get_embeddings:\n",
    "            embeddings.append([float(num) for num in line[1:]])\n",
    "\n",
    "        num_embeddings += 1\n",
    "        \n",
    "    fin.close()\n",
    "    \n",
    "    return torch.FloatTensor(embeddings), word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = load_embeddings('../embeddings/wiki-news-300d-1M.vec', get_embeddings=False, get_w2i=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "      \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = torch.matmul(q , k.transpose(-2, -1) / math.sqrt(q.size(-1)))\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask.unsqueeze(1) == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))        \n",
    "        res = torch.matmul(attn, v)\n",
    "\n",
    "        return res, attn\n",
    "\n",
    "class AdditiveSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.w = nn.Linear(d_model, d_model)\n",
    "        self.q = torch.nn.Parameter(torch.FloatTensor(d_model).uniform_(-0.1, 0.1))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn = torch.tanh(self.dropout(self.w(x)))        \n",
    "        attn = torch.matmul(attn, self.q)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 1, -1e9)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "\n",
    "        \n",
    "        res = torch.einsum('ijk, ij->ik', x, attn)\n",
    "        return res, attn\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_qk, d_v, track_agreement=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_qk = d_qk\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, num_heads * d_qk, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, num_heads * d_v, bias=False)\n",
    "        \n",
    "        self.w_fc = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "        \n",
    "        self.attention = MultiplicativeAttention(dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "        self.track_agreement = track_agreement\n",
    "        self.v_agreement = 0\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):     \n",
    "        batch_size = q.shape[0]\n",
    "        seq_size = q.shape[1]\n",
    "        \n",
    "        q_proj = self.w_q(q).view(q.shape[0], q.shape[1], self.num_heads, self.d_qk)\n",
    "        k_proj = self.w_k(k).view(k.shape[0], k.shape[1], self.num_heads, self.d_qk)\n",
    "        v_proj = self.w_v(v).view(v.shape[0], v.shape[1], self.num_heads, self.d_v) \n",
    "\n",
    "        if self.track_agreement:\n",
    "            self.v_agreement += torch.einsum('bshd, bsnd->', F.normalize(v_proj, dim=3), F.normalize(v_proj, dim=3)) / self.num_heads**2\n",
    "\n",
    "        if mask is None:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2))\n",
    "        else:\n",
    "            q, attn = self.attention(q_proj.transpose(1, 2), k_proj.transpose(1, 2), v_proj.transpose(1, 2), mask.unsqueeze(1))\n",
    "        \n",
    "        q = q.transpose(1, 2).contiguous()\n",
    "        q = q.view(batch_size, seq_size, -1)\n",
    "\n",
    "        q = self.dropout(self.w_fc(q))\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "        \n",
    "        return q, attn\n",
    "\n",
    "    def clear_agreement(self):\n",
    "        self.v_agreement = 0\n",
    "\n",
    "class NonlinearFF(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class TitleEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, d_model, num_heads, d_qk, d_v, d_hid=None, embeddings=None, track_agreement=False, padding_idx=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(num_embeddings, d_model, sparse=True, padding_idx=0)\n",
    "            \n",
    "        self.mh_attn = MultiHeadAttention(d_model, num_heads, d_qk, d_v, track_agreement=track_agreement, dropout=dropout)\n",
    "        self.nff = NonlinearFF(d_model, d_hid if d_hid is not None else d_model * 4, dropout=dropout)\n",
    "        self.add_attn = AdditiveSelfAttention(d_model, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "    def forward(self, title):    \n",
    "        mask = (title == self.padding_idx).byte()\n",
    "        \n",
    "        q = k = v = self.embeddings(title)\n",
    "        title, attn = self.mh_attn(q, k ,v, mask=mask)\n",
    "        \n",
    "        title = self.nff(title)\n",
    "        title, add_attn = self.add_attn(title, mask=mask)\n",
    "        \n",
    "        title = self.layer_norm(title)\n",
    "        \n",
    "        return title\n",
    "    \n",
    "    def load_embeddings(embeddings):\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=120, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitleEmbedding(\n",
       "  (embeddings): Embedding(999994, 300, padding_idx=0, sparse=True)\n",
       "  (mh_attn): MultiHeadAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (w_q): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_k): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_v): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (w_fc): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (attention): MultiplicativeAttention(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (nff): NonlinearFF(\n",
       "    (w_1): Linear(in_features=300, out_features=1200, bias=True)\n",
       "    (w_2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (add_attn): AdditiveSelfAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       "    (w): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embedding = torch.load('../models/model1_2.0.pt', map_location='cpu')\n",
    "title_embedding.to(device)\n",
    "title_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding(torch.LongTensor([[1]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwclient\n",
    "\n",
    "from mwclient.page import Page\n",
    "from mwclient import Site\n",
    "\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "ua = 'WikiRacer/0.1 (radusinovictadija@gmail.com)'\n",
    "\n",
    "wiki = Site('en.wikipedia.org', clients_useragent=ua)\n",
    "wiki_api = 'http://en.wikipedia.org/w/api.php'\n",
    "\n",
    "generator = wiki.random(0, 1)\n",
    "\n",
    "def get_random_page():\n",
    "    for rand in generator:\n",
    "        return Page(wiki, rand['title'])\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "meaningless_tokens = [',', '.', '\\'' , '', '\"', '-', '_', 'â€“', '&', '\\'\\'', '\"\"']\n",
    "\n",
    "def parse_text(text, stopwords=[]): \n",
    "    \n",
    "    \n",
    "    tokens = [word for word in nltk.word_tokenize(text) if word.lower() not in meaningless_tokens and word.lower() not in stopwords]\n",
    "    \n",
    "    trans = str.maketrans('', '', '[]().,;:|`')\n",
    "    \n",
    "    tokens = [token.translate(trans) for token in tokens]\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "    \n",
    "    return list(dict.fromkeys(tokens))\n",
    "\n",
    "def encode_seq(seq, tolerate_miss=True):\n",
    "    res = []\n",
    "    \n",
    "    for token in seq:\n",
    "        if token in word2idx:\n",
    "            res.append(word2idx[token])\n",
    "        elif not tolerate_miss:\n",
    "                return []\n",
    "    \n",
    "    return torch.LongTensor(res)  \n",
    "\n",
    "def get_page_summary(id):\n",
    "    query_params = {\n",
    "                    'action': 'query',\n",
    "                    'prop': 'extracts',\n",
    "                    'exintro': '',\n",
    "                    'explaintext': '',\n",
    "                    'pageids': id,\n",
    "                    'format': 'json'\n",
    "                   }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': ua\n",
    "    }\n",
    "    \n",
    "    r = requests.get(wiki_api, params=query_params, headers=headers).json()\n",
    "    return parse_text(r['query']['pages'][str(id)]['extract'], stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race(a, b, title_embedding, device):\n",
    "    \n",
    "    print('starting at page `{}`'.format(a.name))\n",
    "    print('trying to reach page `{}`'.format(b.name))\n",
    "    \n",
    "\n",
    "    goal_seq = encode_seq(parse_text(b.name))\n",
    "    \n",
    "    if goal_seq == []:\n",
    "        print('error: goal tokens unknown')\n",
    "        return\n",
    "    \n",
    "    goal_seq = F.normalize(title_embedding(goal_seq.unsqueeze(0).to(device)))\n",
    "    visited = set()\n",
    "    \n",
    "    while(a.name != b.name):\n",
    "        best_link = None\n",
    "        best_sim = -1e9\n",
    "        \n",
    "        for link in a.links(0):\n",
    "            \n",
    "            if link.name in visited:\n",
    "                continue\n",
    "                \n",
    "            link_seq = encode_seq(parse_text(link.name))\n",
    "            \n",
    "            if len(link_seq) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                link_seq = link_seq.unsqueeze(0).to(device)\n",
    "            \n",
    "            link_embedding = F.normalize(title_embedding(link_seq))\n",
    "\n",
    "            sim = torch.einsum('ij, ij->', link_embedding, goal_seq)\n",
    "            \n",
    "            if link.pageid is not None and sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_link = link\n",
    "        \n",
    "        if best_link is None:\n",
    "            print('cannot find candidate link')\n",
    "            break\n",
    "        else:\n",
    "            print('{} -> {}'.format(a.name, best_link.name))\n",
    "        \n",
    "            visited.add(best_link.name)\n",
    "            a = best_link        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music\n",
      "Philosophy\n"
     ]
    }
   ],
   "source": [
    "a = Page(wiki, 'Music')\n",
    "b = Page(wiki, 'Philosophy')\n",
    "print(a.name)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at page `Music`\n",
      "trying to reach page `Philosophy`\n",
      "Music -> Academic degree\n",
      "Academic degree -> Theology\n",
      "Theology -> Philosophy\n"
     ]
    }
   ],
   "source": [
    "race(a, b, title_embedding, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(text1, text2):\n",
    "    text1 = encode_seq(parse_text(text1)).unsqueeze(0)\n",
    "    text2 = encode_seq(parse_text(text2)).unsqueeze(0)    \n",
    "\n",
    "    v1 = F.normalize(title_embedding(text1).squeeze(0), dim=0)\n",
    "    v2 = F.normalize(title_embedding(text2).squeeze(0), dim=0)\n",
    "    \n",
    "    return v1.dot(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb(text):\n",
    "    return title_embedding(encode_seq(parse_text(text)).unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb('Music').norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for link in a.links(generator=False):\n",
    "    \n",
    "    lenc = encode_seq(parse_text(link))\n",
    "    \n",
    "    if len(lenc) == 0:\n",
    "        continue\n",
    "        \n",
    "    lenc = lenc.unsqueeze(0)\n",
    "    \n",
    "    if len(lenc) != 0:\n",
    "        lemb = F.normalize(title_embedding(lenc))\n",
    "        \n",
    "        d[link] = torch.einsum('ij, ij->', bemb, lemb)\n",
    "    \n",
    "    \n",
    "d_sort = sorted( ((v,k) for k,v in d.items()), reverse=True)\n",
    "\n",
    "for el in d_sort:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding(encode_seq(parse_text(b.name)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiracer",
   "language": "python",
   "name": "wikiracer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
