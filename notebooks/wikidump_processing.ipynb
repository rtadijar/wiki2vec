{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = str.maketrans('(),_-/', '      ','\\\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET VOCAB AND PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_dict = dict()\n",
    "\n",
    "word_cnt = 1 # 0 reserved\n",
    "word2idx = dict()\n",
    "idx2word = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Tadija\\Desktop\\wikipedia\\page.sql', 'r', encoding='utf8') as _if:\n",
    "    \n",
    "    for i, line in enumerate(_if):\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('processed line {}'.format(i+1))\n",
    "        \n",
    "        \n",
    "        matches = re.findall('\\(([^,]*),0,\\'(.*?)\\',.*?\\)', line)\n",
    "    \n",
    "        for match in matches:    \n",
    "            title = match[1].translate(trans).split()\n",
    "            \n",
    "            if len(title) > 10:\n",
    "                continue\n",
    "            \n",
    "            for token in title:\n",
    "                if token not in word2idx:\n",
    "                    word2idx[token] = word_cnt\n",
    "                    idx2word[word_cnt] = token\n",
    "                    \n",
    "                    word_cnt += 1\n",
    "            \n",
    "            title = [word2idx[token] for token in title]\n",
    "            \n",
    "            pages_dict[int(match[0])] = title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/word2idx.pickle', 'wb') as of:\n",
    "    pickle.dump(word2idx, of)\n",
    "    \n",
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/idx2word.pickle', 'wb') as of:\n",
    "    pickle.dump(idx2word, of)\n",
    "    \n",
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/pages_dict.pickle', 'wb') as of:\n",
    "    pickle.dump(pages_dict, of)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/word2idx.pickle', 'rb') as _if:\n",
    "    word2idx = pickle.load(_if)\n",
    "    \n",
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/idx2word.pickle', 'rb') as _if:\n",
    "    idx2word = pickle.load(_if)\n",
    "    \n",
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/pages_dict.pickle', 'rb') as _if:\n",
    "    pages_dict = pickle.load(_if)\n",
    "    \n",
    "word_cnt = len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Tadija\\Desktop\\wikipedia\\pagelinks.sql','r', encoding='utf8', errors='ignore') as _if:\n",
    "    for i, line in enumerate(_if):\n",
    "        \n",
    "        if i < 18000:\n",
    "            continue\n",
    "            \n",
    "        if i == 24490:\n",
    "            break\n",
    "            \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print('processed line {}'.format(i+1))\n",
    "        \n",
    "        matches = re.findall('\\(([0-9]*),0,\\'(.*?)\\',0\\)', line)\n",
    "\n",
    "        for match in matches:\n",
    "            if int(match[0]) in pages_dict:\n",
    "                \n",
    "                title = match[1].translate(trans).split()\n",
    "                \n",
    "                if len(title) > 10:\n",
    "                    continue\n",
    "                    \n",
    "                to_continue = False\n",
    "                \n",
    "                for token in title:\n",
    "                    if token not in word2idx:\n",
    "                        to_continue = True\n",
    "                        break\n",
    "                \n",
    "                if to_continue:\n",
    "                    continue      \n",
    "                        \n",
    "                title = [word2idx[token] for token in title]\n",
    "            \n",
    "                links_dict[tuple(pages_dict[int(match[0])])].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Tadija/Desktop/wikipedia/pickle/links_dict4.pickle', 'rb') as _if:\n",
    "    links_dict = pickle.load(_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "cutoff = 8388608\n",
    "acc = 0\n",
    "ind = 55\n",
    "\n",
    "for key, value in links_dict.items():\n",
    "    \n",
    "    if len(key) > 8:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    cnt = 0\n",
    "    for v in value:\n",
    "        if len(v) > 0 and len(v) <= 8:\n",
    "            cnt += 1\n",
    "    \n",
    "    x = np.zeros((cnt, 8), dtype=np.int64)\n",
    "    y = np.zeros((cnt, 8), dtype=np.int64)\n",
    "    \n",
    "    x[:,:len(key)] = key\n",
    "    \n",
    "    idx = 0 \n",
    "    for i in range(len(value)):\n",
    "        if len(value[i]) > 0 and len(value[i]) <= 8:\n",
    "            y[idx,:len(value[i])] = value[i]\n",
    "            idx += 1\n",
    "\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    \n",
    "    acc += cnt\n",
    "                   \n",
    "    if acc > cutoff:\n",
    "        xs = np.vstack(xs)\n",
    "        ys = np.vstack(ys)\n",
    "    \n",
    "        np.save('C:/Users/Tadija/Desktop/wikipedia/numpy/xs{}.np'.format(ind), xs)\n",
    "        np.save('C:/Users/Tadija/Desktop/wikipedia/numpy/ys{}.np'.format(ind), ys)\n",
    "\n",
    "        acc = 0\n",
    "        ind += 1\n",
    "        \n",
    "        del xs\n",
    "        del ys\n",
    "        \n",
    "        xs = []\n",
    "        ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.vstack(xs)\n",
    "ys = np.vstack(ys)\n",
    "\n",
    "np.save('C:/Users/Tadija/Desktop/wikipedia/numpy/xs{}.np'.format(ind), xs)\n",
    "np.save('C:/Users/Tadija/Desktop/wikipedia/numpy/ys{}.np'.format(ind), ys)\n",
    "\n",
    "acc = 0\n",
    "ind += 1\n",
    "\n",
    "del xs\n",
    "del ys\n",
    "\n",
    "xs = []\n",
    "ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(0, 68)]\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "indices_groups = []\n",
    "\n",
    "for i in range(7):\n",
    "    indices_groups.append(indices[ i*10 : ((i+1)*10 if (i+1)*10 < 68 else 68) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_groups = [[25, 62, 20, 7, 51, 64, 6, 3, 59, 19],\n",
    " [54, 44, 0, 46, 37, 50, 5, 66, 11, 18],\n",
    " [32, 55, 24, 43, 27, 48, 12, 17, 16, 21],\n",
    " [60, 65, 47, 34, 39, 13, 9, 31, 14, 36],\n",
    " [45, 63, 41, 58, 29, 26, 57, 49, 1, 30],\n",
    " [53, 40, 2, 42, 33, 4, 67, 28, 35, 22],\n",
    " [38, 52, 15, 61, 10, 56, 8, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, group in enumerate(indices_groups):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    for ind in group:\n",
    "        \n",
    "        xs.append(np.load('C:/Users/Tadija/Desktop/wikipedia/numpy/xs{}.np.npy'.format(ind)))\n",
    "        ys.append(np.load('C:/Users/Tadija/Desktop/wikipedia/numpy/ys{}.np.npy'.format(ind)))\n",
    "        \n",
    "    xs = np.vstack(xs)\n",
    "    ys = np.vstack(ys)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(torch.LongTensor(xs), torch.LongTensor(ys))\n",
    "    \n",
    "    torch.save(dataset, 'C:/Users/Tadija/Desktop/wikipedia/tensor/dataset{}.pt'.format(i))\n",
    "    \n",
    "    del xs\n",
    "    del ys\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname, get_embeddings=True, get_w2i=False, get_i2w=False, skip_first_line=True):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    \n",
    "    if skip_first_line:\n",
    "        fin.readline()\n",
    "    \n",
    "    num_embeddings = 0\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for line in fin:\n",
    "        line = line.rstrip().split(' ')\n",
    "        \n",
    "        if get_w2i:\n",
    "            word2idx[line[0]] = num_embeddings\n",
    "        if get_i2w:\n",
    "            idx2word[num_embeddings] = line[0]\n",
    "        if get_embeddings:       \n",
    "            embeddings.append([float(num) for num in line[1:]])\n",
    "        \n",
    "        num_embeddings += 1\n",
    "        \n",
    "        \n",
    "    return torch.FloatTensor(embeddings), word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_used = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.FloatTensor(len(word2idx)+1, 300)\n",
    "\n",
    "torch.nn.init.xavier_normal_(embeddings)\n",
    "\n",
    "embeddings[0] = torch.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "fin = io.open(r'C:\\Users\\Tadija\\Desktop\\word embeddings\\glove.840B.300d.txt', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "for line in fin:\n",
    "    line = line.rstrip().split(' ')\n",
    "    \n",
    "    if line[0] in word2idx:\n",
    "        embeddings_used += 1\n",
    "        \n",
    "        embeddings[word2idx[line[0]]] = torch.FloatTensor([float(num) for num in line[1:]])\n",
    "\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, r'C:\\Users\\Tadija\\Desktop\\wikipedia\\tensor\\embeddings_init.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(a)), a, 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiracer",
   "language": "python",
   "name": "wikiracer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
